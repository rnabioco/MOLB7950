---
title: 'Bootcamp: Stats class 5'
author: "[Neelanjan Mukherjee](neelanjan.mukherjee@cuanschutz.edu)"
date: "[Office Hours](https://calendly.com/molb7950)"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(ggrepel)
library(knitr)
library(ggthemes)
library(cowplot)
library(DT)
library(here)
```

------------------------------------------------------------------------

# Learning Objectives

-   **Visualize** and **Summarize** the data being compared
-   **Formulate** and **Execute** null hypothesis testing
-   **Identify** and **Perform** the proper statistical test for data
    type/comparison
-   **Calculate** and **Interpret** p-values
-   **Prevent** p-hacking and **Recognize** issues with simultaneously
    testing multiple hypotheses.

------------------------------------------------------------------------

# Outline

-   Concepts and Definitions
    -   Simplifying principles: Common tests as linear models
    -   Types of comparisons and statistical tests
    -   Definitions\
-   Day 1: Relationship between two or more continuous variables
    -   Correlation vs Regression
    -   Fitting it a line to data
    -   Linear regression concepts
    -   Multiple regression\
-   Day 2: Relationship between categorical and continuous variables
    -   Comparing means between two groups (t-Test)

    -   Comparing means between 3 or more groups (ANOVA)**\
        **
-   **Day 3: Multiple test correction, Bayesian intro, History**

------------------------------------------------------------------------

# Multiple testing correction

## Genomics -\> Lots of Data -\> Lots of Hypothesis Tests

In a typical RNA-seq experiment, we test \~10K different hypotheses. For
example, you have 10K genes and for each gene you test whether the mean
expression changed in condition A vs condition B. Using a standard
p-value cut-off of 0.05, we'd expect **500 genes** to be deemed
"significant" by chance. Thus, we concerned about **False Positives or
Type I Errors**.

![](img/type-i-and-ii-error-2.png)

So, we want to control our type 1 error. We can approach this in two
different ways.

1.  Control overall α (also known as family-wise error rate or
    [FWER](https://en.wikipedia.org/wiki/Family-wise_error_rate)), which
    will affect the α\* for each test. That is, we are controlling the
    overall probability of making *at least one* false discovery.
    Bonferroni and Sidak corrections all control FWER.

2.  Control [false discovery
    rate](https://en.wikipedia.org/wiki/False_discovery_rate) (FDR).
    Where FWER controls for the probability for making a type 1 error
    *at all*, these procedures allow for type 1 errors (false positives)
    but control the proportion of these false positives in relation to
    true positives. This is done by adjusting the decision made for the
    p-value associated with each individual test to decide rejection or
    not. Because this will result in a higher type 1 error rate, it has
    higher [power](https://en.wikipedia.org/wiki/Power_(statistics)).
    This affords a higher probability of *true discoveries.* The step
    procedures control for FDR.

### Bonferroni Correction

\*\*The most conservative of corrections, the Bonferroni correction is
also perhaps the most straightforward in its approach. Simply divide α
by the number of tests (*m*).

However, with many tests, α\* will become very small. This reduces
power, which means that we are very unlikely to make any true
discoveries.

### Sidak Correction

\*\*α\* = 1-(1-α)\^(1/*m*)

### Holm's Step-Down Procedure

\*\*An update of the Bonferroni correction, this procedure is more
powerful. Rather than controlling the FWER, Holm's procedure controls
for the [false discovery
rate](https://en.wikipedia.org/wiki/False_discovery_rate) (FDR) and is
performed after conducting all hypothesis tests and finding associated
p-values at α within a set.

The step-down procedure is best illustrated with an example. Say we have
three hypotheses, each with the associated p-value:

H1: 0.025\
H2: 0.003\
H3: 0.01

Step 1: Order p-values from smallest to greatest

H2: 0.003\
H3: 0.01\
H1: 0.025

Step 2: Use the Holm-Bonferroni formula for the first-ranked (smallest)
p-value

```         
α* = α/(n-rank+1)
```

α\* = 0.05/(3--1+1) = 0.0167

Step 3: Compare the first-ranked p-value with the α\* calculated from
Step 2

0.003 \< 0.0167

Because the p-value for H2 is less than the calculated α\*, we can
reject H2.

Move onto the next ranked p-value and repeat steps 2--3, calculating the
α\* for its respective rank and comparing it to that p-value. Continue
until you reach the first non-rejected hypothesis. You would then fail
to reject all following hypotheses.

### Hochberg's Step-Up Procedure

\*\*More powerful than Holm's step-down procedure, Hochberg's step-up
procedure also seeks to control the FDR and follows a similar process,
only p-values are ranked from largest to smallest.

For each ranked p-value, it is compared to the α\* calculated for its
respective rank (same formula as Holm's procedure). Testing continues
until you reach the first non-rejected hypothesis. You would then fail
to reject all following hypotheses.

```{r fig.width=10}
rna <- read_csv(file = "data/deltaRNA_test.csv", show_col_types = FALSE) %>%
  select(gene_id, pvalue) %>%
  na.omit()

rna$fdr <- p.adjust(p = rna$pvalue, method = "fdr", n = nrow(rna))

rna$BH <- p.adjust(p = rna$pvalue, method = "BH", n = nrow(rna))

rna$bon <- p.adjust(p = rna$pvalue, method = "bonferroni", n = nrow(rna))


rna_long <- rna %>% pivot_longer(cols = pvalue:bon, names_to = "type")



p_none <- ggplot(data = rna, aes(x = pvalue, y = pvalue)) +
  geom_point(size = .1) +
  ggtitle("None") +
  theme_minimal()


p_FDR <- ggplot(data = rna, aes(x = pvalue, y = fdr)) +
  geom_point(size = .1) +
  ggtitle("FDR") +
  theme_minimal()

p_BH <- ggplot(data = rna, aes(x = pvalue, y = BH)) +
  geom_point(size = .1) +
  ggtitle("BH") +
  theme_minimal()

p_bon <- ggplot(data = rna, aes(x = pvalue, y = bon)) +
  geom_point(size = .1) +
  ggtitle("Bonferroni") +
  theme_minimal()

plot_grid(p_none, p_bon, p_BH, p_FDR, ncol = 2, nrow = 2)

ggplot(data = rna_long, aes(x = value, color = type)) +
  stat_ecdf() +
  theme_minimal() +
  xlab("p-values") +
  ylab("cumulative fraction")
```

# Bayesian Statistics

## Bayes' Rule

The conditional probability of the event $A$ conditional on the event
$B$ is given by $$
  P(A \mid B) = \frac{P(A \,\&\, B)}{P(B)}.
$$

This section introduces how the Bayes' rule is applied to calculating
conditional probability, and several real-life examples are
demonstrated. Finally, we compare the Bayesian and frequentist
definition of probability.

### Bayes' Rule and Diagnostic Testing

To better understand conditional probabilities and their importance, let
us consider an example involving the human immunodeficiency virus (HIV).
In the early 1980s, HIV had just been discovered and was rapidly
expanding. There was major concern with the safety of the blood supply.
Also, virtually no cure existed making an HIV diagnosis basically a
death sentence, in addition to the stigma that was attached to the
disease.

These made false positives and false negatives in HIV testing highly
undesirable. A **false positive** is when a test returns postive while
the truth is negative. That would for instance be that someone without
HIV is wrongly diagnosed with HIV, wrongly telling that person they are
going to die and casting the stigma on them. A **false negative** is
when a test returns negative while the truth is positive. That is when
someone with HIV undergoes an HIV test which wrongly comes back
negative. The latter poses a threat to the blood supply if that person
is about to donate blood.

The probability of a false positive if the truth is negative is called
the false positive rate. Similarly, the false negative rate is the
probability of a false negative if the truth is positive. Note that both
these rates are conditional probabilities: The false positive rate of an
HIV test is the probability of a positive result **conditional on** the
person tested having no HIV.

The HIV test we consider is an enzyme-linked immunosorbent assay,
commonly known as an ELISA. We would like to know the probability that
someone (in the early 1980s) has HIV if ELISA tests positive. For this,
we need the following information. ELISA's true positive rate (one minus
the false negative rate), also referred to as sensitivity, recall, or
probability of detection, is estimated as

$$
  P(\text{ELISA is positive} \mid \text{Person tested has HIV}) = 93\% = 0.93.
$$

Its true negative rate (one minus the false positive rate), also
referred to as specificity, is estimated as

$$
  P(\text{ELISA is negative} \mid \text{Person tested has no HIV}) = 99\% = 0.99.
$$

Also relevant to our question is the prevalence of HIV in the overall
population, which is estimated to be 1.48 out of every 1000 American
adults. We therefore assume

```{=tex}
\begin{equation}
  P(\text{Person tested has HIV}) = \frac{1.48}{1000} = 0.00148.
  (\#eq:HIVpositive)
\end{equation}
```
Note that the above numbers are estimates. For our purposes, however, we
will treat them as if they were exact.

Our goal is to compute the probability of HIV if ELISA is positive, that
is $P(\text{Person tested has HIV} \mid \text{ELISA is positive})$. In
none of the above numbers did we condition on the outcome of ELISA.
Fortunately, Bayes' rule allows is to use the above numbers to compute
the probability we seek. Bayes' rule states that

```{=tex}
\begin{equation}
  P(\text{Person tested has HIV}  \mid \text{ELISA is positive}) = \frac{P(\text{Person tested has HIV} \,\&\, \text{ELISA is positive})}{P(\text{ELISA is positive})}.
   (\#eq:HIVconditional)
\end{equation}
```
This can be derived as follows. For someone to test positive and be HIV
positive, that person first needs to be HIV positive and then secondly
test positive. The probability of the first thing happening is
$P(\text{HIV positive}) = 0.00148$. The probability of then testing
positive is
$P(\text{ELISA is positive} \mid \text{Person tested has HIV}) = 0.93$,
the true positive rate. This yields for the numerator

```{=tex}
\begin{multline}
  P(\text{Person tested has HIV} \,\&\, \text{ELISA is positive}) \\
  \begin{split}
  &= P(\text{Person tested has HIV}) P(\text{ELISA is positive} \mid \text{Person tested has HIV}) \\
  &= 0.00148 \cdot 0.93
  = 0.0013764.
  \end{split}
  (\#eq:HIVjoint)
\end{multline}
```
The first step in the above equation is implied by Bayes' rule: By
multiplying the left- and right-hand side of Bayes' rule by $P(B)$, we
obtain $$
  P(A \mid B) P(B) = P(A \,\&\, B).
$$

The denominator in \@ref(eq:HIVconditional) can be expanded as

```{=tex}
\begin{multline*}
  P(\text{ELISA is positive}) \\
  \begin{split}
  &= P(\text{Person tested has HIV} \,\&\, \text{ELISA is positive}) + P(\text{Person tested has no HIV} \,\&\, \text{ELISA is positive}) \\
  &= 0.0013764 + 0.0099852 = 0.0113616
  \end{split}
\end{multline*}
```
where we used \@ref(eq:HIVjoint) and

```{=tex}
\begin{multline*}
  P(\text{Person tested has no HIV} \,\&\, \text{ELISA is positive}) \\
  \begin{split}
  &= P(\text{Person tested has no HIV}) P(\text{ELISA is positive} \mid \text{Person tested has no HIV}) \\
  &= \left(1 - P(\text{Person tested has HIV})\right) \cdot \left(1 - P(\text{ELISA is negative} \mid \text{Person tested has no HIV})\right) \\
  &= \left(1 - 0.00148\right) \cdot \left(1 - 0.99\right) = 0.0099852.
  \end{split}
\end{multline*}
```
Putting this all together and inserting into \@ref(eq:HIVconditional)
reveals \begin{equation}
  P(\text{Person tested has HIV} \mid \text{ELISA is positive}) = \frac{0.0013764}{0.0113616} \approx 0.12.
  (\#eq:HIVresult)
\end{equation} So even when the ELISA returns positive, the probability
of having HIV is only 12%. An important reason why this number is so low
is due to the prevalence of HIV. Before testing, one's probability of
HIV was 0.148%, so the positive test changes that probability
dramatically, but it is still below 50%. That is, it is more likely that
one is HIV negative rather than positive after one positive ELISA test.

Questions like the one we just answered (What is the probability of a
disease if a test returns positive?) are crucial to make medical
diagnoses. As we saw, just the true positive and true negative rates of
a test do not tell the full story, but also a disease's prevalence plays
a role. Bayes' rule is a tool to synthesize such numbers into a more
useful probability of having a disease after a test result.

```{example}
What is the probability that someone who tests positive does not actually have HIV?
```

We found in \@ref(eq:HIVresult) that someone who tests positive has a
$0.12$ probability of having HIV. That implies that the same person has
a $1-0.12=0.88$ probability of not having HIV, despite testing positive.

```{example}
If the individual is at a higher risk for having HIV than a randomly sampled person from the population considered, how, if at all, would you expect $P(\text{Person tested has HIV} \mid \text{ELISA is positive})$ to change?
```

If the person has a priori a higher risk for HIV and tests positive,
then the probability of having HIV must be higher than for someone not
at increased risk who also tests positive. Therefore,
$P(\text{Person tested has HIV} \mid \text{ELISA is positive}) > 0.12$
where $0.12$ comes from \@ref(eq:HIVresult).

One can derive this mathematically by plugging in a larger number in
\@ref(eq:HIVpositive) than 0.00148, as that number represents the prior
risk of HIV. Changing the calculations accordingly shows
$P(\text{Person tested has HIV} \mid \text{ELISA is positive}) > 0.12$.

```{example}
If the false positive rate of the test is higher than 1%, how, if at all, would you expect $P(\text{Person tested has HIV} \mid \text{ELISA is positive})$ to change?
```

If the false positive rate increases, the probability of a wrong
positive result increases. That means that a positive test result is
more likely to be wrong and thus less indicative of HIV. Therefore, the
probability of HIV after a positive ELISA goes down such that
$P(\text{Person tested has HIV} \mid \text{ELISA is positive}) < 0.12$.

### Bayes Updating

In the previous section, we saw that one positive ELISA test yields a
probability of having HIV of 12%. To obtain a more convincing
probability, one might want to do a second ELISA test after a first one
comes up positive. What is the probability of being HIV positive if also
the second ELISA test comes back positive?

To solve this problem, we will assume that the correctness of this
second test is not influenced by the first ELISA, that is, the tests are
independent from each other. This assumption probably does not hold true
as it is plausible that if the first test was a false positive, it is
more likely that the second one will be one as well. Nonetheless, we
stick with the independence assumption for simplicity.

In the last section, we used
$P(\text{Person tested has HIV}) = 0.00148$, see \@ref(eq:HIVpositive),
to compute the probability of HIV after one positive test. If we repeat
those steps but now with $P(\text{Person tested has HIV}) = 0.12$, the
probability that a person with one positive test has HIV, we exactly
obtain the probability of HIV after two positive tests. Repeating the
maths from the previous section, involving Bayes' rule, gives

```{=tex}
\begin{multline}
  P(\text{Person tested has HIV} \mid \text{Second ELISA is also positive}) \\
  \begin{split}
  &= \frac{P(\text{Person tested has HIV}) P(\text{Second ELISA is positive} \mid \text{Person tested has HIV})}{P(\text{Second ELISA is also positive})} \\
  &= \frac{0.12 \cdot 0.93}{
  \begin{split}
  &P(\text{Person tested has HIV}) P(\text{Second ELISA is positive} \mid \text{Has HIV}) \\
  &+ P(\text{Person tested has no HIV}) P(\text{Second ELISA is positive} \mid \text{Has no HIV})
  \end{split}
  } \\
  &= \frac{0.1116}{0.12 \cdot 0.93 + (1 - 0.12)\cdot (1 - 0.99)} \approx 0.93.
  \end{split}
  (\#eq:Bayes-updating)
\end{multline}
```
Since we are considering the same ELISA test, we used the same true
positive and true negative rates as in Section
\@ref(sec:diagnostic-testing). We see that two positive tests makes it
much more probable for someone to have HIV than when only one test comes
up positive.

This process, of using Bayes' rule to update a probability based on an
event affecting it, is called Bayes' updating. More generally, the what
one tries to update can be considered 'prior' information, sometimes
simply called the **prior**. The event providing information about this
can also be data. Then, updating this prior using Bayes' rule gives the
information conditional on the data, also known as the **posterior**, as
in the information **after** having seen the data. Going from the prior
to the posterior is Bayes updating.

The probability of HIV after one positive ELISA, 0.12, was the posterior
in the previous section as it was an update of the overall prevalence of
HIV, \@ref(eq:HIVpositive). However, in this section we answered a
question where we used this posterior information as the prior. This
process of using a posterior as prior in a new problem is natural in the
Bayesian framework of updating knowledge based on the data.

# Modern Statistics, Beer, and Eugenics

![Image courtesy of the American Philosophical Society, Philadelphia,
USA.](https://media.springernature.com/full/springer-static/image/art%253A10.1038%252F35038589/MediaObjects/41576_2000_Article_BF35038589_Fig1_HTML.gif?as=webp)

## Fathers of statistics

The torch was passed within the triumvirate of Galton, Pearson, and
Fisher.

### Sir Francis Galton (1822-1911)

![from galton.org](https://galton.org/photos/standard-photo.gif)

-   Discovered regression to the mean

-   Re-discovered correlation and regression and discovered how to apply
    these in anthropology, psychology, and more

-   Defined the concept of standard deviation

-   Established the field of Eugenics in 1883

-   Darwin's cousin.

Galton's reasoning for coining the term eugenics:

> "We greatly want a brief word to express the science of improving
> stock, which...takes cognisance of all influences that tend in however
> remote a degree to give the more suitable races or strains of blood a
> better chance of prevailing speedily over the less suitable than they
> otherwise would have had."

### Karl Pearson (1857-1936)

![<https://www.britannica.com/biography/Karl-Pearson>](https://cdn.britannica.com/32/38832-004-24FF6E46/Karl-Pearson-pencil-drawing-FA-de-Biden-1924.jpg?s=1500x700&q=85){width="350"}

Karl Pearson was Galton's protégé and directly or contributed to:

-   Developed hypothesis testing

-   Developed the use of p-values

-   Defined the Chi-Squared test

-   Correlation coefficient

-   Principle components analysis

Also authors of timeless "classics" such as:\
[The Woman's
Question](https://archive.org/details/ethicoffreethoug00pear/page/370/mode/2up?view=theater)

[National Life from the standpoint of
science](https://archive.org/details/nationallifefro00peargoog/page/n12/mode/2up?view=theater)

In the year Mein Kampf was published, Pearson wrote an article called:\
[**THE PROBLEM OF ALIEN IMMIGRATION INTO GREAT BRITAIN, ILLUSTRATED BY
AN EXAMINATION OF RUSSIAN AND POLISH JEWISH
CHILDREN**](https://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1925.tb02037.x)

Here is an excerpt:

> "\[they\] will develop into a parasitic race...Taken *on the average*,
> and regarding both sexes, this alien Jewish population is somewhat
> inferior physically and mentally to the native population."

### Sir Ronald Aylmer Fisher (1890-1962)

![<https://www.42evolution.org/ronald-a-fisher/>](https://www.42evolution.org/wp-content/uploads/2014/07/Ronald-Fisher-from-Royal-Society.jpg)

Fisher's work in statistics established and promoted many important
methods of statistical inference. His contributions include:

-   Establishing p = 0.05 as the normal threshold for significant
    p-values

-   Promoting Maximum Likelihood Estimation

-   Developing the ANalysis Of VAriance (ANOVA) The iris dataset (this
    seems an incredibly minor contribution but I use it daily)

-   [The Genetical Theory of Natural
    Selection](https://en.wikipedia.org/wiki/The_Genetical_Theory_of_Natural_Selection),
    which blended the work of Mendel and Darwin.

There is no lack of Fisher's strong and consistent support for eugenics.
Here is an example from as late as 1954.

![Letter from R.A. Fisher to R. Ruggles Gates. Ronald Fisher Archive.
University of
Adelaide.](https://thisviewoflife.com/wp-content/uploads/2021/04/Fisher-to-Gates-08.27.54-768x534.png)

## Storytime: Galton Laboratory

Galton founded the Eugenics Record Office (1904)

Galton Eugenics Laboratory as part of University College London (UCL).
Created by Pearson and funded by Galton. (1907)

Galton left UCL enough money to create a Chair in National Eugenics,
filled by Pearson and then Fisher. Hell of a name for an endowed chair!

Annals of Human Genetics: It was established in 1925 Pearson as the
Annals of Eugenics, and obtained its current name in 1954.

Galton laboratory was incorporated into the Department of Eugenics,
Biometry and Genetic at UCL in 1944.

Renamed to the Department of Human Genetics and Biometry in 1966.

Became part of the Department of Biology at UCL in 1996.

In 2020: [UCL renames three facilities that honoured prominent
eugenicists](https://www.theguardian.com/education/2020/jun/19/ucl-renames-three-facilities-that-honoured-prominent-eugenicists)

**These horrendous views did not appear to be common at UCL in the
1930s. For example, they were not held by JBS Haldane, Egon Pearson (son
of Karl), and Lionel Penrose.**

### What about in the US?

[CSHL - Eugenics
Archive](http://eugenicsarchive.ca/discover/tree/5233cfa15c2ec500000000ad)

[U.S. Scientists' Role in the Eugenics Movement (1907--1939): A
Contemporary Biologist's
Perspective](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2757926/)

[Charles Davenport (first director of CSHL) and the Carnegie
Insitution](https://carnegiescience.edu/carnegie-institution-science-statement-eugenics-research)

[Cold Spring Harbor and German Eugenics in the
1930s](https://www.cshl.edu/archives/expanded-commentary/cold-spring-harbor-and-german-eugenics-in-the-1930s/)

[Eugenics and the history of Science and
AAAS](https://www.science.org/content/blog-post/eugenics-and-history-science-and-aaas)

## 

![from "America's Shameful History of Eugenics and Forced
Sterilizations"](https://i0.wp.com/www.theifod.com/wp-content/uploads/2021/02/image.png?w=1267&ssl=1)

## Modern day: Eugenics and beyond

### \[Sordid genealogies: a conjectural history of Cambridge Analytica's

eugenic roots\](https://www.nature.com/articles/s41599-020-0505-5)

### [American Renaissance](https://www.amren.com/about/)

![https://sitn.hms.harvard.edu/flash/2017/science-genetics-reshaping-race-debate-21st-century/](https://i0.wp.com/sitn.hms.harvard.edu/wp-content/uploads/2017/04/Fig1-raceConception-2.png){width="350"}\
'Race' cannot be biologically defined due to genetic variation among
human individuals and populations. (A) The old concept of the "five
races:" African, Asian, European, Native American, and Oceanian. (B)
Actual genetic variation in humans.

### **Polygenic Traits, Human Embryos, and Eugenic Dreams**

An academic study debunked the idea of "Screening Human Embryos for
Polygenic Traits," but the CEO of the company Stephen Hsu cofounded
announced that they had screened human embryos for polygenic traits.\
![https://www.geneticsandsociety.org/biopolitical-times/polygenic-traits-human-embryos-and-eugenic-dreams](https://www.geneticsandsociety.org/sites/default/files/styles/teaser/public/screen_shot_2021-09-26_at_5.48.10_pm.png?itok=p9514DCD){width="350"}

[The amoral nonsense of Orchid's embryo
selection](https://liorpachter.wordpress.com/2021/04/12/the-amoral-nonsense-of-orchids-embryo-selection/)\

### **"Superior: The Return of Race Science"**

![<https://en.wikipedia.org/wiki/Superior:_The_Return_of_Race_Science>](https://upload.wikimedia.org/wikipedia/en/9/90/Superior_(book)_cover.jpeg)

### **"Weapons of Math Destruction"**

![<https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction>](https://upload.wikimedia.org/wikipedia/en/0/0b/Weapons_of_Math_Destruction.jpg)

# References and resources

### I have borrowed heavily from, directly taken, and/or highly recommend the following fantastic resources:

## Multiple testing

-   [How does multiple testing correction
    work?](https://www.nature.com/articles/nbt1209-1135)\
-   [Multiple Testing --- How Should You
    Adjust?](https://towardsdatascience.com/multiple-testing-how-should-you-adjust-41048eab4a3d)\
-   [Multiple
    Comparisons](https://benwhalley.github.io/just-enough-r/multiple-comparisons.html)\
-   [An Overview of Methods to Address the Multiple Comparison
    Problem](https://towardsdatascience.com/an-overview-of-methods-to-address-the-multiple-comparison-problem-310427b3ba92)

## Bayesian statistics

-   [High-speed intro to Bayes's
    rule](https://arbital.com/p/bayes_rule/?l=693)

-   [An Introduction to Bayesian
    Thinking](https://statswithr.github.io/book/)

-   [Bayes' Theorem, Clearly
    Explained!!!!](https://www.youtube.com/watch?v=9wCnvr7Xw4E)

## Modern Statistics, Beer, and Eugenics

-   [Why We Might Not Have Statistics Without Guinness
    Brewery](https://www.youtube.com/watch?v=U9Wr7VEPGXA)

-   [Statistics, Eugenics, and
    Me](https://towardsdatascience.com/statistics-eugenics-and-me-29eaf43efac7)

-   [Is Statistics
    Racist?](https://medium.com/swlh/is-statistics-racist-59cd4ddb5fa9)

-   [Beer Vs. Eugenics: The Good And The Bad Uses Of
    Statistics](https://www.forbes.com/sites/jerrybowyer/2016/01/06/beer-vs-eugenics-the-good-and-the-bad-uses-of-statistics/#64831bd32a14)

-   [Engineering American society: the lesson of
    eugenics](https://www.nature.com/articles/35038589)

-   [Eugenics -- journey to the dark side at the dawn of
    statistics](https://www.kdnuggets.com/2016/04/eugenics-journey-dark-side-statistics.html)

-   [How Eugenics Shaped
    Statistics](https://nautil.us/how-eugenics-shaped-statistics-9365/)

-   [Francis Galton's Statistical Ideas: The Influence of
    Eugenics](https://www.jstor.org/stable/229774)

-   [R. A. Fisher: a faith fit for
    eugenics](https://doi.org/10.1016/j.shpsc.2006.12.007)

-   [Sordid genealogies: a conjectural history of Cambridge Analytica's
    eugenic roots](https://www.nature.com/articles/s41599-020-0505-5)

-   [U.S. Scientists' Role in the Eugenics Movement (1907--1939): A
    Contemporary Biologist's
    Perspective](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2757926/)

-   [Biomedical centre memorial to victims of Nazi
    research](https://www.nature.com/articles/35002739)

-   [Berlin Wild---and the Max Delbrück Center for Molecular
    Medicine](https://link.springer.com/article/10.1007/s00109-017-1532-6)

-   [Eugenics
    timeline](https://eugenicsarchive.ca/discover/timeline/543d5ab028f51f0000000003)

-   [Karl Pearson praised Hitler and Nazi Race
    Hygiene](https://profjoecain.net/karl-pearson-praised-hitler-nazi-race-hygiene/)

-   [Ronald Fisher Is Not Being 'Cancelled', But His Eugenic Advocacy
    Should Have
    Consequences](https://thisviewoflife.com/ronald-fisher-is-not-being-cancelled-but-his-eugenic-advocacy-should-have-consequences/)
