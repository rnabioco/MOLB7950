---
title: 'Bootcamp: Stats class 3'
author: "[Neelanjan Mukherjee](neelanjan.mukherjee@cuanschutz.edu)"
date: "[Office Hours](https://calendly.com/molb7950)"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(ggrepel)
library(knitr)
library(ggthemes)
library(cowplot)
library(DT)
```

------------------------------------------------------------------------

# Learning Objectives

-   **Visualize** and **Summarize** the data being compared
-   **Formulate** and **Execute** null hypothesis testing
-   **Identify** and **Perform** the proper statistical test for data
    type/comparison
-   **Calculate** and **Interpret** p-values
-   **Prevent** p-hacking and **Recognize** issues with simultaneously
    testing multiple hypotheses.

------------------------------------------------------------------------

# Outline

-   **Concepts and Definitions**
    -   **Simplifying principles: Common tests as linear models**
    -   **Types of comparisons and statistical tests**
    -   **Definitions\
        **
-   **Day 1: Relationship between two or more continuous variables**
    -   **Correlation vs Regression**
    -   **Fitting it a line to data**
    -   **Linear regression concepts**
    -   **Multiple regression**\
-   Day 2: Relationship between categorical and continuous variables
    -   Comparing means between two groups (t-Test)

    -   Comparing means between 3 or more groups (ANOVA)\
-   Day 3: Multiple test correction, Bayesian intro, History

------------------------------------------------------------------------

## Provide a **simple** and **flexible** framework

![](img/easy_hard.jpg)

------------------------------------------------------------------------

# CHEATSHEET

![](img/linear_tests_cheat_sheet_adapted.png)

------------------------------------------------------------------------

## Variables definitions

### Random variables (x, y)

**Response Variable** ( **y** - aka dependent or outcome variable): this
variable is predicted or its variation is explained by the explanatory
variable. In an experiment, this is the outcome that is measured
following manipulation of the explanatory variable.

**Explanatory Variable** ( **x** - aka independent or predictor
variable): explains variations in the response variable. In an
experiment, it is manipulated by the researcher.

### Quantitative Variables

**Discrete variable**: numeric variables that have a countable number of
values between any two values - `integer` in R (e.g., number of mice,
read counts).

**Continuous variable**: numeric variables that have an infinite number
of values between any two values - `numeric` in R (e.g., normalized
expression values, fluorescent intensity).

### Categorical Variables

**Nominal variable**: (unordered) random variables have categories where
order doesn't matter - `factor` in R (e.g., country, type of gene,
genotype).

**Ordinal variable**: (ordered) random variables have ordered
categories - order of `levels` in R ( e.g. grade of tumor).

------------------------------------------------------------------------

## Hypothesis testing definitions

**Hypothesis testing** is a statistical analysis that uses sample data
to assess two mutually exclusive theories about the properties of a
population. Statisticians call these theories the null hypothesis and
the alternative hypothesis. A hypothesis test assesses your sample
statistic and factors in an estimate of the sample error to determine
which hypothesis the data support.

When you can reject the null hypothesis, the results are statistically
significant, and your data support the theory that an effect exists at
the population level.

[**A legal analogy: Guilty or not
guilty?**](https://www.graphpad.com/guides/prism/latest/statistics/hypothesis_testing_and_statistical_significance.htm)**\
**The statistical concept of 'significant' vs. 'not significant' can be
understood by comparing to the legal concept of 'guilty' vs. 'not
guilty'.

In the American legal system (and much of the world) a criminal
defendant is presumed innocent until proven guilty. If the evidence
proves the defendant guilty beyond a reasonable doubt, the verdict is
'guilty'. Otherwise the verdict is 'not guilty'. In some countries, this
verdict is 'not proven', which is a better description. A 'not guilty'
verdict does not mean the judge or jury concluded that the defendant is
innocent \-- it just means that the evidence was not strong enough to
persuade the judge or jury that the defendant was guilty.

In statistical hypothesis testing, you start with the null hypothesis
(usually that there is no difference between groups). If the evidence
produces a small enough P value, you reject that null hypothesis, and
conclude that the difference is real. If the P value is higher than your
threshold (usually 0.05), you don't reject the null hypothesis. This
doesn't mean the evidence convinced you that the treatment had no
effect, only that the evidence was not persuasive enough to convince you
that there is an effect.

**Effect** --- the difference between the population value and the null
hypothesis value. The effect is also known as population effect or the
difference. Typically, you do not know the size of the actual effect.
However, you can use a hypothesis test to help you determine whether an
effect exists and to estimate its size.

**Null Hypothesis** or $\mathcal{H}_0$ --- one of two mutually exclusive
theories about the properties of the population in hypothesis testing.
Typically, the null hypothesis states that there is no effect (i.e., the
effect size equals zero).

**Alternative Hypothesis** or $\mathcal{H}_1$ --- the other theory about
the properties of the population in hypothesis testing. Typically, the
alternative hypothesis states that a population parameter does not equal
the null hypothesis value. In other words, there is a non-zero effect.
If your sample contains sufficient evidence, you can reject the null and
favor the alternative hypothesis.

**P-values** --- the probability of obtaining test results at least as
extreme as the results actually observed, under the assumption that the
null hypothesis is correct. Lower p-values represent stronger evidence
against the null. P-values in conjunction with the significance level
determines whether your data favor the null or alternative hypothesis.

[StatQuest: P Values, clearly
explained](https://www.youtube.com/watch?v=5Z9OIYA8He8)

[StatQuest: How to calculate
p-values](https://www.youtube.com/watch?v=JQc3yx0-Q9E)

**Significance Level** or $a$ --- an evidentiary standard set before the
study. It is the probability that you say there is an effect when there
is no effect (the probability of rejecting the null hypothesis given
that it is true). Lower significance levels indicate that you require
stronger evidence before you will reject the null.It is usually set at
or below .05.

![Guinness](https://upload.wikimedia.org/wikipedia/commons/3/35/Guinness_Glass_2010.jpg){width="10%"}

------------------------------------------------------------------------

## Null hypothesis testing

1.  Specify the variables
2.  Declare null hypothesis $\mathcal{H}_0$
3.  Calculate test-statistic, exact p-value
4.  *Generate and visualize data reflecting null-distribution*
5.  *Calculate the p-value from the test statistic and null
    distribution*

\*4-5: For calculating empirical p-value

------------------------------------------------------------------------

## The simplicity underlying common tests

Most of the common statistical models (t-test, correlation, ANOVA;
chi-square, etc.) are special cases of linear models or a very close
approximation. This simplicity means that there is less to learn. In
particular, it all comes down to:\
$y = a \cdot x + b$

This needless complexity multiplies when students try to rote learn the
parametric assumptions underlying each test separately rather than
deducing them from the linear model.

------------------------------------------------------------------------

## Parametric vs Non-Parametric tests

**Parametric tests** are suitable for normally distributed data.

**Non-Parametric tests** are suitable for any continuous data. For the
sake of simplicity and sticking with a consistent framework, we will
consider Non-Parametric tests as the **ranked versions of the
corresponding parametric tests**.

[More on choosing Parametric vs
Non-Parametric](https://statisticsbyjim.com/hypothesis-testing/nonparametric-parametric-tests/)

```{r, echo=FALSE}
tibble("Info" = c("better descriptor", "# of samples (N)"), "Parametric" = c("mean", "many"), "Non-Parametric" = c("median  ", "few")) %>%
  kable()
```

------------------------------------------------------------------------

# Import and tidy data

We will be using mouse data from [Resources for Outbred
Mice](https://wp.cs.ucl.ac.uk/outbredmice/). The goal of the study was
to establish genotype-phenotype relationships for highly recombinant
outbred mouse populations. We will be using the phenotypic data for our
exercises.

```{r import data}
# we are reading the data directly from the internet
biochem <- read_tsv("http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/Biochemistry.txt", show_col_types = FALSE) %>%
  janitor::clean_names()

# simplify names a bit more
colnames(biochem) <- gsub(pattern = "biochem_", replacement = "", colnames(biochem))

# we are going to simplify this a bit and only keep some columns
keep <- colnames(biochem)[c(1, 6, 9, 14, 15, 24:28)]
biochem <- biochem[, keep]

# get weights for each individual mouse
# careful: did not come with column names
weight <- read_tsv("http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/weight", col_names = F, show_col_types = FALSE)

# add column names
colnames(weight) <- c("subject_name", "weight")

# add weight to biochem table and get rid of NAs
biochem <- inner_join(biochem, weight, by = "subject_name") %>%
  na.omit()


# explore the data a bit
colnames(biochem)
str(biochem)
# View(biochem)
```

------------------------------------------------------------------------

# Relationship between two or more continuous variables?

What is the relationship between weight and cholesterol?

```{r}
ggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +
  geom_point(size = .5) +
  scale_color_manual() +
  theme_minimal()
```

## Correlation vs Regression

**Correlation** is primarily used to quickly and concisely summarize the
direction and strength of the relationships between a set of 2 or more
numeric variables. 

**Regression** is primarily used to build models/equations to predict a
key response, Y, from a set of predictor (X) variables.

|                                        | Correlation                                                                       | Regression                                                                                               |
|------------------|---------------------------|---------------------------|
| **Description**                        | Association between 2 or more variables                                           | How an independent variable is numerically related to the dependent variable                             |
| **Usage**                              | To represent linear relationship between two variables                            | To fit a best line and estimate one variable on the basis of another variable                            |
| **Dependent vs Independent variables** | Doesn't matter                                                                    | must define (i.e. order of relationship matters)                                                         |
| **Interpretation**                     | Correlation coefficient indicates the extent to which two variables move together | Regression indicates the impact of a unit change in the known variable (x) on the estimated variable (y) |
| **Goal**                               | To find a numerical value expressing the relationship between variables           | To estimate values of random variable on the basis of the values of fixed variable                       |

\*Borrowed from and more info available
[here](https://www.graphpad.com/support/faq/what-is-the-difference-between-correlation-and-linear-regression/)
and
[here](https://keydifferences.com/difference-between-correlation-and-regression.html).

### Pearson Correlation

It was developed by [Karl
Pearson](https://en.wikipedia.org/wiki/Karl_Pearson "Karl Pearson") from
a related idea introduced by [Francis
Galton](https://en.wikipedia.org/wiki/Francis_Galton "Francis Galton")
in the 1880s, and for which the mathematical formula was derived and
published by [Auguste
Bravais](https://en.wikipedia.org/wiki/Auguste_Bravais "Auguste Bravais")
in
1844.^[[a]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-6)[[6]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-7)[[7]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-8)[[8]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-9)[[9]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-10)^
The naming of the coefficient is thus an example of [Stigler's
Law](https://en.wikipedia.org/wiki/Stigler%27s_Law "Stigler's Law") (see
list of examples
[here](https://en.wikipedia.org/wiki/List_of_examples_of_Stigler%27s_law)).

Interpretation of coefficient:

1 = perfect linear correlation

0 = no correlation

-1 = perfect linear anti-correlation

$Corr(x,y) = \displaystyle \frac {\sum_{i=1}^{n} (x_{i} - \overline{x})(y_{i} - \overline{y})}{\sum_{i=1}^{n} \sqrt(x_{i} - \overline{x})^2 \sqrt(y_{i} - \overline{y})^2}$

$x_{i}$ = the "i-th" observation of the variable $x$

$\overline{x}$ = mean of all observations of $x$

$y_{i}$ = the "i-th" observation of the variable $y$

$\overline{y}$ = mean of all observations of $y$

```{r pearson}
r_pearson <- cor(x = biochem$tot_cholesterol, y = biochem$weight)

r_pearson

# average total cholesterol
avg_chol <- mean(biochem$tot_cholesterol)

# average weight
avg_weight <- mean(biochem$weight)

# difference from mean total cholesterol
diff_chol <- biochem$tot_cholesterol - avg_chol

# difference from mean total cholesterol
diff_weight <- biochem$weight - avg_weight

# follow formula above
manual_pearson <- sum(diff_chol * diff_weight) / (
  sqrt(sum(diff_chol^2)) * sqrt(sum(diff_weight^2)))

manual_pearson

identical(manual_pearson, r_pearson)


# cov(x = biochem$tot_cholesterol, y = biochem$weight)/(sd(biochem$tot_cholesterol)*sd(biochem$weight))
```

------------------------------------------------------------------------

### Spearman Correlation (nonparametric)

[Spearman's rank correlation
coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)or
Spearman's ρ, named after Charles Spearman is a **nonparametric**
measure of **rank** correlation (statistical dependence between the
rankings of two variables). It assesses how well the relationship
between two variables can be described using a monotonic function.

More info
[here](https://towardsdatascience.com/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8#:~:text=The%20fundamental%20difference%20between%20the,with%20monotonic%20relationships%20as%20well.).

```{r spearman}
x <- seq(1, 30, 1)
y <- 2^x

plot(x, y, type = "l", las = 2)

cor(x, y, method = "pearson")
cor(x, y, method = "spearman")

cor(x = biochem$tot_cholesterol, y = biochem$weight, method = "spearman")
```

## Regression

## Equation for a line

Remember:\
$y = a \cdot x + b$\
OR\
$y = b + a \cdot x$

$a$ is the **SLOPE**\
$b$ is the **y-intercept**

```{r, echo=FALSE, fig.width=2, fig.height=2}
a <- 2 # slope
b <- 0.5 # y-int

ggplot(data = data.frame(x = c(-1, 3), y = c(-1, 6)), aes(x = x, y = y)) +
  geom_blank() +
  geom_abline(intercept = b, slope = a, col = "red") +
  theme_minimal()
```

$a$ = `r a` (the slope)\
$b$ = `r b` (the y-intercept)

------------------------------------------------------------------------

## Stats equation for a line

Model: the recipe for $y$ is a slope ($\beta_1$) times $x$ plus an
intercept ($\beta_0$).

$y = \beta_0 + \beta_1 x \qquad \qquad \mathcal{H}_0: \beta_1 = 0$

... which is the same has $y = a \cdot x + b$ (here ordered as
$y = b + a \cdot x$). In R we are lazy and write `y ~ 1 + x` which R
reads like `y = 1*number + x*othernumber` and the task of t-tests, lm,
etc., is simply to find the numbers that best predict $y$.

Either way you write it, it's an intercept ($\beta_0$) and a slope
($\beta_1$) yielding a straight line:

```{r, echo=FALSE, fig.width=2, fig.height=2}
ggplot(data = data.frame(x = c(-1, 3), y = c(-1, 6)), aes(x = x, y = y)) +
  geom_blank() +
  geom_abline(intercept = b, slope = a, col = "red") +
  theme_minimal()
```

$\beta_0$ = `r b` (the y-intercept)\
$\beta_1$ = `r a` (the slope)

$y = \beta_0 \cdot 1 + \beta_1 \cdot x$\
$y = .5 \cdot 1 + 2 \cdot x$

Our mission: **FIND THE BEST** $\beta$ coefficients

------------------------------------------------------------------------

## Linear Regression

-   STEP 1: Make a scatter plot visualize the linear relationship
    between x and y.
-   STEP 2: Perform the regression
-   STEP 3: Look at the $R^2$, $F$-value and $p$-value
-   STEP 4: Visualize fit and errors
-   STEP 5: Calculate $R^2$, $F$-value and $p$-value ourselves

------------------------------------------------------------------------

### STEP 1: Can mouse cholesterol levels help explain mouse weight?

Plot weight (y, response variable) and cholesterol levels (x,
explanatory variable) of the mice.

```{r}
ggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +
  geom_point(size = .5) +
  scale_color_manual() +
  theme_minimal()
```

------------------------------------------------------------------------

### STEP 2: Do the regression

Let's fit a line (linear model).

Remember: $y = \beta_0 \cdot 1+ \beta_1 \cdot x$

linear model equation:
$weight = \beta_0 \cdot 1 + \beta_1 \cdot cholesterol$

$\mathcal{H}_0:$ Mouse $cholesterol$ does NOT explain $weight$

Null Hypothesis: $\mathcal{H}_0: \beta_1 = 0$

$weight = \beta_0 \cdot 1 + 0 \cdot cholesterol$

$weight = \beta_0 \cdot 1$

Alternative Hypothesis: $\mathcal{H}_1: \beta_1 \neq 0$\

Full model: $y = \beta_0 \cdot 1 + \beta_1 \cdot x$

$\mathcal{H}_1:$ Mouse $cholesterol$ does explain $weight$

$weight = \beta_0 \cdot 1 + \beta_1 \cdot cholesterol$

The cool thing here is that we can assess and compare our null and
alternative hypothesis by learning and examining the model coefficients
(intercept and slope). Essentially, we are comparing a complex model
(including cholesterol) to a simple model (weight).

<https://statisticsbyjim.com/regression/interpret-constant-y-intercept-regression/>

### STEP 4: Look at the $R^2$, $F$-value and $p$-value

```{r fit}
# fitting a line
fit_WvC <- lm(
  data = biochem,
  formula = weight ~ 1 + tot_cholesterol
)


# base R summary of fit
summary(fit_WvC)
```

That's a lot of info, but how would I access it? Time to meet your new
best friend ---
[Broom](%22https://cran.r-project.org/web/packages/broom/vignettes/broom.html%22)

```{r fit information}
# information about the model fit
glance(fit_WvC)

# information about the intercept and coefficients
tidy(fit_WvC)

chol_intercept <- tidy(fit_WvC)[1, 2]

chol_slope <- tidy(fit_WvC)[2, 2]
# for every 1 unit increase in cholesterol there is a 1.85 unit increase weight


# add residuals and other information
augment(fit_WvC)

# add residuals and other information into the biochem object
biochem_WvC <- augment(fit_WvC, data = biochem)
```

For every 1 unit increase in $cholesterol$ there is a
`r pull(chol_slope)` increase in $weight$.

------------------------------------------------------------------------

### STEP 5: Visualize fit and errors

```{r visualize lm}
ggplot(data = biochem_WvC, aes(y = weight, x = tot_cholesterol)) +
  geom_point(size = .5, col = "white") +
  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = "pink", size = 1) +
  geom_smooth(method = lm, col = "black", se = F, size = 1, linetype = "dashed") +
  scale_color_manual() +
  theme_minimal()


ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +
  geom_point(size = .5, aes(color = .resid)) +
  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = "red") +
  scale_color_gradient2(low = "blue", mid = "black", high = "yellow") + # color code
  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals
  theme_minimal()
```

------------------------------------------------------------------------

STEP 6: Calculate $R^2$, $F$-value and $p$-value ourselves

#### DEFINITIONS

$SS_{mean}$ --- sum of squared errors around the mean of $y$

$SS_{mean} = \sum_{i=1}^{n} (data - mean)^2$\
$SS_{mean} = \sum_{i=1}^{n} (y_{i} - \overline{y})^2$

$Var_{mean}$ --- think of it like the average of the sum of squared
error around the mean of $y$

$Var_{mean} = \displaystyle \frac {\sum_{i=1}^{n} (y_{i} - \overline{y})^2}{n}$

$SS_{fit}$ --- sum of squared errors around the least-squares fit

$SS_{fit} = \sum_{i=1}^{n} (data - line)^2$

$SS_{fit} = \sum_{i=1}^{n} (y_{i} - (\beta_0 \cdot 1+ \beta_1 \cdot x)^2$

**Residuals**, $e$ --- the difference between the observed value of the
dependent variable $y$ and the predicted value $\widehat{y}$ is called
the residual. Each data point has one residual.

$e = y_{i} - \widehat{y}$

$Var_{fit}$ --- think of it like the average of the sum of squared
errors around the least-squares fit

$Var_{fit} = \displaystyle \frac {\sum_{i=1}^{n} (y_{i} - (\beta_0 \cdot 1+ \beta_1 \cdot x))}{n}$

```{r}
fit_W <- lm(formula = weight ~ 1, data = biochem)

summary(fit_W)

biochem_W <- augment(fit_W, data = biochem)
```

#### Super nice way of [visualizing](https://drsimonj.svbtle.com/visualising-residuals)

First, let's look at $SS_{mean}$ for avg weight:

```{r}
p_W <- ggplot(data = biochem_W, aes(x = tot_cholesterol, y = weight)) +
  geom_hline(yintercept = biochem_W$.fitted, col = "red", size = .5) + # plot linear model fit
  geom_point(size = .5, aes(color = .resid)) + # plot height as points and color code by the value of the residual
  scale_color_gradient2(low = "blue", mid = "black", high = "yellow") + # color code for plotting residuals
  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) + # plot line representing residuals
  theme_minimal()
```

Now, let's look at $SS_{fit}$ for the $weight$ \~ $cholesterol$ :

```{r, fig.width=10}
p_WvC <- ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +
  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = "red") +
  geom_point(size = .5, aes(color = .resid)) + # plot height as points and color code by the value of the residual
  scale_color_gradient2(low = "blue", mid = "black", high = "yellow") + # color code for plotting residuals
  # guides(color = FALSE) + # no legend for color code
  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals
  theme_minimal()


plot_grid(p_W, p_WvC, ncol = 2, labels = c("weight by intercept", "weight by cholesterol"))
```

------------------------------------------------------------------------

#### For which mice does the model fit perform the most poorly?

Gotta check residuals!

```{r}
# make new variable exception = absolute value of resid > 9 then subject id



biochem_WvC$exceptions <- if_else(
  condition = abs(biochem_WvC$.resid) < 9,
  true = "",
  false = biochem_WvC$subject_name
)

ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight, label = exceptions)) +
  geom_point(color = ifelse(biochem_WvC$exceptions == "", "grey50", "red")) +
  geom_text_repel() +
  geom_point(size = .5, aes(color = .resid)) + # plot height as points and color code by the value of the residual
  scale_color_gradient2(low = "blue", mid = "black", high = "yellow") + # color code for plotting residuals
  # guides(color = FALSE) + # no legend for color code
  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals
  theme_minimal()
```

------------------------------------------------------------------------

#### Big Baby and KryptoNate

Nate Robinson aka "KryptoNate"\
5' 9"\
180 lbs

![](img/Baby_Nate.jpg){width="15%"}

Glen Davis aka "Big Baby"\
6' 9"\
280 lbs\
![](img/Baby_Nate2.jpg){width="15%"}

------------------------------------------------------------------------

$R^2$ or coefficient of determination --- the proportion of the variance
in the dependent variable that is predictable from the independent
variable(s).

$R^2 = \displaystyle \frac {SS_{mean} - SS_{fit}}{SS_{mean}}$

$R^2 = \displaystyle \frac {SS_{w} - SS_{wc}}{SS_{w}}$

```{r}
ss.fit <- sum(biochem_WvC$.resid^2)


ss.mean <- sum(biochem_W$.resid^2)


# Calc R^2 value
biochem_WvC_rsq <- (ss.mean - ss.fit) / ss.mean
biochem_WvC_rsq

glance(fit_WvC) %>% pull(r.squared)
```

#### Interpretation of $R^2$

There is a `r glance(fit_WvC) %>% pull(r.squared) %>% percent()`
reduction in the variance when we take mouse $cholesterol$ into account\
OR\
Mouse $cholesterol$ explains
`r glance(fit_WvC) %>% pull(r.squared) %>% percent()` in player $weight$

By the way, this is the same $R$ as from the Pearson correlation:

```{r}
# Pearson correlation R value
cor(biochem$weight, biochem$tot_cholesterol, method = "pearson")

# Pearson correlation R^2 value
cor(biochem$weight, biochem$tot_cholesterol, method = "pearson")^2
```

------------------------------------------------------------------------

#### The F-statistic

**F-statistic** --- the proportion of the variance in the dependent
variable that is predictable from the independent variable(s).

$F = \displaystyle \frac{SS_{fit}/(p_{fit}-p_{mean})} {SS_{mean}/(n-p_{fit})}$

$p_{fit}$ --- number of parameters in the fit line\
$p_{mean}$ --- number of parameters in the mean line\
$n$ --- number of data points

```{r}
# F-value
biochem_WvC_F <- ((ss.mean - ss.fit) / (2 - 1)) /
  (ss.fit / (nrow(biochem_WvC) - 2))

biochem_WvC_F

glance(fit_WvC) %>% pull(statistic)
```

#### P-value from the F-statistic

We need to generate a null distribution of $F-statistic$ values to
compare to our observed $F-statistic$.

Therefore, we will randomize the player Height and Weight and then
calculate the $F-statistic$.

![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width="10%"}

</br>

We will do this many many times to generate a null distribution of
$F-statistic$s.\
</br>

![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width="5%"}
![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width="5%"}\
![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width="5%"}
![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width="5%"}
</br>

The p-value will be the probability of obtaining an $F-statistic$ in the
null distribution at least as extreme as our observed $F-statistic$.

Another beautiful Statquest explaining [how to go from F-statistic to
p-value](https://www.youtube.com/watch?v=nk2CQITm_eo)

```{r}
# set up an empty tibble to hold our null distribution
fake_biochem <- tribble()


# sample function to randomize/permute data
sample(x = 1:10)


# we will perform 100 permutations
myPerms <- 100

for (i in 1:myPerms) {
  tmp <- bind_cols(
    biochem_WvC[sample(nrow(biochem_WvC)), "weight"],
    biochem_WvC[sample(nrow(biochem_WvC)), "tot_cholesterol"],
    "perm" = factor(rep(i, nrow(biochem_WvC)))
  )

  fake_biochem <- bind_rows(fake_biochem, tmp)
  rm(tmp)
}


# let's look at permutations 1 and 2
ggplot(fake_biochem %>% filter(perm %in% c(1:2)), aes(x = weight, y = tot_cholesterol, color = perm)) +
  geom_point(size = .1) +
  theme_minimal()
```

#### Remember your best friend

[BROOM](https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html)

```{r}
# here we will calculate and extract linear model results for each permutation individualy using nest, mutate, and map functions

fake_biochem_lms <- fake_biochem %>%
  nest(data = -perm) %>%
  mutate(
    fit = map(data, ~ lm(weight ~ tot_cholesterol, data = .x)),
    glanced = map(fit, glance)
  ) %>%
  unnest(glanced)
```

#### Let's take a look at the null distribution of F-statistics from the randomized values

```{r}
fake_biochem_lms %>%
  ggplot(., aes(x = statistic)) +
  geom_density(color = "red") +
  theme_minimal()
```

remember that the $F-statistic$ we observed was
\~`r round(biochem_WvC_F)`!

```{r}
fake_biochem_lms %>%
  ggplot(., aes(x = statistic)) +
  xlim(0, biochem_WvC_F * 1.1) +
  geom_density(color = "red") +
  geom_vline(xintercept = biochem_WvC_F, color = "blue") +
  #  scale_x_log10() +
  theme_minimal()

glance(fit_WvC) %>% pull(p.value)
```

#### Interpretation of $p-value$

There is no value more extreme than our observed $F-statistic$.\
Therefore, the empirical $p-value < 0.001$ --- empirical what we
calculated by randomizing our data.

The exact $p-value$ is `r glance(fit_WvC) %>% pull(p.value)` AKA *"a
ridiculously small number"*.

**Correct** --- Assuming that $cholesterol$ has zero effect on $weight$
in the population, you'd obtain the sample effect, or larger, in
`r glance(fit_WvC) %>% pull(p.value)` AKA *"a ridiculously small
number"* of studies because of random sample error.

**Incorrect** ---There's a `r glance(fit_WvC) %>% pull(p.value)` AKA *"a
ridiculously small number"* chance of making a mistake by rejecting the
null hypothesis.

More on [p-value
(mis)interpretation](https://statisticsbyjim.com/hypothesis-testing/interpreting-p-values/)

------------------------------------------------------------------------

### How to find the best (least squares) fit?

1.  Rotate the line of fit\
2.  Find the fit that minimizes the Sum of Squared Residuals or
    $SS_{fit}$\
3.  This is the derivative (slope of tangent at best point = 0) of the
    function describing the $SS_{fit}$ and the next rotation is 0.

[StatQuest: Fitting a line to data, aka least squares, aka linear
regression.](https://youtu.be/PaFPbb66DxQ)

[StatQuest: Gradient Descent,
Step-by-Step](https://youtu.be/sDv4f4s2SB8)

------------------------------------------------------------------------

### Non-parametric version

#### Theory: rank-transformation {#rank}

`rank` simply takes a list of numbers and "replace" them with the
integers of their rank (1st smallest, 2nd smallest, 3rd smallest, etc.).
So the result of the rank-transformation `rank(c(3.6, 3.4, -5.0, 8.2))`
is `3, 2, 1, 4`. See that in the figure above?

A *signed* rank is the same, just where we rank according to absolute
size first and then add in the sign second. So the signed rank here
would be `2, 1, -3, 4`. Or in code:

```{r}
npfit_WvC <- lm(formula = rank(weight) ~ 1 + rank(tot_cholesterol), data = biochem)

tidy(npfit_WvC)

glance(npfit_WvC) %>% pull(r.squared)

# cor(biochem$weight, biochem$tot_cholesterol, method = "spearman")^2
```

------------------------------------------------------------------------

## Multiple regression

Remember:

$y = \beta_0 \cdot 1+ \beta_1 \cdot x$

Let's add an explanatory variable:

$y = \beta_0 \cdot 1 + \beta_1 \cdot x + \beta_2 \cdot z$

linear model equation:
$weight = \beta_0 \cdot 1 + \beta_1 \cdot cholesterol + \beta_2 \cdot sodium$

$\mathcal{H}_0:$ Mouse $cholesterol$ and $sodium$ does NOT explain
$weight$

Null Hypothesis: $\mathcal{H}_0: \beta_1, \beta_2 = 0$

$weight = \beta_0 \cdot 1 + 0 \cdot cholesterol + 0 \cdot sodium$

$weight = \beta_0 \cdot 1$

Alternative Hypothesis: $\mathcal{H}_1: \beta_1,\beta_2 \neq 0$\

Full model: $y = \beta_0 \cdot 1 + \beta_1 \cdot x + \beta_2 \cdot z$\
$\mathcal{H}_1:$ Mouse $cholesterol$ and $sodium$ does explain $weight$

$weight = \beta_0 \cdot 1 + \beta_1 \cdot cholesterol + \beta_2 \cdot sodium$

```{r multiple regression}
# how do different variables correlate with weight?
biochem %>%
  select_if(is.numeric) %>%
  cor() %>%
  as.data.frame() %>%
  select(weight) %>%
  arrange(-weight) %>%
  rownames()


# does sodium + cholesterol predict weight better than cholesterol alone?
fit_WvC_S <- lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + sodium)


fit_WvC_S %>% glance()

biochem_WvC_S <- augment(fit_WvC_S, data = biochem)

ss.fit_C_S <- sum(biochem_WvC_S$.resid^2)

var.fit_C_S <- ss.fit_C_S / nrow(biochem_WvC_S)


ss.fit_C <- sum(biochem_WvC$.resid^2)

var.fit_C <- ss.fit_C / nrow(biochem_WvC)



fit_Wvall <- lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + age + sodium + glucose + calcium + litter + cage_density)

fit_Wvall %>% glance()
fit_Wvall %>% tidy()



ggplot(data = biochem, aes(y = weight, x = calcium)) +
  geom_point(size = .5) +
  scale_color_manual() +
  theme_minimal()
```

------------------------------------------------------------------------

## Non-linear regression

**Non-linear regression** --- observational data are modeled by a
function which is a nonlinear combination of the model parameters and
depends on one or more independent variables. The data are fitted by a
method of successive approximations.

**DANGER OVERFITTING**

#### Perform loess regression

**Loess regression** --- a non-parametric technique that uses local
weighted regression to fit a smooth curve through points in a scatter
plot. LOESS combines much of the simplicity of linear least squares
regression with the flexibility of nonlinear regression. It does this by
fitting simple models to localized subsets of the data to build up a
function that describes the deterministic part of the variation in the
data, point by point. In fact, one of the chief attractions of this
method is that the data analyst is not required to specify a global
function of any form to fit a model to the data, only to fit segments of
the data.

```{r}
# local-weighted regression fit
loessfit_WvC <- loess(formula = weight ~ tot_cholesterol, data = biochem)

loess_biochem_WvC <- augment(loessfit_WvC, biochem)


summary(loessfit_WvC)
summary(fit_WvC)
```

#### Plot loess fit depicting residuals

```{r}
ggplot(data = loess_biochem_WvC, aes(x = tot_cholesterol, y = weight)) +
  geom_smooth(method = lm, col = "black", se = F, size = .25, linetype = "dashed") + # linear fit black dashed line
  geom_smooth(method = loess, col = "red", se = F, size = .25) + # loess fit red line
  geom_point(size = .5, aes(color = .resid)) +
  scale_color_gradient2(low = "blue", mid = "black", high = "yellow") +
  # guides(color = FALSE) +
  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) +
  theme_minimal()
```

------------------------------------------------------------------------

# References and resources

I have borrowed heavily from, directly taken, and/or highly recommend
the following fantastic resources:

1.  [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/) from Jonas
    Kristoffer Lindeløv\
2.  [Statquest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)
3.  [Stats gobbledygook](https://www.rapidtables.com/math/symbols/Statistical_Symbols.html)
4.  [Linear Regression Assumptions and Diagnostics in R: Essentials](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/)\
5.  [PRINCIPLES OF STATISTICS](https://www.graphpad.com/guides/prism/latest/statistics/stat_---_principles_of_statistics_-.htm)
    from GraphPad/SAS.
