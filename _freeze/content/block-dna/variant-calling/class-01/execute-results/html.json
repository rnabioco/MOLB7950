{
  "hash": "883897d5347cf17f6b28f8972fcbe2a0",
  "result": {
    "markdown": "---\ntitle: \"Genome sequencing 1 - theory\"\nauthor: \"Your name here\"\n---\n\n\n\n\n# Variant Calling --- theory\n\n## Reading\n\n-   [VCF format](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3137218/)\n-   [Framework for variation discovery](https://www.nature.com/articles/ng.806)\n\n------------------------------------------------------------------------\n\n## Learning objectives\n\n1.  Learn to think about allele sampling as a binomial process\n2.  Understand the implications of the binomial process for variant detection\n3.  Learn to think about sequencing coverage as a Poisson distribution\n4.  Understand the implications of the Poisson distribution on experimental design and cost.\n\n## Allele sampling and the binomial distribution\n\nAllele sampling can be modeled as a coin flipping exercise.\n\nWith fair coin, the $P(heads)$ = 0.5 and $P(tails)$ = 0.5.\n\nWith fair genome sequencing of heterozygous C/T position, P(C) = 0.5 and P(T) = 0.5\n\nThe probability of `k` successes in `n` trials is given by the probability mass function:\n\n$$ Pr(X=k) = {{n}\\choose{k}} p^k(1-p)^{n-k} $$\n\nUsing this, we can ask: what is the probability of seeing $k = 1$ tails in $n = 3$ flips of a fair coin with $P(tail)$ = 0.5?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchoose(3, 1) * 0.5**1 * (1 - 0.5)**(3 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.375\n```\n:::\n\n```{.r .cell-code}\n# or . . .\ndbinom(x = 1, size = 3, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.375\n```\n:::\n:::\n\n\n## Simulating sequencing outcomes\n\nLet's say we sequence my genome to 30x coverage, i.e. each site is covered by a mean of 30 reads.\n\nEach read is a random sample from millions of double-stranded DNA fragments. After read alignment, is every site going to be covered by 30 reads?\n\n**No!** Some sites have 17 reads aligned, some have 35. And this randomness is the problem we're trying to address.\n\nWe model the alleles at each site as a binomial random variable that can hold one of two values. Think heads and tails of a coin, or reference and alternative alleles at a site.\n\nWe can use `rbinom()` to look at the distribution of outcomes.\n\n-   `n`: number of people tossing coins, or sites in the genome\n-   `size`: number of times you looked at the coin, or looked at a site with a sequencing read, i.e. the sequencing coverage\n-   `prob`: probability of success (i.e., probability of tails with a fair coin)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 30\nsize <- 5\nprob <- 0.5\n\nbarplot(table(rbinom(n, size, prob)))\n```\n\n::: {.cell-output-display}\n![](class-01_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n### Coin tossing exercise\n\n**Ready your coin!**\n\nWe're going to play a game.\n\nImagine you are all different sites in the genome. Your true identity (your genotype) is hidden. We are going to discover your true identity by flipping a coin.\n\nFor each set of coin flips, record the number of alternative alleles (tails) you see.\n\nFor those of you that are HETEROZYGOUS, we will learn your identity ONLY after 50% of your flips are alternative alleles (3 at toss6, 6 at toss12, 16 at toss32)\n\nEnter the number of times you see tails after flipping 6, 12, and 32 times on this spreadsheet:\n\n<https://docs.google.com/spreadsheets/d/149q8wg6ctodnjZxnDj1SWUIw_Bjgvf9T0iVAvud0dVI/edit?usp=sharing>\n\n#### BUT WAIT I FORGOT MY COIN\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(c(\"REF\", \"ALT\"), 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ALT\"\n```\n:::\n:::\n\n\n## Comparing theory with real data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_tosses <- function(x) {\n  # returns a plot\n  ggplot(fct_count(as.factor(x)), aes(f, n)) +\n    geom_col() +\n    labs(\n      x = \"Number of tails (alternative alleles)\",\n      y = \"count\"\n    ) +\n    theme_minimal_grid()\n}\n```\n:::\n\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 6 tosses?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# theory\nn <- 30\nsize <- 5\nprob <- 0.5\n\n# plot of counts\nx <- rbinom(n, size, prob)\nplot_tosses(x)\n```\n\n::: {.cell-output-display}\n![](class-01_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# real data\n```\n:::\n\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 12 tosses?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# theory\n# theory\nn <- 30\nsize <- 15\nprob <- 0.5\n\n# plot of counts\nx <- rbinom(n, size, prob)\nplot_tosses(x)\n```\n\n::: {.cell-output-display}\n![](class-01_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# real data\n```\n:::\n\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 32 tosses?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# theory\n\n# real data\n```\n:::\n\n\n### Assumptions\n\nThe binomial makes two assumptions:\n\n1.  The probability of success is the same in each trial (i.e. coin flip).\n2.  The trials are independent and do not affect each others' outcome.\n\nWhen might these assumptions be violated by real genome sequencing data?\n\n**Examine the heterozygous alignment slide**\n\n## Reproducibility with seeds (short aside)\n\nIn the chunks above, each time we run `rbinom()` we get a different random sample. We can make this sample reproducible by setting a seed just before each random call:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# not reproducible\nx <- rbinom(10, 30, 0.5)\ny <- rbinom(10, 30, 0.5)\n\nall(x == y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n\n```{.r .cell-code}\n# reproducible\nset.seed(1)\nx <- rbinom(10, 30, 0.5)\nset.seed(1)\ny <- rbinom(10, 30, 0.5)\n\nall(x == y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nWhat happens if you change the seed value above? I.e. Compare the values produced after `set.seed(1)` and `set.seed(42)`.\n\nThe setting of seeds is a valuable tool if you want to generate a completely reproducible pipeline for others to run.\n\nFor example, the process of generating two-dimensional UMAP (or t-SNE) projection is a random process. You can recover the same projection each time by setting a seed at the appropriate step.\n\n## Excercises\n\n1.  What is the probability of seeing 10 or more tails in 20 flips of a fair coin? \\[*0.5880985*\\]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(dbinom(10:20, 20, 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5880985\n```\n:::\n:::\n\n\n2.  What is the probability of seeing 10 or fewer alternative alleles at 17-fold coverage, assuming equal allele probabilities? \\[*0.833847*\\]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(dbinom(0:10, 17, 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.833847\n```\n:::\n:::\n\n\n3.  Generate a random sample of 100 numbers **unif**ormly distributed between 0 and 1. Now generate two more identical samples. Use `all()` to confirm whether the samples are identical .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunif(100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 0.20597457 0.17655675 0.68702285 0.38410372 0.76984142 0.49769924\n  [7] 0.71761851 0.99190609 0.38003518 0.77744522 0.93470523 0.21214252\n [13] 0.65167377 0.12555510 0.26722067 0.38611409 0.01339033 0.38238796\n [19] 0.86969085 0.34034900 0.48208012 0.59956583 0.49354131 0.18621760\n [25] 0.82737332 0.66846674 0.79423986 0.10794363 0.72371095 0.41127443\n [31] 0.82094629 0.64706019 0.78293276 0.55303631 0.52971958 0.78935623\n [37] 0.02333120 0.47723007 0.73231374 0.69273156 0.47761962 0.86120948\n [43] 0.43809711 0.24479728 0.07067905 0.09946616 0.31627171 0.51863426\n [49] 0.66200508 0.40683019 0.91287592 0.29360337 0.45906573 0.33239467\n [55] 0.65087047 0.25801678 0.47854525 0.76631067 0.08424691 0.87532133\n [61] 0.33907294 0.83944035 0.34668349 0.33377493 0.47635125 0.89219834\n [67] 0.86433947 0.38998954 0.77732070 0.96061800 0.43465948 0.71251468\n [73] 0.39999437 0.32535215 0.75708715 0.20269226 0.71112122 0.12169192\n [79] 0.24548851 0.14330438 0.23962942 0.05893438 0.64228826 0.87626921\n [85] 0.77891468 0.79730883 0.45527445 0.41008408 0.81087024 0.60493329\n [91] 0.65472393 0.35319727 0.27026015 0.99268406 0.63349326 0.21320814\n [97] 0.12937235 0.47811803 0.92407447 0.59876097\n```\n:::\n\n```{.r .cell-code}\nset.seed(1)\nx <- runif(100)\n\nset.seed(1)\ny <- runif(100)\n\nall(x == y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n4.  Thought experiment: You create a new organism with an expanded genetic code containing a third base-pair (i.e, A:T, G:C, X:Y) that follows Chargaff's rule. Does this expanded code change how you might approach sequencing its genome? What if the ploidy of this organism was larger, say 45N (like *Tetrahymena*)?\n\n## Sequencing Coverage\n\nFrom binomial theory, we learned we need a certain level of coverage to generate reliable variant calls, due to the randomness of sampling alleles at a site using sequencing.\n\nWe also learned that reads are not distributed randomly in a genome. Coverage is itself a distribution, and we typically discuss it's mean. I.e. 30-fold coverage means *on average* each base is sequenced 30 times.\n\nHow do we estimate the numbers of times a base is expected to be sequenced given a certain level of coverage?\n\nThis theory comes from [Lander and Waterman](https://pubmed.ncbi.nlm.nih.gov/3294162/), who made two assumptions about sequencing:\n\n1.  Reads will be distributed randomly across the genome.\n2.  The ability to detect overlaps (alignments) doesn't vary between reads\n\nThey concluded that read coverage is modeled by the Poisson distribution.\n\nThe Poisson probability function is:\n\n$$ P(Y = y) = \\frac{{ e^{ - C } C ^ y }} {{ y!}} $$ where:\n\n1.  **`y` is the number of times a base is read.** It is the exact number of times a base is sequenced.\n2.  **`C` is the mean coverage** (lambda in the traditional Poisson sense). It's the mean number of aligned reads covering a site.\n\nThe formula above gives the probability of a base being sequenced a certain number of times.\n\nFor example, what is the probability of a base being sequenced *3 times or less* at at mean coverage of 10?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- 0:3\nC <- 10\n\nsum(dpois(y, C))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01033605\n```\n:::\n\n```{.r .cell-code}\n# or ...\nppois(3, C)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01033605\n```\n:::\n:::\n\n\n### Excercises\n\nWhat if we're trying to characterize heterozygous alleles in a human genome at differing levels of coverage?\n\n1.  How many variants are in an average human genome? \\[Answer *4285714*\\]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# size (bp) of the haploid human genome\nG <- 3e9\n# chance of seeing a common variant in a random person. comes from e.g.\n# https://www.nature.com/articles/nature15393\np_var <- 1 / 700\n\n# calculate number of variants\nn_var <- G * p_var\nn_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4285714\n```\n:::\n:::\n\n\n2.  How many of these variants have zero coverage (i.e., no reads covering those sites) after sequencing to mean coverage of 5, 15, 30-fold? \\[Answer *28876.92, 1.61, 4.01041e-07*\\].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppois(0, 5) * n_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28876.92\n```\n:::\n\n```{.r .cell-code}\nppois(0, 15) * n_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.31101\n```\n:::\n\n```{.r .cell-code}\nppois(0, 30) * n_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.01041e-07\n```\n:::\n:::\n\n\n### A simple formula for coverage\n\nThis theory boils down to a general equation, the Lander/Waterman equation, for computing the coverage you have given a certain level of sequencing:\n\n$$ C = LN / G $$\n\nwhere:\n\n-   `C` is coverage. I.e., the mean number of times each base is covered by a read.\n-   `G` is the haploid genome length. E.g., 3e9 for the human genome.\n-   `L` is the read length. On Illumina, typically 300 bp.\n-   `N` is the number of reads.\n\nLet's use this equation to calculate the coverage for a few different genomes, given fixed read number and length.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nG_yeast <- 12e6\nG_human <- 3e9\n\nL <- 300\nN <- 20e6\n\nC_yeast <- (L * N) / G_yeast\nC_human <- (L * N) / G_human\n```\n:::\n\n\nObviously those parameters provide a powerful approach for yeast genetics, but much less so for human genetics.\n\n### Cost of a human genome sequencing experiment\n\nOn an Illumina Novaseq 6000, each 300 bp read costs about 3\\times 10^{-5} USD.\n\nHow much would it cost to sequence a single human at 30X coverage? Does that number surprise you?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many reads do we need?\n# note that each read is 300 bp (150 bp from each end of a fragment)\nn <- (30 * 3e9) / 300\n\n# how much does each read cost?\ncost <- 3e4 / 10e9\n\nn * cost\n```\n:::\n\n\n## Sequencing error rate\n\nError rate of sequencing can have a big impact on variant interpretation.\n\nIllumina error rates are relatively low, ONT rates are relatively high. We'll look at this by pulling quality scores from a 1,000 reads from the Illumina and ONT platforms.\n\nFirst, read data from the FASTQ files. This is not rectangular data, so we'll use Python.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport gzip\nfrom collections import Counter\n\ndef qual_counts(fq_gz):\n  quals = Counter()\n  nl = 0\n  with gzip.open(fq_gz, 'rb') as fq:\n    for line in fq:\n      if nl % 4 == 3:\n        quals.update(line.strip())\n      nl += 1\n  return quals      \n\nillumina_quals = qual_counts('data/illumina.fastq.gz')\nont_quals = qual_counts('data/ont.fastq.gz')\n\nfh = open('quals.tsv', 'w')\nfor qual, count in illumina_quals.items():\n  print(qual, count, 'illumina', file=fh, sep='\\t')\nfor qual, count in ont_quals.items():\n  print(qual, count, 'ont', file=fh, sep='\\t')\nfh.close()\n```\n:::\n\n\nThe counts are in TSV format now, back to R (phew).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab <- read_tsv(\"quals.tsv\", col_names = c(\"qual\", \"count\", \"type\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 59 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (1): type\ndbl (2): qual, count\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nggplot(tab, aes(qual, count, fill = type, alpha = 0.2)) +\n  geom_col() +\n  labs(\n    title = \"Quality score comparison Illumina and ONT reads\",\n    subtitle = \"Thoughts?\"\n  ) +\n  theme_minimal_grid() +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](class-01_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## Excercises\n\n1.  Why don't we just sequence everybody to, like, 1000-fold coverage, and not worry about binomial sampling theory?\n\n    Determine the number of reads needed to sequence the human genome to 1000X coverage given the data below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# standard Illumina read length\nL <- 300\n# size (bp) of haploid human genome\nG <- 3e9\n\n# Use the Lander-Waterman equation to solve for read number:\n```\n:::\n\n\nNow figure out how much this experiment would cost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How much would this cost?\nusd_per_bp <- 30e4 / 10e9\n```\n:::\n\n\nHow might you dramatically reduce the cost of this experiment, but retain much of the useful information acquired from the genome? I.e., how would you \"zoom in\" to informative portions of the genome?\n\n2.  Thought experiment: The organism you're studying happens to be tetraploid (like salmon). You want to use genome sequencing to identify variants in an individual. How does this change the way you think about allele sampling and coverage?\n\n3.  Thought experiment: How does this theory impact your thinking in designing an experiment where coverage variation along the genome is the **signal** (e.g., chromatin immunoprecipitation + sequencing).\n\n    Given what you learned about read coverage sampling, what statistical models do you think \"peak calling\" algorithms use?\n\n## Resources\n\nThe slides and content around the theory of variant sampling were borrowed from Aaron Quinlan's [Applied Computational Genomics course](https://github.com/quinlan-lab/applied-computational-genomics).\n",
    "supporting": [
      "class-01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}