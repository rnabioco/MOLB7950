{
  "hash": "ff8c33acf245264a18b580f37d808594",
  "result": {
    "markdown": "---\ntitle: 'Bootcamp: Stats class 3'\nauthor: \"[Neelanjan Mukherjee](neelanjan.mukherjee@cuanschutz.edu)\"\ndate: \"[Office Hours](https://calendly.com/molb7950)\"\noutput:\n  html_document: default\n  pdf_document: default\neditor_options: \n  chunk_output_type: console\n  markdown: \n    wrap: 72\n---\n\n\n\n\n------------------------------------------------------------------------\n\n# Learning Objectives\n\n-   **Visualize** and **Summarize** the data being compared\n-   **Formulate** and **Execute** null hypothesis testing\n-   **Identify** and **Perform** the proper statistical test for data\n    type/comparison\n-   **Calculate** and **Interpret** p-values\n-   **Prevent** p-hacking and **Recognize** issues with simultaneously\n    testing multiple hypotheses.\n\n------------------------------------------------------------------------\n\n# Outline\n\n-   **Concepts and Definitions**\n    -   **Simplifying principles: Common tests as linear models**\n    -   **Types of comparisons and statistical tests**\n    -   **Definitions\\\n        **\n-   **Day 1: Relationship between two or more continuous variables**\n    -   **Correlation vs Regression**\n    -   **Fitting it a line to data**\n    -   **Linear regression concepts**\n    -   **Multiple regression**\\\n-   Day 2: Relationship between categorical and continuous variables\n    -   Comparing means between two groups (t-Test)\n\n    -   Comparing means between 3 or more groups (ANOVA)\\\n-   Day 3: Multiple test correction, Bayesian intro, History\n\n------------------------------------------------------------------------\n\n## Provide a **simple** and **flexible** framework\n\n![](img/easy_hard.jpg)\n\n------------------------------------------------------------------------\n\n# CHEATSHEET\n\n![](img/linear_tests_cheat_sheet_adapted.png)\n\n------------------------------------------------------------------------\n\n## Variables definitions\n\n### Random variables (x, y)\n\n**Response Variable** ( **y** - aka dependent or outcome variable): this\nvariable is predicted or its variation is explained by the explanatory\nvariable. In an experiment, this is the outcome that is measured\nfollowing manipulation of the explanatory variable.\n\n**Explanatory Variable** ( **x** - aka independent or predictor\nvariable): explains variations in the response variable. In an\nexperiment, it is manipulated by the researcher.\n\n### Quantitative Variables\n\n**Discrete variable**: numeric variables that have a countable number of\nvalues between any two values - `integer` in R (e.g., number of mice,\nread counts).\n\n**Continuous variable**: numeric variables that have an infinite number\nof values between any two values - `numeric` in R (e.g., normalized\nexpression values, fluorescent intensity).\n\n### Categorical Variables\n\n**Nominal variable**: (unordered) random variables have categories where\norder doesn't matter - `factor` in R (e.g., country, type of gene,\ngenotype).\n\n**Ordinal variable**: (ordered) random variables have ordered\ncategories - order of `levels` in R ( e.g. grade of tumor).\n\n------------------------------------------------------------------------\n\n## Hypothesis testing definitions\n\n**Hypothesis testing** is a statistical analysis that uses sample data\nto assess two mutually exclusive theories about the properties of a\npopulation. Statisticians call these theories the null hypothesis and\nthe alternative hypothesis. A hypothesis test assesses your sample\nstatistic and factors in an estimate of the sample error to determine\nwhich hypothesis the data support.\n\nWhen you can reject the null hypothesis, the results are statistically\nsignificant, and your data support the theory that an effect exists at\nthe population level.\n\n[**A legal analogy: Guilty or not\nguilty?**](https://www.graphpad.com/guides/prism/latest/statistics/hypothesis_testing_and_statistical_significance.htm)**\\\n**The statistical concept of 'significant' vs. 'not significant' can be\nunderstood by comparing to the legal concept of 'guilty' vs. 'not\nguilty'.\n\nIn the American legal system (and much of the world) a criminal\ndefendant is presumed innocent until proven guilty. If the evidence\nproves the defendant guilty beyond a reasonable doubt, the verdict is\n'guilty'. Otherwise the verdict is 'not guilty'. In some countries, this\nverdict is 'not proven', which is a better description. A 'not guilty'\nverdict does not mean the judge or jury concluded that the defendant is\ninnocent \\-- it just means that the evidence was not strong enough to\npersuade the judge or jury that the defendant was guilty.\n\nIn statistical hypothesis testing, you start with the null hypothesis\n(usually that there is no difference between groups). If the evidence\nproduces a small enough P value, you reject that null hypothesis, and\nconclude that the difference is real. If the P value is higher than your\nthreshold (usually 0.05), you don't reject the null hypothesis. This\ndoesn't mean the evidence convinced you that the treatment had no\neffect, only that the evidence was not persuasive enough to convince you\nthat there is an effect.\n\n**Effect** --- the difference between the population value and the null\nhypothesis value. The effect is also known as population effect or the\ndifference. Typically, you do not know the size of the actual effect.\nHowever, you can use a hypothesis test to help you determine whether an\neffect exists and to estimate its size.\n\n**Null Hypothesis** or $\\mathcal{H}_0$ --- one of two mutually exclusive\ntheories about the properties of the population in hypothesis testing.\nTypically, the null hypothesis states that there is no effect (i.e., the\neffect size equals zero).\n\n**Alternative Hypothesis** or $\\mathcal{H}_1$ --- the other theory about\nthe properties of the population in hypothesis testing. Typically, the\nalternative hypothesis states that a population parameter does not equal\nthe null hypothesis value. In other words, there is a non-zero effect.\nIf your sample contains sufficient evidence, you can reject the null and\nfavor the alternative hypothesis.\n\n**P-values** --- the probability of obtaining test results at least as\nextreme as the results actually observed, under the assumption that the\nnull hypothesis is correct. Lower p-values represent stronger evidence\nagainst the null. P-values in conjunction with the significance level\ndetermines whether your data favor the null or alternative hypothesis.\n\n[StatQuest: P Values, clearly\nexplained](https://www.youtube.com/watch?v=5Z9OIYA8He8)\n\n[StatQuest: How to calculate\np-values](https://www.youtube.com/watch?v=JQc3yx0-Q9E)\n\n**Significance Level** or $a$ --- an evidentiary standard set before the\nstudy. It is the probability that you say there is an effect when there\nis no effect (the probability of rejecting the null hypothesis given\nthat it is true). Lower significance levels indicate that you require\nstronger evidence before you will reject the null.It is usually set at\nor below .05.\n\n![Guinness](https://upload.wikimedia.org/wikipedia/commons/3/35/Guinness_Glass_2010.jpg){width=\"10%\"}\n\n------------------------------------------------------------------------\n\n## Null hypothesis testing\n\n1.  Specify the variables\n2.  Declare null hypothesis $\\mathcal{H}_0$\n3.  Calculate test-statistic, exact p-value\n4.  *Generate and visualize data reflecting null-distribution*\n5.  *Calculate the p-value from the test statistic and null\n    distribution*\n\n\\*4-5: For calculating empirical p-value\n\n------------------------------------------------------------------------\n\n## The simplicity underlying common tests\n\nMost of the common statistical models (t-test, correlation, ANOVA;\nchi-square, etc.) are special cases of linear models or a very close\napproximation. This simplicity means that there is less to learn. In\nparticular, it all comes down to:\\\n$y = a \\cdot x + b$\n\nThis needless complexity multiplies when students try to rote learn the\nparametric assumptions underlying each test separately rather than\ndeducing them from the linear model.\n\n------------------------------------------------------------------------\n\n## Parametric vs Non-Parametric tests\n\n**Parametric tests** are suitable for normally distributed data.\n\n**Non-Parametric tests** are suitable for any continuous data. For the\nsake of simplicity and sticking with a consistent framework, we will\nconsider Non-Parametric tests as the **ranked versions of the\ncorresponding parametric tests**.\n\n[More on choosing Parametric vs\nNon-Parametric](https://statisticsbyjim.com/hypothesis-testing/nonparametric-parametric-tests/)\n\n\n::: {.cell}\n::: {.cell-output-display}\n|Info              |Parametric |Non-Parametric |\n|:-----------------|:----------|:--------------|\n|better descriptor |mean       |median         |\n|# of samples (N)  |many       |few            |\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Import and tidy data\n\nWe will be using mouse data from [Resources for Outbred\nMice](https://wp.cs.ucl.ac.uk/outbredmice/). The goal of the study was\nto establish genotype-phenotype relationships for highly recombinant\noutbred mouse populations. We will be using the phenotypic data for our\nexercises.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we are reading the data directly from the internet\nbiochem <- read_tsv(\"http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/Biochemistry.txt\", show_col_types = FALSE) %>%\n  janitor::clean_names()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00C4>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00D6>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00DC>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00E4>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00F6>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00FC>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00DF>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00C6>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00E6>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00D8>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00F8>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00C5>' to native encoding\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in FUN(X[[i]], ...): unable to translate '<U+00E5>' to native encoding\n```\n:::\n\n```{.r .cell-code}\n# simplify names a bit more\ncolnames(biochem) <- gsub(pattern = \"biochem_\", replacement = \"\", colnames(biochem))\n\n# we are going to simplify this a bit and only keep some columns\nkeep <- colnames(biochem)[c(1,6,9,14,15,24:28)]\nbiochem <- biochem[,keep]\n\n# get weights for each individual mouse\n# careful: did not come with column names\nweight <- read_tsv(\"http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/weight\", col_names = F, show_col_types = FALSE)\n\n# add column names\ncolnames(weight) <- c(\"subject_name\",\"weight\")\n\n# add weight to biochem table and get rid of NAs\nbiochem <- inner_join(biochem, weight, by=\"subject_name\") %>%\n  na.omit()\n\n\n# explore the data a bit\ncolnames(biochem)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"subject_name\"    \"calcium\"         \"glucose\"         \"sodium\"         \n [5] \"tot_cholesterol\" \"family\"          \"gender\"          \"age\"            \n [9] \"cage_density\"    \"litter\"          \"weight\"         \n```\n:::\n\n```{.r .cell-code}\nstr(biochem)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [1,782 x 11] (S3: tbl_df/tbl/data.frame)\n $ subject_name   : chr [1:1782] \"A048005080\" \"A048005112\" \"A048006555\" \"A048007096\" ...\n $ calcium        : num [1:1782] 1.97 2.11 1.71 2.49 2.14 2.16 2.29 1.69 1.84 2.31 ...\n $ glucose        : num [1:1782] 12.23 10.99 5.97 10.61 11.88 ...\n $ sodium         : num [1:1782] 123 133 119 148 131 134 146 117 121 144 ...\n $ tot_cholesterol: num [1:1782] 3.01 2.46 3.57 2.61 2.04 2.86 3.22 3.47 3.35 2.29 ...\n $ family         : chr [1:1782] \"H2.3:C5.2(3) H2.3:G2.2(3)\" \"H2.2:C3.1(4) H2.2:G3.1(3)\" \"E1.3:H1.2(3) E1.3:D1.2(3)\" \"D3.2:G2.1(5) D3.2:C5.1(5)\" ...\n $ gender         : chr [1:1782] \"F\" \"F\" \"M\" \"M\" ...\n $ age            : num [1:1782] 66 70 72 66 63 72 66 72 64 66 ...\n $ cage_density   : num [1:1782] 5 3 4 4 6 7 4 5 6 3 ...\n $ litter         : num [1:1782] 2 4 1 4 1 3 1 1 1 1 ...\n $ weight         : num [1:1782] 20.3 16.1 19.5 22.2 17.3 18.1 25.6 18.6 23.1 17.3 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:237] 3 8 11 17 18 23 25 35 38 40 ...\n  ..- attr(*, \"names\")= chr [1:237] \"3\" \"8\" \"11\" \"17\" ...\n```\n:::\n\n```{.r .cell-code}\n# View(biochem)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# Relationship between two or more continuous variables?\n\nWhat is the relationship between weight and cholesterol?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Correlation vs Regression\n\n**Correlation** is primarily used to quickly and concisely summarize the\ndirection and strength of the relationships between a set of 2 or more\nnumeric variables.<c2><a0>\n\n**Regression** is primarily used to build models/equations to predict a\nkey response, Y, from a set of predictor (X) variables.\n\n|                                        | Correlation                                                                       | Regression                                                                                               |\n|------------------|---------------------------|---------------------------|\n| **Description**                        | Association between 2 or more variables                                           | How an independent variable is numerically related to the dependent variable                             |\n| **Usage**                              | To represent linear relationship between two variables                            | To fit a best line and estimate one variable on the basis of another variable                            |\n| **Dependent vs Independent variables** | Doesn't matter                                                                    | must define (i.e. order of relationship matters)                                                         |\n| **Interpretation**                     | Correlation coefficient indicates the extent to which two variables move together | Regression indicates the impact of a unit change in the known variable (x) on the estimated variable (y) |\n| **Goal**                               | To find a numerical value expressing the relationship between variables           | To estimate values of random variable on the basis of the values of fixed variable                       |\n\n\\*Borrowed from and more info available\n[here](https://www.graphpad.com/support/faq/what-is-the-difference-between-correlation-and-linear-regression/)\nand\n[here](https://keydifferences.com/difference-between-correlation-and-regression.html).\n\n### Pearson Correlation\n\nIt was developed by [Karl\nPearson](https://en.wikipedia.org/wiki/Karl_Pearson \"Karl Pearson\") from\na related idea introduced by [Francis\nGalton](https://en.wikipedia.org/wiki/Francis_Galton \"Francis Galton\")\nin the 1880s, and for which the mathematical formula was derived and\npublished by [Auguste\nBravais](https://en.wikipedia.org/wiki/Auguste_Bravais \"Auguste Bravais\")\nin\n1844.^[[a]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-6)[[6]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-7)[[7]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-8)[[8]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-9)[[9]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#cite_note-10)^\nThe naming of the coefficient is thus an example of [Stigler's\nLaw](https://en.wikipedia.org/wiki/Stigler%27s_Law \"Stigler's Law\") (see\nlist of examples\n[here](https://en.wikipedia.org/wiki/List_of_examples_of_Stigler%27s_law)).\n\nInterpretation of coefficient:\n\n1 = perfect linear correlation\n\n0 = no correlation\n\n-1 = perfect linear anti-correlation\n\n$Corr(x,y) = \\displaystyle \\frac {\\sum_{i=1}^{n} (x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i=1}^{n} \\sqrt(x_{i} - \\overline{x})^2 \\sqrt(y_{i} - \\overline{y})^2}$\n\n$x_{i}$ = the \"i-th\" observation of the variable $x$\n\n$\\overline{x}$ = mean of all observations of $x$\n\n$y_{i}$ = the \"i-th\" observation of the variable $y$\n\n$\\overline{y}$ = mean of all observations of $y$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_pearson <- cor(x = biochem$tot_cholesterol, y = biochem$weight)\n\nr_pearson\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3540731\n```\n:::\n\n```{.r .cell-code}\n# average total cholesterol\navg_chol <- mean(biochem$tot_cholesterol)\n\n# average weight\navg_weight <- mean(biochem$weight)\n\n# difference from mean total cholesterol\ndiff_chol <- biochem$tot_cholesterol - avg_chol\n\n# difference from mean total cholesterol\ndiff_weight <- biochem$weight - avg_weight\n\n# follow formula above\nmanual_pearson <- sum(diff_chol*diff_weight)/(\nsqrt(sum(diff_chol^2))*sqrt(sum(diff_weight^2)))\n\nmanual_pearson\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3540731\n```\n:::\n\n```{.r .cell-code}\nidentical(manual_pearson, r_pearson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n\n```{.r .cell-code}\n# cov(x = biochem$tot_cholesterol, y = biochem$weight)/(sd(biochem$tot_cholesterol)*sd(biochem$weight))\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### Spearman Correlation (nonparametric)\n\n[Spearman's rank correlation\ncoefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)or\nSpearman's <cf><81>, named after Charles Spearman is a **nonparametric**\nmeasure of **rank** correlation (statistical dependence between the\nrankings of two variables). It assesses how well the relationship\nbetween two variables can be described using a monotonic function.\n\nMore info\n[here](https://towardsdatascience.com/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8#:~:text=The%20fundamental%20difference%20between%20the,with%20monotonic%20relationships%20as%20well.).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(1,30, 1)\ny <- 2^x\n\nplot(x,y, type = \"l\", las=2)\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/spearman-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(x,y, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5199042\n```\n:::\n\n```{.r .cell-code}\ncor(x,y, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\ncor(x = biochem$tot_cholesterol, y = biochem$weight, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3596281\n```\n:::\n:::\n\n\n## Regression\n\n## Equation for a line\n\nRemember:\\\n$y = a \\cdot x + b$\\\nOR\\\n$y = b + a \\cdot x$\n\n$a$ is the **SLOPE**\\\n$b$ is the **y-intercept**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-3-1.png){width=192}\n:::\n:::\n\n\n$a$ = 2 (the slope)\\\n$b$ = 0.5 (the y-intercept)\n\n------------------------------------------------------------------------\n\n## Stats equation for a line\n\nModel: the recipe for $y$ is a slope ($\\beta_1$) times $x$ plus an\nintercept ($\\beta_0$).\n\n$y = \\beta_0 + \\beta_1 x \\qquad \\qquad \\mathcal{H}_0: \\beta_1 = 0$\n\n... which is the same has $y = a \\cdot x + b$ (here ordered as\n$y = b + a \\cdot x$). In R we are lazy and write `y ~ 1 + x` which R\nreads like `y = 1*number + x*othernumber` and the task of t-tests, lm,\netc., is simply to find the numbers that best predict $y$.\n\nEither way you write it, it's an intercept ($\\beta_0$) and a slope\n($\\beta_1$) yielding a straight line:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-4-1.png){width=192}\n:::\n:::\n\n\n$\\beta_0$ = 0.5 (the y-intercept)\\\n$\\beta_1$ = 2 (the slope)\n\n$y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x$\\\n$y = .5 \\cdot 1 + 2 \\cdot x$\n\nOur mission: **FIND THE BEST** $\\beta$ coefficients\n\n------------------------------------------------------------------------\n\n## Linear Regression\n\n-   STEP 1: Make a scatter plot visualize the linear relationship\n    between x and y.\n-   STEP 2: Perform the regression\n-   STEP 3: Look at the $R^2$, $F$-value and $p$-value\n-   STEP 4: Visualize fit and errors\n-   STEP 5: Calculate $R^2$, $F$-value and $p$-value ourselves\n\n------------------------------------------------------------------------\n\n### STEP 1: Can mouse cholesterol levels help explain mouse weight?\n\nPlot weight (y, response variable) and cholesterol levels (x,\nexplanatory variable) of the mice.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### STEP 2: Do the regression\n\nLet's fit a line (linear model).\n\nRemember: $y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x$\n\nlinear model equation:\n$weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol$\n\n$\\mathcal{H}_0:$ Mouse $cholesterol$ does NOT explain $weight$\n\nNull Hypothesis: $\\mathcal{H}_0: \\beta_1 = 0$\n\n$weight = \\beta_0 \\cdot 1 + 0 \\cdot cholesterol$\n\n$weight = \\beta_0 \\cdot 1$\n\nAlternative Hypothesis: $\\mathcal{H}_1: \\beta_1 \\neq 0$\\\n\nFull model: $y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x$\n\n$\\mathcal{H}_1:$ Mouse $cholesterol$ does explain $weight$\n\n$weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol$\n\nThe cool thing here is that we can assess and compare our null and\nalternative hypothesis by learning and examining the model coefficients\n(intercept and slope). Essentially, we are comparing a complex model\n(including cholesterol) to a simple model (weight).\n\n<https://statisticsbyjim.com/regression/interpret-constant-y-intercept-regression/>\n\n### STEP 4: Look at the $R^2$, $F$-value and $p$-value\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fitting a line\nfit_WvC <- lm(data = biochem,\n              formula = weight ~ 1 + tot_cholesterol)\n\n\n# base R summary of fit\nsummary(fit_WvC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ 1 + tot_cholesterol, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9624 -2.1349 -0.2627  2.0113 10.2927 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      14.5560     0.3635   40.04   <2e-16 ***\ntot_cholesterol   1.8516     0.1159   15.97   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 1780 degrees of freedom\nMultiple R-squared:  0.1254,\tAdjusted R-squared:  0.1249 \nF-statistic: 255.1 on 1 and 1780 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThat's a lot of info, but how would I access it? Time to meet your new\nbest friend ---\n[Broom](%22https://cran.r-project.org/web/packages/broom/vignettes/broom.html%22)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# information about the model fit\nglance(fit_WvC) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.125         0.125  3.12      255. 8.92e-54     1 -4556. 9117. 9134.\n# i 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\n# information about the intercept and coefficients\ntidy(fit_WvC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 5\n  term            estimate std.error statistic   p.value\n  <chr>              <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        14.6      0.363      40.0 1.48e-250\n2 tot_cholesterol     1.85     0.116      16.0 8.92e- 54\n```\n:::\n\n```{.r .cell-code}\nchol_intercept <- tidy(fit_WvC)[1,2]\n\nchol_slope <- tidy(fit_WvC)[2,2]\n# for every 1 unit increase in cholesterol there is a 1.85 unit increase weight \n\n\n# add residuals and other information\naugment(fit_WvC) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,782 x 8\n   weight tot_cholesterol .fitted .resid     .hat .sigma     .cooksd .std.resid\n    <dbl>           <dbl>   <dbl>  <dbl>    <dbl>  <dbl>       <dbl>      <dbl>\n 1   20.3            3.01    20.1  0.171 0.000566   3.12 0.000000846     0.0547\n 2   16.1            2.46    19.1 -3.01  0.00107    3.12 0.000501       -0.965 \n 3   19.5            3.57    21.2 -1.67  0.000906   3.12 0.000129       -0.534 \n 4   22.2            2.61    19.4  2.81  0.000853   3.12 0.000347        0.901 \n 5   17.3            2.04    18.3 -1.03  0.00203    3.12 0.000111       -0.331 \n 6   18.1            2.86    19.9 -1.75  0.000622   3.12 0.0000981      -0.561 \n 7   25.6            3.22    20.5  5.08  0.000592   3.12 0.000786        1.63  \n 8   18.6            3.47    21.0 -2.38  0.000782   3.12 0.000228       -0.763 \n 9   23.1            3.35    20.8  2.34  0.000669   3.12 0.000189        0.750 \n10   17.3            2.29    18.8 -1.50  0.00140    3.12 0.000161       -0.480 \n# i 1,772 more rows\n```\n:::\n\n```{.r .cell-code}\n# add residuals and other information into the biochem object\nbiochem_WvC <- augment(fit_WvC, data = biochem)\n```\n:::\n\n\nFor every 1 unit increase in $cholesterol$ there is a\n1.8516281 increase in $weight$.\n\n------------------------------------------------------------------------\n\n### STEP 5: Visualize fit and errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = biochem_WvC, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size=.5, col=\"white\") +\n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"pink\", size=1) +\n  geom_smooth(method=lm, col = \"black\", se = F, size=1, linetype=\"dashed\") +\n  scale_color_manual() +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/visualize lm-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_point(size=.5, aes(color = .resid)) + \n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"red\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/visualize lm-2.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nSTEP 6: Calculate $R^2$, $F$-value and $p$-value ourselves\n\n#### DEFINITIONS\n\n$SS_{mean}$ --- sum of squared errors around the mean of $y$\n\n$SS_{mean} = \\sum_{i=1}^{n} (data - mean)^2$\\\n$SS_{mean} = \\sum_{i=1}^{n} (y_{i} - \\overline{y})^2$\n\n$Var_{mean}$ --- think of it like the average of the sum of squared\nerror around the mean of $y$\n\n$Var_{mean} = \\displaystyle \\frac {\\sum_{i=1}^{n} (y_{i} - \\overline{y})^2}{n}$\n\n$SS_{fit}$ --- sum of squared errors around the least-squares fit\n\n$SS_{fit} = \\sum_{i=1}^{n} (data - line)^2$\n\n$SS_{fit} = \\sum_{i=1}^{n} (y_{i} - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot x)^2$\n\n**Residuals**, $e$ --- the difference between the observed value of the\ndependent variable $y$ and the predicted value $\\widehat{y}$ is called\nthe residual. Each data point has one residual.\n\n$e = y_{i} - \\widehat{y}$\n\n$Var_{fit}$ --- think of it like the average of the sum of squared\nerrors around the least-squares fit\n\n$Var_{fit} = \\displaystyle \\frac {\\sum_{i=1}^{n} (y_{i} - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot x))}{n}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_W <- lm(formula = weight ~ 1, data = biochem)\n\nsummary(fit_W)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ 1, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3409 -2.4409 -0.4409  2.2591  9.9591 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 20.24091    0.07903   256.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.336 on 1781 degrees of freedom\n```\n:::\n\n```{.r .cell-code}\nbiochem_W <- augment(fit_W, data = biochem)\n```\n:::\n\n\n#### Super nice way of [visualizing](https://drsimonj.svbtle.com/visualising-residuals)\n\nFirst, let's look at $SS_{mean}$ for avg weight:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_W <- ggplot(data = biochem_W, aes(x = tot_cholesterol, y = weight)) +\n  geom_hline(yintercept = biochem_W$.fitted, col = \"red\", size=.5) + # plot linear model fit\n  geom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) + # plot line representing residuals\n  theme_minimal()\n```\n:::\n\n\nNow, let's look at $SS_{fit}$ for the $weight$ \\~ $cholesterol$ :\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_WvC <- ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"red\") +\n  geom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  # guides(color = FALSE) + # no legend for color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\nplot_grid(p_W, p_WvC, ncol = 2, labels = c(\"weight by intercept\",\"weight by cholesterol\"))\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n#### For which mice does the model fit perform the most poorly?\n\nGotta check residuals!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make new variable exception = absolute value of resid > 9 then subject id\n\n\n\nbiochem_WvC$exceptions <- if_else(\n  condition = abs(biochem_WvC$.resid) < 9,\n  true = \"\",\n  false = biochem_WvC$subject_name\n  )\n\nggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight, label = exceptions)) +\n  geom_point(color = ifelse(biochem_WvC$exceptions == \"\", \"grey50\", \"red\")) +\n  geom_text_repel() +\ngeom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  # guides(color = FALSE) + # no legend for color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n#### Big Baby and KryptoNate\n\nNate Robinson aka \"KryptoNate\"\\\n5' 9\"\\\n180 lbs\n\n![](img/Baby_Nate.jpg){width=\"15%\"}\n\nGlen Davis aka \"Big Baby\"\\\n6' 9\"\\\n280 lbs\\\n![](img/Baby_Nate2.jpg){width=\"15%\"}\n\n------------------------------------------------------------------------\n\n$R^2$ or coefficient of determination --- the proportion of the variance\nin the dependent variable that is predictable from the independent\nvariable(s).\n\n$R^2 = \\displaystyle \\frac {SS_{mean} - SS_{fit}}{SS_{mean}}$\n\n$R^2 = \\displaystyle \\frac {SS_{w} - SS_{wc}}{SS_{w}}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nss.fit <- sum(biochem_WvC$.resid^2)\n\n\nss.mean <- sum(biochem_W$.resid^2)\n\n\n# Calc R^2 value\nbiochem_WvC_rsq <- (ss.mean - ss.fit) / ss.mean \nbiochem_WvC_rsq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1253678\n```\n:::\n\n```{.r .cell-code}\nglance(fit_WvC) %>% pull(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1253678\n```\n:::\n:::\n\n\n#### Interpretation of $R^2$\n\nThere is a 13%\nreduction in the variance when we take mouse $cholesterol$ into account\\\nOR\\\nMouse $cholesterol$ explains\n13% in player $weight$\n\nBy the way, this is the same $R$ as from the Pearson correlation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pearson correlation R value\ncor(biochem$weight, biochem$tot_cholesterol, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3540731\n```\n:::\n\n```{.r .cell-code}\n# Pearson correlation R^2 value\ncor(biochem$weight, biochem$tot_cholesterol, method = \"pearson\")^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1253678\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n#### The F-statistic\n\n**F-statistic** --- the proportion of the variance in the dependent\nvariable that is predictable from the independent variable(s).\n\n$F = \\displaystyle \\frac{SS_{fit}/(p_{fit}-p_{mean})} {SS_{mean}/(n-p_{fit})}$\n\n$p_{fit}$ --- number of parameters in the fit line\\\n$p_{mean}$ --- number of parameters in the mean line\\\n$n$ --- number of data points\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# F-value\nbiochem_WvC_F <- ((ss.mean - ss.fit) / (2 - 1)) / \n  (ss.fit / (nrow(biochem_WvC) - 2))\n\nbiochem_WvC_F \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 255.1411\n```\n:::\n\n```{.r .cell-code}\nglance(fit_WvC) %>% pull(statistic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   value \n255.1411 \n```\n:::\n:::\n\n\n#### P-value from the F-statistic\n\nWe need to generate a null distribution of $F-statistic$ values to\ncompare to our observed $F-statistic$.\n\nTherefore, we will randomize the player Height and Weight and then\ncalculate the $F-statistic$.\n\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"10%\"}\n\n</br>\n\nWe will do this many many times to generate a null distribution of\n$F-statistic$s.\\\n</br>\n\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"}\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"}\\\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"}\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"}\n</br>\n\nThe p-value will be the probability of obtaining an $F-statistic$ in the\nnull distribution at least as extreme as our observed $F-statistic$.\n\nAnother beautiful Statquest explaining [how to go from F-statistic to\np-value](https://www.youtube.com/watch?v=nk2CQITm_eo)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up an empty tibble to hold our null distribution\nfake_biochem <- tribble()\n\n\n# sample function to randomize/permute data\nsample(x = 1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  2  8  3  1  5  6  9  7  4 10\n```\n:::\n\n```{.r .cell-code}\n# we will perform 100 permutations\nmyPerms <- 100\n\nfor (i in 1:myPerms) {\n\n  tmp <- bind_cols(\n    biochem_WvC[sample(nrow(biochem_WvC)), \"weight\"],\n    biochem_WvC[sample(nrow(biochem_WvC)), \"tot_cholesterol\"],\n    \"perm\"=factor(rep(i,nrow(biochem_WvC)))\n    )\n\n  fake_biochem <- bind_rows(fake_biochem,tmp)\n  rm(tmp)\n\n}\n\n\n# let's look at permutations 1 and 2\nggplot(fake_biochem %>% filter(perm %in% c(1:2)), aes(x=weight, y=tot_cholesterol, color=perm)) +\n  geom_point(size=.1) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n#### Remember your best friend\n\n[BROOM](https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# here we will calculate and extract linear model results for each permutation individualy using nest, mutate, and map functions\n\nfake_biochem_lms <- fake_biochem %>%\n  nest(data = -perm) %>%\n  mutate(\n    fit = map(data, ~ lm(weight ~ tot_cholesterol, data = .x)),\n    glanced = map(fit, glance)\n  ) %>%\n  unnest(glanced)\n```\n:::\n\n\n#### Let's take a look at the null distribution of F-statistics from the randomized values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_biochem_lms %>%\n  ggplot(., aes(x = statistic)) +\n  geom_density(color=\"red\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nremember that the $F-statistic$ we observed was\n\\~255!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_biochem_lms %>%\n  ggplot(., aes(x = statistic)) +\n  xlim(0,biochem_WvC_F*1.1) +\n  geom_density(color=\"red\") +\n  geom_vline(xintercept = biochem_WvC_F, color = \"blue\") +\n#  scale_x_log10() +\n  theme_minimal() \n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nglance(fit_WvC) %>% pull(p.value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       value \n8.924155e-54 \n```\n:::\n:::\n\n\n#### Interpretation of $p-value$\n\nThere is no value more extreme than our observed $F-statistic$.\\\nTherefore, the empirical $p-value < 0.001$ --- empirical what we\ncalculated by randomizing our data.\n\nThe exact $p-value$ is 8.9241551\\times 10^{-54} AKA *\"a\nridiculously small number\"*.\n\n**Correct** --- Assuming that $cholesterol$ has zero effect on $weight$\nin the population, you'd obtain the sample effect, or larger, in\n8.9241551\\times 10^{-54} AKA *\"a ridiculously small\nnumber\"* of studies because of random sample error.\n\n**Incorrect** ---There's a 8.9241551\\times 10^{-54} AKA *\"a\nridiculously small number\"* chance of making a mistake by rejecting the\nnull hypothesis.\n\nMore on [p-value\n(mis)interpretation](https://statisticsbyjim.com/hypothesis-testing/interpreting-p-values/)\n\n------------------------------------------------------------------------\n\n### How to find the best (least squares) fit?\n\n1.  Rotate the line of fit\\\n2.  Find the fit that minimizes the Sum of Squared Residuals or\n    $SS_{fit}$\\\n3.  This is the derivative (slope of tangent at best point = 0) of the\n    function describing the $SS_{fit}$ and the next rotation is 0.\n\n[StatQuest: Fitting a line to data, aka least squares, aka linear\nregression.](https://youtu.be/PaFPbb66DxQ)\n\n[StatQuest: Gradient Descent,\nStep-by-Step](https://youtu.be/sDv4f4s2SB8)\n\n------------------------------------------------------------------------\n\n### Non-parametric version\n\n#### Theory: rank-transformation {#rank}\n\n`rank` simply takes a list of numbers and \"replace\" them with the\nintegers of their rank (1st smallest, 2nd smallest, 3rd smallest, etc.).\nSo the result of the rank-transformation `rank(c(3.6, 3.4, -5.0, 8.2))`\nis `3, 2, 1, 4`. See that in the figure above?\n\nA *signed* rank is the same, just where we rank according to absolute\nsize first and then add in the sign second. So the signed rank here\nwould be `2, 1, -3, 4`. Or in code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnpfit_WvC <- lm(formula = rank(weight) ~ 1 + rank(tot_cholesterol), data = biochem)\n\ntidy(npfit_WvC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 5\n  term                  estimate std.error statistic   p.value\n  <chr>                    <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)            571.      22.8         25.1 4.01e-119\n2 rank(tot_cholesterol)    0.360    0.0221      16.3 1.54e- 55\n```\n:::\n\n```{.r .cell-code}\nglance(npfit_WvC) %>% pull(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1293324\n```\n:::\n\n```{.r .cell-code}\n# cor(biochem$weight, biochem$tot_cholesterol, method = \"spearman\")^2\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Multiple regression\n\nRemember:\n\n$y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x$\n\nLet's add an explanatory variable:\n\n$y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z$\n\nlinear model equation:\n$weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium$\n\n$\\mathcal{H}_0:$ Mouse $cholesterol$ and $sodium$ does NOT explain\n$weight$\n\nNull Hypothesis: $\\mathcal{H}_0: \\beta_1, \\beta_2 = 0$\n\n$weight = \\beta_0 \\cdot 1 + 0 \\cdot cholesterol + 0 \\cdot sodium$\n\n$weight = \\beta_0 \\cdot 1$\n\nAlternative Hypothesis: $\\mathcal{H}_1: \\beta_1,\\beta_2 \\neq 0$\\\n\nFull model: $y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z$\\\n$\\mathcal{H}_1:$ Mouse $cholesterol$ and $sodium$ does explain $weight$\n\n$weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how do different variables correlate with weight?\nbiochem %>%\n  select_if(is.numeric) %>%\n  cor() %>%\n  as.data.frame %>%\n  select(weight) %>%\n  arrange(-weight) %>% rownames()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"weight\"          \"tot_cholesterol\" \"age\"             \"sodium\"         \n[5] \"glucose\"         \"calcium\"         \"litter\"          \"cage_density\"   \n```\n:::\n\n```{.r .cell-code}\n# does sodium + cholesterol predict weight better than cholesterol alone?\nfit_WvC_S <-  lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + sodium)  \n\n\nfit_WvC_S %>% glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.171         0.170  3.04      183. 3.72e-73     2 -4508. 9024. 9046.\n# i 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nbiochem_WvC_S <- augment(fit_WvC_S, data = biochem)\n\nss.fit_C_S <- sum(biochem_WvC_S$.resid^2)\n\nvar.fit_C_S <- ss.fit_C_S/nrow(biochem_WvC_S)\n \n\nss.fit_C <- sum(biochem_WvC$.resid^2)\n\nvar.fit_C <- ss.fit_C/nrow(biochem_WvC)\n\n\n\nfit_Wvall <-  lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + age + sodium + glucose + calcium + litter + cage_density)  \n\nfit_Wvall %>% glance() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.253         0.250  2.89      85.7 1.33e-107     7 -4415. 8849. 8898.\n# i 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nfit_Wvall %>% tidy() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      -6.53      1.68       -3.90 9.98e- 5\n2 tot_cholesterol   1.97      0.109      18.2  9.08e-68\n3 age               0.133     0.0161      8.28 2.49e-16\n4 sodium            0.179     0.0145     12.4  1.12e-33\n5 glucose           0.159     0.0284      5.59 2.59e- 8\n6 calcium          -5.65      0.692      -8.16 6.13e-16\n7 litter           -0.0557    0.0523     -1.06 2.88e- 1\n8 cage_density     -0.316     0.0648     -4.87 1.22e- 6\n```\n:::\n\n```{.r .cell-code}\nggplot(data = biochem, aes(y = weight, x = calcium)) +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/multiple regression-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Non-linear regression\n\n**Non-linear regression** --- observational data are modeled by a\nfunction which is a nonlinear combination of the model parameters and\ndepends on one or more independent variables. The data are fitted by a\nmethod of successive approximations.\n\n**DANGER OVERFITTING**\n\n#### Perform loess regression\n\n**Loess regression** --- a non-parametric technique that uses local\nweighted regression to fit a smooth curve through points in a scatter\nplot. LOESS combines much of the simplicity of linear least squares\nregression with the flexibility of nonlinear regression. It does this by\nfitting simple models to localized subsets of the data to build up a\nfunction that describes the deterministic part of the variation in the\ndata, point by point. In fact, one of the chief attractions of this\nmethod is that the data analyst is not required to specify a global\nfunction of any form to fit a model to the data, only to fit segments of\nthe data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# local-weighted regression fit\nloessfit_WvC <- loess(formula = weight ~  tot_cholesterol, data = biochem)\n\nloess_biochem_WvC <- augment(loessfit_WvC, biochem) \n\n\nsummary(loessfit_WvC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nloess(formula = weight ~ tot_cholesterol, data = biochem)\n\nNumber of Observations: 1782 \nEquivalent Number of Parameters: 5.24 \nResidual Standard Error: 3.112 \nTrace of smoother matrix: 5.73  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate\t  cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n```\n:::\n\n```{.r .cell-code}\nsummary(fit_WvC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ 1 + tot_cholesterol, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9624 -2.1349 -0.2627  2.0113 10.2927 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      14.5560     0.3635   40.04   <2e-16 ***\ntot_cholesterol   1.8516     0.1159   15.97   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 1780 degrees of freedom\nMultiple R-squared:  0.1254,\tAdjusted R-squared:  0.1249 \nF-statistic: 255.1 on 1 and 1780 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n#### Plot loess fit depicting residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = loess_biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_smooth(method=lm, col = \"black\", se = F, size=.25, linetype=\"dashed\") + # linear fit black dashed line\n  geom_smooth(method=loess, col = \"red\", se = F, size=.25) + # loess fit red line\n  geom_point(size=.5, aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") +\n  # guides(color = FALSE) +\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](class-03_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# References and resources\n\nI have borrowed heavily from, directly taken, and/or highly recommend\nthe following fantastic resources:\n\n1.  [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/) from Jonas\n    Kristoffer Lindel<c3><b8>v\\\n2.  [Statquest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)\n3.  [Stats gobbledygook](https://www.rapidtables.com/math/symbols/Statistical_Symbols.html)\n4.  [Linear Regression Assumptions and Diagnostics in R: Essentials](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/)\\\n5.  [PRINCIPLES OF STATISTICS](https://www.graphpad.com/guides/prism/latest/statistics/stat_---_principles_of_statistics_-.htm)\n    from GraphPad/SAS.\n",
    "supporting": [
      "class-03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}