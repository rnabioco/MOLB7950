{
  "hash": "3c191ceb5ba2515e277effc2e82286e6",
  "result": {
    "markdown": "---\ntitle: 'Bootcamp: Stats class 5'\nauthor: \"[Neelanjan Mukherjee](neelanjan.mukherjee@cuanschutz.edu)\"\ndate: \"[Office Hours](https://calendly.com/molb7950)\"\noutput:\n  html_document: default\n  pdf_document: default\neditor_options: \n  chunk_output_type: console\n  markdown: \n    wrap: 72\n---\n\n\n\n\n------------------------------------------------------------------------\n\n# Learning Objectives\n\n-   **Visualize** and **Summarize** the data being compared\n-   **Formulate** and **Execute** null hypothesis testing\n-   **Identify** and **Perform** the proper statistical test for data\n    type/comparison\n-   **Calculate** and **Interpret** p-values\n-   **Prevent** p-hacking and **Recognize** issues with simultaneously\n    testing multiple hypotheses.\n\n------------------------------------------------------------------------\n\n# Outline\n\n-   Concepts and Definitions\n    -   Simplifying principles: Common tests as linear models\n    -   Types of comparisons and statistical tests\n    -   Definitions\\\n-   Day 1: Relationship between two or more continuous variables\n    -   Correlation vs Regression\n    -   Fitting it a line to data\n    -   Linear regression concepts\n    -   Multiple regression\\\n-   Day 2: Relationship between categorical and continuous variables\n    -   Comparing means between two groups (t-Test)\n\n    -   Comparing means between 3 or more groups (ANOVA)**\\\n        **\n-   **Day 3: Multiple test correction, Bayesian intro, History**\n\n------------------------------------------------------------------------\n\n# Multiple testing correction\n\n## Genomics -\\> Lots of Data -\\> Lots of Hypothesis Tests\n\nIn a typical RNA-seq experiment, we test \\~10K different hypotheses. For\nexample, you have 10K genes and for each gene you test whether the mean\nexpression changed in condition A vs condition B. Using a standard\np-value cut-off of 0.05, we'd expect **500 genes** to be deemed\n\"significant\" by chance. Thus, we concerned about **False Positives or\nType I Errors**.\n\n![](img/type-i-and-ii-error-2.png)\n\nSo, we want to control our type 1 error. We can approach this in two\ndifferent ways.\n\n1.  Control overall <ce><b1> (also known as family-wise error rate or\n    [FWER](https://en.wikipedia.org/wiki/Family-wise_error_rate)), which\n    will affect the <ce><b1>\\* for each test. That is, we are controlling the\n    overall probability of making *at least one* false discovery.\n    Bonferroni and Sidak corrections all control FWER.\n\n2.  Control [false discovery\n    rate](https://en.wikipedia.org/wiki/False_discovery_rate) (FDR).\n    Where FWER controls for the probability for making a type 1 error\n    *at all*, these procedures allow for type 1 errors (false positives)\n    but control the proportion of these false positives in relation to\n    true positives. This is done by adjusting the decision made for the\n    p-value associated with each individual test to decide rejection or\n    not. Because this will result in a higher type 1 error rate, it has\n    higher [power](https://en.wikipedia.org/wiki/Power_(statistics)).\n    This affords a higher probability of *true discoveries.* The step\n    procedures control for FDR.\n\n### Bonferroni Correction\n\n\\*\\*The most conservative of corrections, the Bonferroni correction is\nalso perhaps the most straightforward in its approach. Simply divide <ce><b1>\nby the number of tests (*m*).\n\nHowever, with many tests, <ce><b1>\\* will become very small. This reduces\npower, which means that we are very unlikely to make any true\ndiscoveries.\n\n### Sidak Correction\n\n\\*\\*<ce><b1>\\* = 1-(1-<ce><b1>)\\^(1/*m*)\n\n### Holm's Step-Down Procedure\n\n\\*\\*An update of the Bonferroni correction, this procedure is more\npowerful. Rather than controlling the FWER, Holm's procedure controls\nfor the [false discovery\nrate](https://en.wikipedia.org/wiki/False_discovery_rate) (FDR) and is\nperformed after conducting all hypothesis tests and finding associated\np-values at <ce><b1> within a set.\n\nThe step-down procedure is best illustrated with an example. Say we have\nthree hypotheses, each with the associated p-value:\n\nH1: 0.025\\\nH2: 0.003\\\nH3: 0.01\n\nStep 1: Order p-values from smallest to greatest\n\nH2: 0.003\\\nH3: 0.01\\\nH1: 0.025\n\nStep 2: Use the Holm-Bonferroni formula for the first-ranked (smallest)\np-value\n\n```         \n<ce><b1>* = <ce><b1>/(n-rank+1)\n```\n\n<ce><b1>\\* = 0.05/(3--1+1) = 0.0167\n\nStep 3: Compare the first-ranked p-value with the <ce><b1>\\* calculated from\nStep 2\n\n0.003 \\< 0.0167\n\nBecause the p-value for H2 is less than the calculated <ce><b1>\\*, we can\nreject H2.\n\nMove onto the next ranked p-value and repeat steps 2--3, calculating the\n<ce><b1>\\* for its respective rank and comparing it to that p-value. Continue\nuntil you reach the first non-rejected hypothesis. You would then fail\nto reject all following hypotheses.\n\n### Hochberg's Step-Up Procedure\n\n\\*\\*More powerful than Holm's step-down procedure, Hochberg's step-up\nprocedure also seeks to control the FDR and follows a similar process,\nonly p-values are ranked from largest to smallest.\n\nFor each ranked p-value, it is compared to the <ce><b1>\\* calculated for its\nrespective rank (same formula as Holm's procedure). Testing continues\nuntil you reach the first non-rejected hypothesis. You would then fail\nto reject all following hypotheses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrna <- read_csv(file = \"data/deltaRNA_test.csv\", show_col_types = FALSE) %>% \n  select(gene_id, pvalue) %>%\n  na.omit()\n\nrna$fdr <- p.adjust(p = rna$pvalue, method = \"fdr\", n = nrow(rna))\n\nrna$BH <- p.adjust(p = rna$pvalue, method = \"BH\", n = nrow(rna))\n\nrna$bon <- p.adjust(p = rna$pvalue, method = \"bonferroni\", n = nrow(rna))\n\n\nrna_long <- rna %>% pivot_longer(cols = pvalue:bon, names_to = \"type\")\n\n\n\np_none <- ggplot(data = rna, aes(x=pvalue, y=pvalue)) +\n  geom_point(size=.1) +\n  ggtitle(\"None\") +\n  theme_minimal()\n\n\np_FDR <- ggplot(data = rna, aes(x=pvalue, y=fdr)) +\n  geom_point(size=.1) +\n  ggtitle(\"FDR\") +\n  theme_minimal()\n\np_BH <- ggplot(data = rna, aes(x=pvalue, y=BH)) +\n  geom_point(size=.1) +\n  ggtitle(\"BH\") +\n  theme_minimal()\n\np_bon <- ggplot(data = rna, aes(x=pvalue, y=bon)) +\n  geom_point(size=.1) +\n  ggtitle(\"Bonferroni\") +\n  theme_minimal()\n\nplot_grid(p_none, p_bon, p_BH, p_FDR, ncol = 2, nrow = 2)\n```\n\n::: {.cell-output-display}\n![](stats-class-05_files/figure-html/unnamed-chunk-1-1.png){width=960}\n:::\n\n```{.r .cell-code}\nggplot(data = rna_long, aes(x=value, color=type)) +\n  stat_ecdf() +\n  theme_minimal() +\n  xlab(\"p-values\") + \n  ylab(\"cumulative fraction\")\n```\n\n::: {.cell-output-display}\n![](stats-class-05_files/figure-html/unnamed-chunk-1-2.png){width=960}\n:::\n:::\n\n\n# Bayesian Statistics\n\n## Bayes' Rule\n\nThe conditional probability of the event $A$ conditional on the event\n$B$ is given by $$\n  P(A \\mid B) = \\frac{P(A \\,\\&\\, B)}{P(B)}.\n$$\n\nThis section introduces how the Bayes' rule is applied to calculating\nconditional probability, and several real-life examples are\ndemonstrated. Finally, we compare the Bayesian and frequentist\ndefinition of probability.\n\n### Bayes' Rule and Diagnostic Testing\n\nTo better understand conditional probabilities and their importance, let\nus consider an example involving the human immunodeficiency virus (HIV).\nIn the early 1980s, HIV had just been discovered and was rapidly\nexpanding. There was major concern with the safety of the blood supply.\nAlso, virtually no cure existed making an HIV diagnosis basically a\ndeath sentence, in addition to the stigma that was attached to the\ndisease.\n\nThese made false positives and false negatives in HIV testing highly\nundesirable. A **false positive** is when a test returns postive while\nthe truth is negative. That would for instance be that someone without\nHIV is wrongly diagnosed with HIV, wrongly telling that person they are\ngoing to die and casting the stigma on them. A **false negative** is\nwhen a test returns negative while the truth is positive. That is when\nsomeone with HIV undergoes an HIV test which wrongly comes back\nnegative. The latter poses a threat to the blood supply if that person\nis about to donate blood.\n\nThe probability of a false positive if the truth is negative is called\nthe false positive rate. Similarly, the false negative rate is the\nprobability of a false negative if the truth is positive. Note that both\nthese rates are conditional probabilities: The false positive rate of an\nHIV test is the probability of a positive result **conditional on** the\nperson tested having no HIV.\n\nThe HIV test we consider is an enzyme-linked immunosorbent assay,\ncommonly known as an ELISA. We would like to know the probability that\nsomeone (in the early 1980s) has HIV if ELISA tests positive. For this,\nwe need the following information. ELISA's true positive rate (one minus\nthe false negative rate), also referred to as sensitivity, recall, or\nprobability of detection, is estimated as\n\n$$\n  P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 93\\% = 0.93.\n$$\n\nIts true negative rate (one minus the false positive rate), also\nreferred to as specificity, is estimated as\n\n$$\n  P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV}) = 99\\% = 0.99.\n$$\n\nAlso relevant to our question is the prevalence of HIV in the overall\npopulation, which is estimated to be 1.48 out of every 1000 American\nadults. We therefore assume\n\n\n```{=tex}\n\\begin{equation}\n  P(\\text{Person tested has HIV}) = \\frac{1.48}{1000} = 0.00148.\n  (\\#eq:HIVpositive)\n\\end{equation}\n```\n\nNote that the above numbers are estimates. For our purposes, however, we\nwill treat them as if they were exact.\n\nOur goal is to compute the probability of HIV if ELISA is positive, that\nis $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$. In\nnone of the above numbers did we condition on the outcome of ELISA.\nFortunately, Bayes' rule allows is to use the above numbers to compute\nthe probability we seek. Bayes' rule states that\n\n\n```{=tex}\n\\begin{equation}\n  P(\\text{Person tested has HIV}  \\mid \\text{ELISA is positive}) = \\frac{P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive})}{P(\\text{ELISA is positive})}.\n   (\\#eq:HIVconditional)\n\\end{equation}\n```\n\nThis can be derived as follows. For someone to test positive and be HIV\npositive, that person first needs to be HIV positive and then secondly\ntest positive. The probability of the first thing happening is\n$P(\\text{HIV positive}) = 0.00148$. The probability of then testing\npositive is\n$P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 0.93$,\nthe true positive rate. This yields for the numerator\n\n\n```{=tex}\n\\begin{multline}\n  P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) \\\\\n  &= 0.00148 \\cdot 0.93\n  = 0.0013764.\n  \\end{split}\n  (\\#eq:HIVjoint)\n\\end{multline}\n```\n\nThe first step in the above equation is implied by Bayes' rule: By\nmultiplying the left- and right-hand side of Bayes' rule by $P(B)$, we\nobtain $$\n  P(A \\mid B) P(B) = P(A \\,\\&\\, B).\n$$\n\nThe denominator in \\@ref(eq:HIVconditional) can be expanded as\n\n\n```{=tex}\n\\begin{multline*}\n  P(\\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) + P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  &= 0.0013764 + 0.0099852 = 0.0113616\n  \\end{split}\n\\end{multline*}\n```\n\nwhere we used \\@ref(eq:HIVjoint) and\n\n\n```{=tex}\n\\begin{multline*}\n  P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has no HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has no HIV}) \\\\\n  &= \\left(1 - P(\\text{Person tested has HIV})\\right) \\cdot \\left(1 - P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV})\\right) \\\\\n  &= \\left(1 - 0.00148\\right) \\cdot \\left(1 - 0.99\\right) = 0.0099852.\n  \\end{split}\n\\end{multline*}\n```\n\nPutting this all together and inserting into \\@ref(eq:HIVconditional)\nreveals \\begin{equation}\n  P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) = \\frac{0.0013764}{0.0113616} \\approx 0.12.\n  (\\#eq:HIVresult)\n\\end{equation} So even when the ELISA returns positive, the probability\nof having HIV is only 12%. An important reason why this number is so low\nis due to the prevalence of HIV. Before testing, one's probability of\nHIV was 0.148%, so the positive test changes that probability\ndramatically, but it is still below 50%. That is, it is more likely that\none is HIV negative rather than positive after one positive ELISA test.\n\nQuestions like the one we just answered (What is the probability of a\ndisease if a test returns positive?) are crucial to make medical\ndiagnoses. As we saw, just the true positive and true negative rates of\na test do not tell the full story, but also a disease's prevalence plays\na role. Bayes' rule is a tool to synthesize such numbers into a more\nuseful probability of having a disease after a test result.\n\n\n::: {.cell}\n\n```{.example .cell-code}\nWhat is the probability that someone who tests positive does not actually have HIV?\n```\n:::\n\n\nWe found in \\@ref(eq:HIVresult) that someone who tests positive has a\n$0.12$ probability of having HIV. That implies that the same person has\na $1-0.12=0.88$ probability of not having HIV, despite testing positive.\n\n\n::: {.cell}\n\n```{.example .cell-code}\nIf the individual is at a higher risk for having HIV than a randomly sampled person from the population considered, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n```\n:::\n\n\nIf the person has a priori a higher risk for HIV and tests positive,\nthen the probability of having HIV must be higher than for someone not\nat increased risk who also tests positive. Therefore,\n$P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) > 0.12$\nwhere $0.12$ comes from \\@ref(eq:HIVresult).\n\nOne can derive this mathematically by plugging in a larger number in\n\\@ref(eq:HIVpositive) than 0.00148, as that number represents the prior\nrisk of HIV. Changing the calculations accordingly shows\n$P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) > 0.12$.\n\n\n::: {.cell}\n\n```{.example .cell-code}\nIf the false positive rate of the test is higher than 1%, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n```\n:::\n\n\nIf the false positive rate increases, the probability of a wrong\npositive result increases. That means that a positive test result is\nmore likely to be wrong and thus less indicative of HIV. Therefore, the\nprobability of HIV after a positive ELISA goes down such that\n$P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) < 0.12$.\n\n### Bayes Updating\n\nIn the previous section, we saw that one positive ELISA test yields a\nprobability of having HIV of 12%. To obtain a more convincing\nprobability, one might want to do a second ELISA test after a first one\ncomes up positive. What is the probability of being HIV positive if also\nthe second ELISA test comes back positive?\n\nTo solve this problem, we will assume that the correctness of this\nsecond test is not influenced by the first ELISA, that is, the tests are\nindependent from each other. This assumption probably does not hold true\nas it is plausible that if the first test was a false positive, it is\nmore likely that the second one will be one as well. Nonetheless, we\nstick with the independence assumption for simplicity.\n\nIn the last section, we used\n$P(\\text{Person tested has HIV}) = 0.00148$, see \\@ref(eq:HIVpositive),\nto compute the probability of HIV after one positive test. If we repeat\nthose steps but now with $P(\\text{Person tested has HIV}) = 0.12$, the\nprobability that a person with one positive test has HIV, we exactly\nobtain the probability of HIV after two positive tests. Repeating the\nmaths from the previous section, involving Bayes' rule, gives\n\n\n```{=tex}\n\\begin{multline}\n  P(\\text{Person tested has HIV} \\mid \\text{Second ELISA is also positive}) \\\\\n  \\begin{split}\n  &= \\frac{P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Person tested has HIV})}{P(\\text{Second ELISA is also positive})} \\\\\n  &= \\frac{0.12 \\cdot 0.93}{\n  \\begin{split}\n  &P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has HIV}) \\\\\n  &+ P(\\text{Person tested has no HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has no HIV})\n  \\end{split}\n  } \\\\\n  &= \\frac{0.1116}{0.12 \\cdot 0.93 + (1 - 0.12)\\cdot (1 - 0.99)} \\approx 0.93.\n  \\end{split}\n  (\\#eq:Bayes-updating)\n\\end{multline}\n```\n\nSince we are considering the same ELISA test, we used the same true\npositive and true negative rates as in Section\n\\@ref(sec:diagnostic-testing). We see that two positive tests makes it\nmuch more probable for someone to have HIV than when only one test comes\nup positive.\n\nThis process, of using Bayes' rule to update a probability based on an\nevent affecting it, is called Bayes' updating. More generally, the what\none tries to update can be considered 'prior' information, sometimes\nsimply called the **prior**. The event providing information about this\ncan also be data. Then, updating this prior using Bayes' rule gives the\ninformation conditional on the data, also known as the **posterior**, as\nin the information **after** having seen the data. Going from the prior\nto the posterior is Bayes updating.\n\nThe probability of HIV after one positive ELISA, 0.12, was the posterior\nin the previous section as it was an update of the overall prevalence of\nHIV, \\@ref(eq:HIVpositive). However, in this section we answered a\nquestion where we used this posterior information as the prior. This\nprocess of using a posterior as prior in a new problem is natural in the\nBayesian framework of updating knowledge based on the data.\n\n# Modern Statistics, Beer, and Eugenics\n\n![Image courtesy of the American Philosophical Society, Philadelphia,\nUSA.](https://media.springernature.com/full/springer-static/image/art%253A10.1038%252F35038589/MediaObjects/41576_2000_Article_BF35038589_Fig1_HTML.gif?as=webp)\n\n## Fathers of statistics\n\nThe torch was passed within the triumvirate of Galton, Pearson, and\nFisher.\n\n### Sir Francis Galton (1822-1911)\n\n![from galton.org](https://galton.org/photos/standard-photo.gif)\n\n-   Discovered regression to the mean\n\n-   Re-discovered correlation and regression and discovered how to apply\n    these in anthropology, psychology, and more\n\n-   Defined the concept of standard deviation\n\n-   Established the field of Eugenics in 1883\n\n-   Darwin's cousin.\n\nGalton's reasoning for coining the term eugenics:\n\n> \"We greatly want a brief word to express the science of improving\n> stock, which...takes cognisance of all influences that tend in however\n> remote a degree to give the more suitable races or strains of blood a\n> better chance of prevailing speedily over the less suitable than they\n> otherwise would have had.\"\n\n### Karl Pearson (1857-1936)\n\n![<https://www.britannica.com/biography/Karl-Pearson>](https://cdn.britannica.com/32/38832-004-24FF6E46/Karl-Pearson-pencil-drawing-FA-de-Biden-1924.jpg?s=1500x700&q=85){width=\"350\"}\n\nKarl Pearson was Galton's prot<c3><a9>g<c3><a9> and directly or contributed to:\n\n-   Developed hypothesis testing\n\n-   Developed the use of p-values\n\n-   Defined the Chi-Squared test\n\n-   Correlation coefficient\n\n-   Principle components analysis\n\nAlso authors of timeless \"classics\" such as:\\\n[The Woman's\nQuestion](https://archive.org/details/ethicoffreethoug00pear/page/370/mode/2up?view=theater)\n\n[National Life from the standpoint of\nscience](https://archive.org/details/nationallifefro00peargoog/page/n12/mode/2up?view=theater)\n\nIn the year Mein Kampf was published, Pearson wrote an article called:\\\n[**THE PROBLEM OF ALIEN IMMIGRATION INTO GREAT BRITAIN, ILLUSTRATED BY\nAN EXAMINATION OF RUSSIAN AND POLISH JEWISH\nCHILDREN**](https://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1925.tb02037.x)\n\nHere is an excerpt:\n\n> \"\\[they\\] will develop into a parasitic race...Taken *on the average*,\n> and regarding both sexes, this alien Jewish population is somewhat\n> inferior physically and mentally to the native population.\"\n\n### Sir Ronald Aylmer Fisher (1890-1962)\n\n![<https://www.42evolution.org/ronald-a-fisher/>](https://www.42evolution.org/wp-content/uploads/2014/07/Ronald-Fisher-from-Royal-Society.jpg)\n\nFisher's work in statistics established and promoted many important\nmethods of statistical inference. His contributions include:\n\n-   Establishing p = 0.05 as the normal threshold for significant\n    p-values\n\n-   Promoting Maximum Likelihood Estimation\n\n-   Developing the ANalysis Of VAriance (ANOVA) The iris dataset (this\n    seems an incredibly minor contribution but I use it daily)\n\n-   [The Genetical Theory of Natural\n    Selection](https://en.wikipedia.org/wiki/The_Genetical_Theory_of_Natural_Selection),\n    which blended the work of Mendel and Darwin.\n\nThere is no lack of Fisher's strong and consistent support for eugenics.\nHere is an example from as late as 1954.\n\n![Letter from R.A. Fisher to R. Ruggles Gates. Ronald Fisher Archive.\nUniversity of\nAdelaide.](https://thisviewoflife.com/wp-content/uploads/2021/04/Fisher-to-Gates-08.27.54-768x534.png)\n\n## Storytime: Galton Laboratory\n\nGalton founded the Eugenics Record Office (1904)\n\nGalton Eugenics Laboratory as part of University College London (UCL).\nCreated by Pearson and funded by Galton. (1907)\n\nGalton left UCL enough money to create a Chair in National Eugenics,\nfilled by Pearson and then Fisher. Hell of a name for an endowed chair!\n\nAnnals of Human Genetics: It was established in 1925 Pearson as the\nAnnals of Eugenics, and obtained its current name in 1954.\n\nGalton laboratory was incorporated into the Department of Eugenics,\nBiometry and Genetic at UCL in 1944.\n\nRenamed to the Department of Human Genetics and Biometry in 1966.\n\nBecame part of the Department of Biology at UCL in 1996.\n\nIn 2020: [UCL renames three facilities that honoured prominent\neugenicists](https://www.theguardian.com/education/2020/jun/19/ucl-renames-three-facilities-that-honoured-prominent-eugenicists)\n\n**These horrendous views did not appear to be common at UCL in the\n1930s. For example, they were not held by JBS Haldane, Egon Pearson (son\nof Karl), and Lionel Penrose.**\n\n### What about in the US?\n\n[CSHL - Eugenics\nArchive](http://eugenicsarchive.ca/discover/tree/5233cfa15c2ec500000000ad)\n\n[U.S. Scientists' Role in the Eugenics Movement (1907--1939): A\nContemporary Biologist's\nPerspective](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2757926/)\n\n[Charles Davenport (first director of CSHL) and the Carnegie\nInsitution](https://carnegiescience.edu/carnegie-institution-science-statement-eugenics-research)\n\n[Cold Spring Harbor and German Eugenics in the\n1930s](https://www.cshl.edu/archives/expanded-commentary/cold-spring-harbor-and-german-eugenics-in-the-1930s/)\n\n[Eugenics and the history of Science and\nAAAS](https://www.science.org/content/blog-post/eugenics-and-history-science-and-aaas)\n\n## \n\n![from \"America's Shameful History of Eugenics and Forced\nSterilizations\"](https://i0.wp.com/www.theifod.com/wp-content/uploads/2021/02/image.png?w=1267&ssl=1)\n\n## Modern day: Eugenics and beyond\n\n### \\[Sordid genealogies: a conjectural history of Cambridge Analytica's\n\neugenic roots\\](https://www.nature.com/articles/s41599-020-0505-5)\n\n### [American Renaissance](https://www.amren.com/about/)\n\n![https://sitn.hms.harvard.edu/flash/2017/science-genetics-reshaping-race-debate-21st-century/](https://i0.wp.com/sitn.hms.harvard.edu/wp-content/uploads/2017/04/Fig1-raceConception-2.png){width=\"350\"}\\\n'Race' cannot be biologically defined due to genetic variation among\nhuman individuals and populations. (A) The old concept of the \"five\nraces:\" African, Asian, European, Native American, and Oceanian. (B)\nActual genetic variation in humans.\n\n### **Polygenic Traits, Human Embryos, and Eugenic Dreams**\n\nAn academic study debunked the idea of \"Screening Human Embryos for\nPolygenic Traits,\" but the CEO of the company Stephen Hsu cofounded\nannounced that they had screened human embryos for polygenic traits.\\\n![https://www.geneticsandsociety.org/biopolitical-times/polygenic-traits-human-embryos-and-eugenic-dreams](https://www.geneticsandsociety.org/sites/default/files/styles/teaser/public/screen_shot_2021-09-26_at_5.48.10_pm.png?itok=p9514DCD){width=\"350\"}\n\n[The amoral nonsense of Orchid's embryo\nselection](https://liorpachter.wordpress.com/2021/04/12/the-amoral-nonsense-of-orchids-embryo-selection/)\\\n\n### **\"Superior: The Return of Race Science\"**\n\n![<https://en.wikipedia.org/wiki/Superior:_The_Return_of_Race_Science>](https://upload.wikimedia.org/wikipedia/en/9/90/Superior_(book)_cover.jpeg)\n\n### **\"Weapons of Math Destruction\"**\n\n![<https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction>](https://upload.wikimedia.org/wikipedia/en/0/0b/Weapons_of_Math_Destruction.jpg)\n\n# References and resources\n\n### I have borrowed heavily from, directly taken, and/or highly recommend the following fantastic resources:\n\n## Multiple testing\n\n-   [How does multiple testing correction\n    work?](https://www.nature.com/articles/nbt1209-1135)\\\n-   [Multiple Testing --- How Should You\n    Adjust?](https://towardsdatascience.com/multiple-testing-how-should-you-adjust-41048eab4a3d)\\\n-   [Multiple\n    Comparisons](https://benwhalley.github.io/just-enough-r/multiple-comparisons.html)\\\n-   [An Overview of Methods to Address the Multiple Comparison\n    Problem](https://towardsdatascience.com/an-overview-of-methods-to-address-the-multiple-comparison-problem-310427b3ba92)\n\n## Bayesian statistics\n\n-   [High-speed intro to Bayes's\n    rule](https://arbital.com/p/bayes_rule/?l=693)\n\n-   [An Introduction to Bayesian\n    Thinking](https://statswithr.github.io/book/)\n\n-   [Bayes' Theorem, Clearly\n    Explained!!!!](https://www.youtube.com/watch?v=9wCnvr7Xw4E)\n\n## Modern Statistics, Beer, and Eugenics\n\n-   [Why We Might Not Have Statistics Without Guinness\n    Brewery](https://www.youtube.com/watch?v=U9Wr7VEPGXA)\n\n-   [Statistics, Eugenics, and\n    Me](https://towardsdatascience.com/statistics-eugenics-and-me-29eaf43efac7)\n\n-   [Is Statistics\n    Racist?](https://medium.com/swlh/is-statistics-racist-59cd4ddb5fa9)\n\n-   [Beer Vs. Eugenics: The Good And The Bad Uses Of\n    Statistics](https://www.forbes.com/sites/jerrybowyer/2016/01/06/beer-vs-eugenics-the-good-and-the-bad-uses-of-statistics/#64831bd32a14)\n\n-   [Engineering American society: the lesson of\n    eugenics](https://www.nature.com/articles/35038589)\n\n-   [Eugenics -- journey to the dark side at the dawn of\n    statistics](https://www.kdnuggets.com/2016/04/eugenics-journey-dark-side-statistics.html)\n\n-   [How Eugenics Shaped\n    Statistics](https://nautil.us/how-eugenics-shaped-statistics-9365/)\n\n-   [Francis Galton's Statistical Ideas: The Influence of\n    Eugenics](https://www.jstor.org/stable/229774)\n\n-   [R. A. Fisher: a faith fit for\n    eugenics](https://doi.org/10.1016/j.shpsc.2006.12.007)\n\n-   [Sordid genealogies: a conjectural history of Cambridge Analytica's\n    eugenic roots](https://www.nature.com/articles/s41599-020-0505-5)\n\n-   [U.S. Scientists' Role in the Eugenics Movement (1907--1939): A\n    Contemporary Biologist's\n    Perspective](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2757926/)\n\n-   [Biomedical centre memorial to victims of Nazi\n    research](https://www.nature.com/articles/35002739)\n\n-   [Berlin Wild---and the Max Delbr<c3><bc>ck Center for Molecular\n    Medicine](https://link.springer.com/article/10.1007/s00109-017-1532-6)\n\n-   [Eugenics\n    timeline](https://eugenicsarchive.ca/discover/timeline/543d5ab028f51f0000000003)\n\n-   [Karl Pearson praised Hitler and Nazi Race\n    Hygiene](https://profjoecain.net/karl-pearson-praised-hitler-nazi-race-hygiene/)\n\n-   [Ronald Fisher Is Not Being 'Cancelled', But His Eugenic Advocacy\n    Should Have\n    Consequences](https://thisviewoflife.com/ronald-fisher-is-not-being-cancelled-but-his-eugenic-advocacy-should-have-consequences/)\n",
    "supporting": [
      "stats-class-05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}