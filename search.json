[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOLB 7950: Informatics for Molecular Biology",
    "section": "",
    "text": "MOLB 7950 - Fall 2023 Schedule\n    \n    \n      Classes held in-person in AHSB 2201, 9:00-10:30am\n    \n    \n      \n      Date\n      Block\n      Topic\n      Instructor\n      Title\n      Problem Set\n      \n        Links\n      \n    \n    \n      Page\n      Slides\n    \n  \n  \n    \n      Week 1\n    \n    01\nMon, Aug 28, 2023\nBootcamp\nR\nHesselberth\nIntro to RStudo / tidyverse\n1\n📃\n📃\n    02\nTue, Aug 29, 2023\nBootcamp\nR\n-\n-\n2\n📃\n📃\n    03\nWed, Aug 30, 2023\nBootcamp\nR\n-\n-\n3\n📃\n📃\n    04\nThu, Aug 31, 2023\nBootcamp\nR\n-\n-\n4\n📃\n📃\n    05\nFri, Sep 1, 2023\nBootcamp\nR\n-\n-\n5\n📃\n📃\n    \n      Week 2\n    \n    06\nMon, Sep 4, 2023\nBootcamp\nR\n-\nNO CLASS: LABOR DAY\n6\n📃\n📃\n    07\nTue, Sep 5, 2023\nBootcamp\nR\n-\n-\n7\n📃\n📃\n    08\nWed, Sep 6, 2023\nBootcamp\nR\n-\n-\n8\n📃\n📃\n    09\nThu, Sep 7, 2023\nBootcamp\nR\n-\n-\n9\n📃\n📃\n    10\nFri, Sep 8, 2023\nBootcamp\nStatistics\n-\n-\n10\n📃\n📃\n    \n      Week 3\n    \n    11\nMon, Sep 11, 2023\nBootcamp\nStatistics\n-\n-\n11\n📃\n📃\n    12\nTue, Sep 12, 2023\nBootcamp\nStatistics\n-\n-\n12\n📃\n📃\n    13\nWed, Sep 13, 2023\nBootcamp\nStatistics\n-\n-\n13\n📃\n📃\n    14\nThu, Sep 14, 2023\nBootcamp\nStatistics\n-\n-\n14\n📃\n📃\n    15\nFri, Sep 15, 2023\nBootcamp\nStatistics\n-\n-\n15\n📃\n📃\n    \n      Week 4\n    \n    16\nMon, Sep 18, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    17\nWed, Sep 20, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    18\nFri, Sep 22, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    \n      Week 5\n    \n    19\nMon, Sep 25, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    20\nWed, Sep 27, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    21\nFri, Sep 29, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    \n      Week 6\n    \n    22\nMon, Oct 2, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    23\nWed, Oct 4, 2023\nDNA\n-\n-\n-\n-\n📃\n📃\n    24\nFri, Oct 6, 2023\nRNA\nOverview\n-\n-\n-\n📃\n📃\n    \n      Week 7\n    \n    25\nMon, Oct 9, 2023\nRNA\nBulk\n-\n-\n-\n📃\n📃\n    26\nWed, Oct 11, 2023\nRNA\nBulk\n-\n-\n-\n📃\n📃\n    27\nFri, Oct 13, 2023\nNA\n-\n-\nNO CLASS: CSDV RETREAT\n-\n📃\n📃\n    \n      Week 8\n    \n    28\nMon, Oct 16, 2023\nRNA\nLong-read\n-\n-\n-\n📃\n📃\n    29\nWed, Oct 18, 2023\nRNA\nRBP\n-\n-\n-\n📃\n📃\n    30\nFri, Oct 20, 2023\nRNA\nRBP\n-\n-\n-\n📃\n📃\n    \n      Week 9\n    \n    31\nMon, Oct 23, 2023\nRNA\nSingle-cell\n-\n-\n-\n📃\n📃\n    32\nWed, Oct 25, 2023\nRNA\nSingle-cell\n-\n-\n-\n📃\n📃\n    33\nFri, Oct 27, 2023\nNA\n-\n-\nNO CLASS: MOLB RETREAT\n-\n📃\n📃\n    \n      Week 10\n    \n    34\nMon, Oct 30, 2023\nFinal\n-\n-\n-\n-\n📃\n📃\n    35\nWed, Nov 1, 2023\nFinal\n-\n-\n-\n-\n📃\n📃"
  },
  {
    "objectID": "content/course-info/team.slides.html#teaching-team-and-office-hours",
    "href": "content/course-info/team.slides.html#teaching-team-and-office-hours",
    "title": "MOLB 7950 — Teaching Team",
    "section": "Teaching team and office hours",
    "text": "Teaching team and office hours\nInstructors\n\n\n\n\n\n\n\n\n\nInstructor\nE-mail\nGitHub\nSchedule a meeting\n\n\n\n\nJay Hesselberth\n\n\n\n\n\nNeel Mukherjee\n\n\n\n\n\nSrinivas Ramachandran\n\n\n\n\n\nMatt Taliaferro\n\n\n\n\n\nKent Riemondy\n\n\n\n\n\n\nTeaching Assistants\n\n\n\n\n\n\n\n\nInstructor\nE-mail\nGitHub\n\n\n\n\nEvan Morrison\n\n\n\n\nGrant Lo"
  },
  {
    "objectID": "content/course-info/team.html",
    "href": "content/course-info/team.html",
    "title": "MOLB 7950 — Teaching Team",
    "section": "",
    "text": "Instructor\nE-mail\nGitHub\nSchedule a meeting\n\n\n\n\nJay Hesselberth\n\n\n\n\n\nNeel Mukherjee\n\n\n\n\n\nSrinivas Ramachandran\n\n\n\n\n\nMatt Taliaferro\n\n\n\n\n\nKent Riemondy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructor\nE-mail\nGitHub\n\n\n\n\nEvan Morrison\n\n\n\n\nGrant Lo"
  },
  {
    "objectID": "content/course-info/team.html#teaching-team-and-office-hours",
    "href": "content/course-info/team.html#teaching-team-and-office-hours",
    "title": "MOLB 7950 — Teaching Team",
    "section": "",
    "text": "Instructor\nE-mail\nGitHub\nSchedule a meeting\n\n\n\n\nJay Hesselberth\n\n\n\n\n\nNeel Mukherjee\n\n\n\n\n\nSrinivas Ramachandran\n\n\n\n\n\nMatt Taliaferro\n\n\n\n\n\nKent Riemondy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructor\nE-mail\nGitHub\n\n\n\n\nEvan Morrison\n\n\n\n\nGrant Lo"
  },
  {
    "objectID": "content/course-info/links.slides.html#recommended-reading",
    "href": "content/course-info/links.slides.html#recommended-reading",
    "title": "MOLB 7950 — Course links",
    "section": "Recommended reading",
    "text": "Recommended reading\nData processing and visualization\n\nFundamentals of Data Visualization\n\nStatistics\n\nModern Statistics for Modern Biology\nStatistics for Biologists\nThink Stats"
  },
  {
    "objectID": "content/course-info/links.html",
    "href": "content/course-info/links.html",
    "title": "MOLB 7950 — Course links",
    "section": "",
    "text": "In previous iterations of this course, we taught command-line (bash, grep, awk, etc) and Python programming. These skills are of course useful, but for consistency we opted to focus on R programming and RStudio as an analysis environment.\nWe provide some of those older materials here: XXX\nAMC also offers shorter workshops on specific analysis strategies that you might find helpful."
  },
  {
    "objectID": "content/course-info/links.html#recommended-reading",
    "href": "content/course-info/links.html#recommended-reading",
    "title": "MOLB 7950 — Course links",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nData processing and visualization\n\nFundamentals of Data Visualization\n\n\n\nStatistics\n\nModern Statistics for Modern Biology\nStatistics for Biologists\nThink Stats"
  },
  {
    "objectID": "content/course-info/problem-sets.slides.html#problem-set-overview",
    "href": "content/course-info/problem-sets.slides.html#problem-set-overview",
    "title": "MOLB 7950 — Problem Sets",
    "section": "Problem Set Overview",
    "text": "Problem Set Overview\nProblem sets\nWe reinforce concepts with problem sets assigned at the end of each class. During the main blocks, problem sets on Mon and Wed should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors / TAs\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\nProblem Set Grading Rubric\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run"
  },
  {
    "objectID": "content/course-info/problem-sets.html",
    "href": "content/course-info/problem-sets.html",
    "title": "MOLB 7950 — Problem Sets",
    "section": "",
    "text": "We reinforce concepts with problem sets assigned at the end of each class. During the main blocks, problem sets on Mon and Wed should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors / TAs\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\n\n\n\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run"
  },
  {
    "objectID": "content/course-info/problem-sets.html#problem-set-overview",
    "href": "content/course-info/problem-sets.html#problem-set-overview",
    "title": "MOLB 7950 — Problem Sets",
    "section": "",
    "text": "We reinforce concepts with problem sets assigned at the end of each class. During the main blocks, problem sets on Mon and Wed should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors / TAs\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\n\n\n\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run"
  },
  {
    "objectID": "content/course-info/final-projects.slides.html#final-projects",
    "href": "content/course-info/final-projects.slides.html#final-projects",
    "title": "MOLB 7950 — Course links",
    "section": "Final projects",
    "text": "Final projects"
  },
  {
    "objectID": "content/course-info/support.slides.html#how-to-get-help",
    "href": "content/course-info/support.slides.html#how-to-get-help",
    "title": "MOLB 7950 — Getting help",
    "section": "How to get help",
    "text": "How to get help\nDiscussion\nCourse discussion will be through the Slack MOLB7950 organization.\nGuidelines for using Slack:\n\nUse dedicated channels for discussion in #class, questions about your #problem-sets, and your #final-project\nYou can ask for help by tagging the TAs in the #class channel. If you post @ta help, someone will start a thread where you can ask a question.\nIf needed, we can talk face-to-face via the /zoom integration.\n\nOffice Hours\n\nOur TAs will be available Tuesday & Thursday afternoons from 1-4pm."
  },
  {
    "objectID": "content/course-info/support.html",
    "href": "content/course-info/support.html",
    "title": "MOLB 7950 — Getting help",
    "section": "",
    "text": "Course discussion will be through the Slack MOLB7950 organization.\nGuidelines for using Slack:\n\nUse dedicated channels for discussion in #class, questions about your #problem-sets, and your #final-project\nYou can ask for help by tagging the TAs in the #class channel. If you post @ta help, someone will start a thread where you can ask a question.\nIf needed, we can talk face-to-face via the /zoom integration.\n\n\n\n\n\nOur TAs will be available Tuesday & Thursday afternoons from 1-4pm."
  },
  {
    "objectID": "content/course-info/support.html#how-to-get-help",
    "href": "content/course-info/support.html#how-to-get-help",
    "title": "MOLB 7950 — Getting help",
    "section": "",
    "text": "Course discussion will be through the Slack MOLB7950 organization.\nGuidelines for using Slack:\n\nUse dedicated channels for discussion in #class, questions about your #problem-sets, and your #final-project\nYou can ask for help by tagging the TAs in the #class channel. If you post @ta help, someone will start a thread where you can ask a question.\nIf needed, we can talk face-to-face via the /zoom integration.\n\n\n\n\n\nOur TAs will be available Tuesday & Thursday afternoons from 1-4pm."
  },
  {
    "objectID": "content/course-info/syllabus.slides.html#course-overview",
    "href": "content/course-info/syllabus.slides.html#course-overview",
    "title": "MOLB 7950 Syllabus",
    "section": "Course Overview",
    "text": "Course Overview\nMOLB 7950 is a hands-on tutorial of skills and theory needed to process, analyze, and visualize output from large biological data sets. We emphasize command-line tools and the R statistical computing environment.\n🗓️ Class will run from Aug 28 - Nov 1\n📍 Classes will be held in-person in AHSB 2201\n🕘 Class time is 9:00-10:30am\nMOLB 7950 is a three credit hour course.\nThe course is divided into blocks:\nBootcamp\nTHe Bootcamp block covers R programming and introduces important statistical concepts and approaches. We will also cover data types you will encounter during biological data analysis and approaches for their analysis.\nDuring the bootcamp block, we will meet everyday for 90 minutes to cover fundamental concepts you will need throughout the course.\nExperimental blocks\nAfter Bootcamp, will cover experimental approaches used to analyze DNA and RNA. Each block spans ~4 weeks, with each week focused on a particular type of experiment (see below). Each block covers statistical concepts needed for rigorous analysis and analysis approaches to process raw data to results (tables and figures) using reproducible coding techniques.\nIn most weeks we will discuss and analyze data from a publication. You are responsible for reading the week’s material before class begins on Monday.\nBlock experiments\n\nThe DNA block covers genome sequencing for identifying mutations, and two approaches for analyzing chromatin state (ChIP-seq and MNase-seq).\nThe RNA block covers RNA-seq, alternative splicing, differential gene expression, and RNA:protein interactions."
  },
  {
    "objectID": "content/course-info/syllabus.slides.html#schedule",
    "href": "content/course-info/syllabus.slides.html#schedule",
    "title": "MOLB 7950 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nClasses begin on August 28 and end on November 1. Dates are from the Fall 2023 Academic Calendar.\nDuring the Bootcamp block, classes will be held every day, Mon-Fri from 9:00-10:30am.\nDuring the DNA & RNA blocks, we will have in-class exercises and discussion on Mon-Wed-Fri 9:00-10:30am."
  },
  {
    "objectID": "content/course-info/syllabus.slides.html#location",
    "href": "content/course-info/syllabus.slides.html#location",
    "title": "MOLB 7950 Syllabus",
    "section": "Location",
    "text": "Location\nClasses will be held in-person in AHSB 2201. All classes will be recorded and made available through Canvas."
  },
  {
    "objectID": "content/course-info/syllabus.slides.html#policies",
    "href": "content/course-info/syllabus.slides.html#policies",
    "title": "MOLB 7950 Syllabus",
    "section": "Policies",
    "text": "Policies\nAttendance\nClass attendance is a firm expectation; frequent absences or tardiness are considered cause for a grade reduction.\nif you are sick, please let us know (e-mail Jay and Neel) and stay home.\nAnticipated absences outside of sickness should be reported to the instructors of a given block as soon as possible to make plans for possible accommodation.\nWe will record all lectures on Panopto and they will be available online through Canvas.\nLate and missed work\nWe have a late work policy for homework assignments:\n\nIf a problem set set is late but within 24 hours of due date/time, the grade will be reduced by 50%\nIf a problem set is returned any later, no credit will be given.\nAll regrade requests must be discussed with the professor within one week of receiving your grade. There will be no grade changes after the final project.\n\nDiversity & Inclusiveness\nOur view is that students from all diverse backgrounds and perspectives will be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class iss a resource, strength, and benefit.\nDisability Policy\nStudents with disabilities who need accommodations are encouraged to contact the Office of Disability, Access & Inclusion as soon as possible to ensure that accommodations are implemented in a timely fashion.\nHonor code\nAcademic dishonesty will not be tolerated and is grounds for dismissal from the class with a failing grade (“F”). For other information, please consult the Graduate Student Handbook."
  },
  {
    "objectID": "content/course-info/syllabus.slides.html#course-components",
    "href": "content/course-info/syllabus.slides.html#course-components",
    "title": "MOLB 7950 Syllabus",
    "section": "Course components",
    "text": "Course components\nPre-requisites\nYou will need to complete three pre-requisites before the relevant section of the bootcamp starts:\n\nIntroduction to R (before week 2 of bootcamp)\nIntroduction to the tidyverse (before week 2 of bootcamp)\n\nThese should be listed under the Assignments in your DataCamp workspace.\nProblem Sets\n\nProblem sets will be assigned at the end of each class.\nYou can use external resources but must explicitly cite where you have obtained code (both code you used directly and “paraphrased” code / code used as inspiration). Any reused code that is not explicitly cited will be treated as plagiarism.\nYou can discuss the content of assignments with others in this class. If you do so, you must acknowledge your collaborator(s) at the top of your assignment, for example: “Collaborators: Hillary and Bernie”. Failure to acknowledge collaborators will result in a grade of 0. You may not copy code and/or answers directly from another student. If you copy other work, both parties will receive a grade of 0.\nThe problem set with the lowest score for each student will be dropped.\nRather than copying someone’s work, ask for help. You are not alone in this course!\n\nProfessionalism\n\nPlease refrain from texting or using your computer for anything other than coursework during class."
  },
  {
    "objectID": "content/course-info/syllabus.slides.html#assignments-and-grading",
    "href": "content/course-info/syllabus.slides.html#assignments-and-grading",
    "title": "MOLB 7950 Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThe course measures learning through daily problem sets, a final project, and your participation.\n\n\n\nType\n% of grade\n\n\n\n\nProblem Sets\n60\n\n\nFinal Project\n20\n\n\nParticpation\n20\n\n\n\nGrades will be assigned as follows:\n\n\n\nPercent total points\nGrade\n\n\n\n\n&gt;= 95\nA\n\n\n&gt;= 90\nA-\n\n\n&gt;= 85\nB+\n\n\n&gt;= 80\nB\n\n\n\nProblem sets\nWe reinforce concepts with problem sets assigned at the end of class that should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nTue @ 12pm\nWed @ 5pm\nThurs @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors\n60 min\n\n\nThurs @ 12pm\nFri @ 5pm\nTues @ 5pm\nInstructors\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\nFinal projects\nFinal projects can be completed in groups of 1-3 people. Projects will involve analysis of existing public data sets and end with a short presentation the last week of class. The final project constitutes 20% of your grade.\nGrading Rubrics\nProblem Set Rubric\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run\n\n\n\nParticipation rubric\nParticipation is worth 20% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds improvement\n\n\n\n\nAttendance (physically present for class, or coordinating with instructor when absent)\nAttends class regularly (5)\nAttends most classes (4)\nAttends some classes (0-3)\n\n\nPreparation (activities required for in-class participation, like surveys and software installation)\nCompletes requested activities prior to class (5)\nCompletes most requested activities prior to class, sometimes needs to finish during class (4)\nRarely completes requested activities prior to class, often takes class time to complete (0-3)\n\n\nEngagement (in-class activities like coding exercises and discussion)\nActively engages in class activities (10)\nSometimes engages in class activities (8)\nDoesn’t engage in class activities (0-7)"
  },
  {
    "objectID": "content/course-info/syllabus.html",
    "href": "content/course-info/syllabus.html",
    "title": "MOLB 7950 Syllabus",
    "section": "",
    "text": "MOLB 7950 is a hands-on tutorial of skills and theory needed to process, analyze, and visualize output from large biological data sets. We emphasize command-line tools and the R statistical computing environment.\n🗓️ Class will run from Aug 28 - Nov 1\n📍 Classes will be held in-person in AHSB 2201\n🕘 Class time is 9:00-10:30am\nMOLB 7950 is a three credit hour course.\nThe course is divided into blocks:\n\n\nTHe Bootcamp block covers R programming and introduces important statistical concepts and approaches. We will also cover data types you will encounter during biological data analysis and approaches for their analysis.\nDuring the bootcamp block, we will meet everyday for 90 minutes to cover fundamental concepts you will need throughout the course.\n\n\n\nAfter Bootcamp, will cover experimental approaches used to analyze DNA and RNA. Each block spans ~4 weeks, with each week focused on a particular type of experiment (see below). Each block covers statistical concepts needed for rigorous analysis and analysis approaches to process raw data to results (tables and figures) using reproducible coding techniques.\nIn most weeks we will discuss and analyze data from a publication. You are responsible for reading the week’s material before class begins on Monday.\n\n\n\nThe DNA block covers genome sequencing for identifying mutations, and two approaches for analyzing chromatin state (ChIP-seq and MNase-seq).\nThe RNA block covers RNA-seq, alternative splicing, differential gene expression, and RNA:protein interactions."
  },
  {
    "objectID": "content/course-info/syllabus.html#course-overview",
    "href": "content/course-info/syllabus.html#course-overview",
    "title": "MOLB 7950 Syllabus",
    "section": "",
    "text": "MOLB 7950 is a hands-on tutorial of skills and theory needed to process, analyze, and visualize output from large biological data sets. We emphasize command-line tools and the R statistical computing environment.\n🗓️ Class will run from Aug 28 - Nov 1\n📍 Classes will be held in-person in AHSB 2201\n🕘 Class time is 9:00-10:30am\nMOLB 7950 is a three credit hour course.\nThe course is divided into blocks:\n\n\nTHe Bootcamp block covers R programming and introduces important statistical concepts and approaches. We will also cover data types you will encounter during biological data analysis and approaches for their analysis.\nDuring the bootcamp block, we will meet everyday for 90 minutes to cover fundamental concepts you will need throughout the course.\n\n\n\nAfter Bootcamp, will cover experimental approaches used to analyze DNA and RNA. Each block spans ~4 weeks, with each week focused on a particular type of experiment (see below). Each block covers statistical concepts needed for rigorous analysis and analysis approaches to process raw data to results (tables and figures) using reproducible coding techniques.\nIn most weeks we will discuss and analyze data from a publication. You are responsible for reading the week’s material before class begins on Monday.\n\n\n\nThe DNA block covers genome sequencing for identifying mutations, and two approaches for analyzing chromatin state (ChIP-seq and MNase-seq).\nThe RNA block covers RNA-seq, alternative splicing, differential gene expression, and RNA:protein interactions."
  },
  {
    "objectID": "content/course-info/syllabus.html#schedule",
    "href": "content/course-info/syllabus.html#schedule",
    "title": "MOLB 7950 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nClasses begin on August 28 and end on November 1. Dates are from the Fall 2023 Academic Calendar.\nDuring the Bootcamp block, classes will be held every day, Mon-Fri from 9:00-10:30am.\nDuring the DNA & RNA blocks, we will have in-class exercises and discussion on Mon-Wed-Fri 9:00-10:30am."
  },
  {
    "objectID": "content/course-info/syllabus.html#location",
    "href": "content/course-info/syllabus.html#location",
    "title": "MOLB 7950 Syllabus",
    "section": "Location",
    "text": "Location\nClasses will be held in-person in AHSB 2201. All classes will be recorded and made available through Canvas."
  },
  {
    "objectID": "content/course-info/syllabus.html#policies",
    "href": "content/course-info/syllabus.html#policies",
    "title": "MOLB 7950 Syllabus",
    "section": "Policies",
    "text": "Policies\n\nAttendance\nClass attendance is a firm expectation; frequent absences or tardiness are considered cause for a grade reduction.\nif you are sick, please let us know (e-mail Jay and Neel) and stay home.\nAnticipated absences outside of sickness should be reported to the instructors of a given block as soon as possible to make plans for possible accommodation.\nWe will record all lectures on Panopto and they will be available online through Canvas.\n\n\nLate and missed work\nWe have a late work policy for homework assignments:\n\nIf a problem set set is late but within 24 hours of due date/time, the grade will be reduced by 50%\nIf a problem set is returned any later, no credit will be given.\nAll regrade requests must be discussed with the professor within one week of receiving your grade. There will be no grade changes after the final project.\n\n\n\nDiversity & Inclusiveness\nOur view is that students from all diverse backgrounds and perspectives will be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class iss a resource, strength, and benefit.\n\n\nDisability Policy\nStudents with disabilities who need accommodations are encouraged to contact the Office of Disability, Access & Inclusion as soon as possible to ensure that accommodations are implemented in a timely fashion.\n\n\nHonor code\nAcademic dishonesty will not be tolerated and is grounds for dismissal from the class with a failing grade (“F”). For other information, please consult the Graduate Student Handbook."
  },
  {
    "objectID": "content/course-info/syllabus.html#course-components",
    "href": "content/course-info/syllabus.html#course-components",
    "title": "MOLB 7950 Syllabus",
    "section": "Course components",
    "text": "Course components\n\nPre-requisites\nYou will need to complete three pre-requisites before the relevant section of the bootcamp starts:\n\nIntroduction to R (before week 2 of bootcamp)\nIntroduction to the tidyverse (before week 2 of bootcamp)\n\nThese should be listed under the Assignments in your DataCamp workspace.\n\n\nProblem Sets\n\nProblem sets will be assigned at the end of each class.\nYou can use external resources but must explicitly cite where you have obtained code (both code you used directly and “paraphrased” code / code used as inspiration). Any reused code that is not explicitly cited will be treated as plagiarism.\nYou can discuss the content of assignments with others in this class. If you do so, you must acknowledge your collaborator(s) at the top of your assignment, for example: “Collaborators: Hillary and Bernie”. Failure to acknowledge collaborators will result in a grade of 0. You may not copy code and/or answers directly from another student. If you copy other work, both parties will receive a grade of 0.\nThe problem set with the lowest score for each student will be dropped.\nRather than copying someone’s work, ask for help. You are not alone in this course!\n\n\n\nProfessionalism\n\nPlease refrain from texting or using your computer for anything other than coursework during class."
  },
  {
    "objectID": "content/course-info/syllabus.html#assignments-and-grading",
    "href": "content/course-info/syllabus.html#assignments-and-grading",
    "title": "MOLB 7950 Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThe course measures learning through daily problem sets, a final project, and your participation.\n\n\n\nType\n% of grade\n\n\n\n\nProblem Sets\n60\n\n\nFinal Project\n20\n\n\nParticpation\n20\n\n\n\nGrades will be assigned as follows:\n\n\n\nPercent total points\nGrade\n\n\n\n\n&gt;= 95\nA\n\n\n&gt;= 90\nA-\n\n\n&gt;= 85\nB+\n\n\n&gt;= 80\nB\n\n\n\n\nProblem sets\nWe reinforce concepts with problem sets assigned at the end of class that should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nTue @ 12pm\nWed @ 5pm\nThurs @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors\n60 min\n\n\nThurs @ 12pm\nFri @ 5pm\nTues @ 5pm\nInstructors\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\n\n\nFinal projects\nFinal projects can be completed in groups of 1-3 people. Projects will involve analysis of existing public data sets and end with a short presentation the last week of class. The final project constitutes 20% of your grade.\n\n\nGrading Rubrics\n\nProblem Set Rubric\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run\n\n\n\n\n\nParticipation rubric\nParticipation is worth 20% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds improvement\n\n\n\n\nAttendance (physically present for class, or coordinating with instructor when absent)\nAttends class regularly (5)\nAttends most classes (4)\nAttends some classes (0-3)\n\n\nPreparation (activities required for in-class participation, like surveys and software installation)\nCompletes requested activities prior to class (5)\nCompletes most requested activities prior to class, sometimes needs to finish during class (4)\nRarely completes requested activities prior to class, often takes class time to complete (0-3)\n\n\nEngagement (in-class activities like coding exercises and discussion)\nActively engages in class activities (10)\nSometimes engages in class activities (8)\nDoesn’t engage in class activities (0-7)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#provide-a-simple-and-flexible-framework",
    "href": "content/bootcamp/stats/class-04.slides.html#provide-a-simple-and-flexible-framework",
    "title": "Bootcamp: Stats class 4",
    "section": "Provide a simple and flexible framework",
    "text": "Provide a simple and flexible framework"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#variables-definitions",
    "href": "content/bootcamp/stats/class-04.slides.html#variables-definitions",
    "title": "Bootcamp: Stats class 4",
    "section": "Variables definitions",
    "text": "Variables definitions\nRandom variables (x, y)\nResponse Variable ( y - aka dependent or outcome variable): this variable is predicted or its variation is explained by the explanatory variable. In an experiment, this is the outcome that is measured following manipulation of the explanatory variable.\nExplanatory Variable ( x - aka independent or predictor variable): explains variations in the response variable. In an experiment, it is manipulated by the researcher.\nQuantitative Variables\nDiscrete variable: numeric variables that have a countable number of values between any two values - integer in R (e.g., number of mice, read counts).\nContinuous variable: numeric variables that have an infinite number of values between any two values - numeric in R (e.g., normalized expression values, fluorescent intensity).\nCategorical Variables\nNominal variable: (unordered) random variables have categories where order doesn’t matter - factor in R (e.g., country, type of gene, genotype).\nOrdinal variable: (ordered) random variables have ordered categories - order of levels in R ( e.g. grade of tumor)."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#hypothesis-testing-definitions",
    "href": "content/bootcamp/stats/class-04.slides.html#hypothesis-testing-definitions",
    "title": "Bootcamp: Stats class 4",
    "section": "Hypothesis testing definitions",
    "text": "Hypothesis testing definitions\nHypothesis testing is a statistical analysis that uses sample data to assess two mutually exclusive theories about the properties of a population. Statisticians call these theories the null hypothesis and the alternative hypothesis. A hypothesis test assesses your sample statistic and factors in an estimate of the sample error to determine which hypothesis the data support.\nWhen you can reject the null hypothesis, the results are statistically significant, and your data support the theory that an effect exists at the population level.\nA legal analogy: Guilty or not guilty?\nThe statistical concept of ‘significant’ vs. ‘not significant’ can be understood by comparing to the legal concept of ‘guilty’ vs. ‘not guilty’.\nIn the American legal system (and much of the world) a criminal defendant is presumed innocent until proven guilty. If the evidence proves the defendant guilty beyond a reasonable doubt, the verdict is ‘guilty’. Otherwise the verdict is ‘not guilty’. In some countries, this verdict is ‘not proven’, which is a better description. A ‘not guilty’ verdict does not mean the judge or jury concluded that the defendant is innocent -- it just means that the evidence was not strong enough to persuade the judge or jury that the defendant was guilty.\nIn statistical hypothesis testing, you start with the null hypothesis (usually that there is no difference between groups). If the evidence produces a small enough P value, you reject that null hypothesis, and conclude that the difference is real. If the P value is higher than your threshold (usually 0.05), you don’t reject the null hypothesis. This doesn’t mean the evidence convinced you that the treatment had no effect, only that the evidence was not persuasive enough to convince you that there is an effect.\nEffect — the difference between the population value and the null hypothesis value. The effect is also known as population effect or the difference. Typically, you do not know the size of the actual effect. However, you can use a hypothesis test to help you determine whether an effect exists and to estimate its size.\nNull Hypothesis or \\(\\mathcal{H}_0\\) — one of two mutually exclusive theories about the properties of the population in hypothesis testing. Typically, the null hypothesis states that there is no effect (i.e., the effect size equals zero).\nAlternative Hypothesis or \\(\\mathcal{H}_1\\) — the other theory about the properties of the population in hypothesis testing. Typically, the alternative hypothesis states that a population parameter does not equal the null hypothesis value. In other words, there is a non-zero effect. If your sample contains sufficient evidence, you can reject the null and favor the alternative hypothesis.\nP-values — the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. Lower p-values represent stronger evidence against the null. P-values in conjunction with the significance level determines whether your data favor the null or alternative hypothesis.\nStatQuest: P Values, clearly explained\nStatQuest: How to calculate p-values\nSignificance Level or \\(a\\) — an evidentiary standard set before the study. It is the probability that you say there is an effect when there is no effect (the probability of rejecting the null hypothesis given that it is true). Lower significance levels indicate that you require stronger evidence before you will reject the null.It is usually set at or below .05.\n\nGuinness"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#null-hypothesis-testing",
    "href": "content/bootcamp/stats/class-04.slides.html#null-hypothesis-testing",
    "title": "Bootcamp: Stats class 4",
    "section": "Null hypothesis testing",
    "text": "Null hypothesis testing\n\nSpecify the variables\nDeclare null hypothesis \\(\\mathcal{H}_0\\)\nCalculate test-statistic, exact p-value\nGenerate and visualize data reflecting null-distribution\nCalculate the p-value from the test statistic and null distribution\n\n*4-5: For calculating empirical p-value"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#the-simplicity-underlying-common-tests",
    "href": "content/bootcamp/stats/class-04.slides.html#the-simplicity-underlying-common-tests",
    "title": "Bootcamp: Stats class 4",
    "section": "The simplicity underlying common tests",
    "text": "The simplicity underlying common tests\nMost of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. In particular, it all comes down to:\n\\(y = a \\cdot x + b\\)\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#parametric-vs-non-parametric-tests",
    "href": "content/bootcamp/stats/class-04.slides.html#parametric-vs-non-parametric-tests",
    "title": "Bootcamp: Stats class 4",
    "section": "Parametric vs Non-Parametric tests",
    "text": "Parametric vs Non-Parametric tests\nParametric tests are suitable for normally distributed data.\nNon-Parametric tests are suitable for any continuous data. For the sake of simplicity and sticking with a consistent framework, we will consider Non-Parametric tests as the ranked versions of the corresponding parametric tests.\nMore on choosing Parametric vs Non-Parametric\n\n\n\n\n\nInfo\nParametric\nNon-Parametric\n\n\n\n\nbetter descriptor\nmean\nmedian\n\n\n# of samples (N)\nmany\nfew"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#equation-for-a-line-stats-version",
    "href": "content/bootcamp/stats/class-04.slides.html#equation-for-a-line-stats-version",
    "title": "Bootcamp: Stats class 4",
    "section": "Equation for a line (Stats version)",
    "text": "Equation for a line (Stats version)\nModel: the recipe for \\(y\\) is a slope (\\(\\beta_1\\)) times \\(x\\) plus an intercept (\\(\\beta_0\\), aka a straight line).\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x \\qquad \\mathcal{H}_0: \\beta_1 = 0 \\qquad y = \\beta_0 \\cdot 1\\)\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x \\qquad \\mathcal{H}_0: \\beta_1 \\neq 0\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#find-the-best-beta-coefficients",
    "href": "content/bootcamp/stats/class-04.slides.html#find-the-best-beta-coefficients",
    "title": "Bootcamp: Stats class 4",
    "section": "Find the best \\(\\beta\\) coefficients",
    "text": "Find the best \\(\\beta\\) coefficients\nThese \\(\\beta\\) coefficients are also called the paramters of the model. The \\(\\beta\\) coefficients returned are for the lineear model that best fits the data."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#specify-variables-and-hypothesis",
    "href": "content/bootcamp/stats/class-04.slides.html#specify-variables-and-hypothesis",
    "title": "Bootcamp: Stats class 4",
    "section": "Specify variables and hypothesis",
    "text": "Specify variables and hypothesis\nRemember: \\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(cholesterol\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\nSimple model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(\\mathcal{H}_1:\\) mouse \\(cholesterol\\) does explain \\(weight\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#relationship-between-mouse-weight-and-cholesterol",
    "href": "content/bootcamp/stats/class-04.slides.html#relationship-between-mouse-weight-and-cholesterol",
    "title": "Bootcamp: Stats class 4",
    "section": "Relationship between mouse weight and cholesterol",
    "text": "Relationship between mouse weight and cholesterol\n\n# fitting a line weight vs intercept (mean weight)\nfit_W &lt;- lm(formula = weight ~ 1 , data = biochem)\n\n# augment data to add fit/residuals\nbiochem_W &lt;- augment(fit_W, data = biochem)\n\n#plot data\np_wch &lt;- ggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size=.5) +\n  geom_smooth(method=lm, col = \"red\") +\n  scale_color_manual() +\n  theme_minimal()\n\np_WvInt_res &lt;- ggplot(data = biochem_W, aes(x = tot_cholesterol, y = weight)) +\n  geom_hline(yintercept = biochem_W$.fitted, col = \"red\", size=.5) + # plot linear model fit\n  geom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) + # plot line representing residuals\n  theme_minimal()\n\n# fitting a line weight vs icholesterol\nfit_WvC &lt;- lm(data = biochem,\n              formula = weight ~ 1 + tot_cholesterol)\n\n# augment data to add fit/residuals\nbiochem_WvC &lt;- augment(fit_WvC, data = biochem)\n\n# plot data\np_WvC_res &lt;- ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_point(size=.5, aes(color = .resid)) +\n  geom_smooth(method=lm, col = \"red\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\n\nbiochem_WvC_rsq &lt;- fit_WvC %&gt;% glance() %&gt;% pull(r.squared)\n\nbiochem_WvC_pval &lt;- fit_WvC %&gt;% glance() %&gt;% pull(p.value)\n\n\nplot_grid(p_WvInt_res, p_WvC_res, ncol = 2, labels = c(\"weight by intercept\",\"weight by cholesterol\"))"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#guinness-brewery-in-dublin",
    "href": "content/bootcamp/stats/class-04.slides.html#guinness-brewery-in-dublin",
    "title": "Bootcamp: Stats class 4",
    "section": "Guinness Brewery in Dublin",
    "text": "Guinness Brewery in Dublin\n \nWe will compare mouse \\(weight\\) by \\(gender\\)."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#specify-variables-and-hypothesis-1",
    "href": "content/bootcamp/stats/class-04.slides.html#specify-variables-and-hypothesis-1",
    "title": "Bootcamp: Stats class 4",
    "section": "Specify variables and hypothesis",
    "text": "Specify variables and hypothesis\nModel: \\(y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(gender\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\\(\\mathcal{H}_1:\\) mouse \\(gender\\) does explain \\(weight\\)\nImportant: \\(x_{i}\\) is an indicator (0 or 1) saying whether data point i was sampled from one or the other group (female or male).\nWe will explore this in more detail soon."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#calculate-ss_mean-for-weight-by-gender-female-vs-male",
    "href": "content/bootcamp/stats/class-04.slides.html#calculate-ss_mean-for-weight-by-gender-female-vs-male",
    "title": "Bootcamp: Stats class 4",
    "section": "1. Calculate \\(SS_{mean}\\) for weight by gender (female vs male)",
    "text": "1. Calculate \\(SS_{mean}\\) for weight by gender (female vs male)\nCompare \\(SS_{mean}\\) for weight by cholesterol versus weight by gender\n\\(SS_{mean}\\) — sum of squares around the overall mean of \\(y\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (data - mean)^2\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (y_{i} - \\overline{y})^2\\)\n\nResiduals, \\(e\\) — the difference between the observed value of the dependent variable \\(y\\) and the predicted value \\(\\widehat{y}\\) is called the residual. Each data point has one residual.\n\\(e = y_{i} - \\widehat{y}\\)\nClass exercise 1:\nWhich of these are valid ways to calculate \\(SS_{mean}\\) from biochem_W?\n\n# A. sum((biochem_W$weight - biochem_W$.fitted)^2)  \n# B. sum((biochem_W$weight - biochem_W$.resid)^2)    \n# C. sum(biochem_W$.resid^2)  \n# D. sum((biochem_W$weight - mean(biochem_W$weight))^2)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#calculate-ss_fit-for-weight-by-gender",
    "href": "content/bootcamp/stats/class-04.slides.html#calculate-ss_fit-for-weight-by-gender",
    "title": "Bootcamp: Stats class 4",
    "section": "2. Calculate \\(SS_{fit}\\) for weight by gender",
    "text": "2. Calculate \\(SS_{fit}\\) for weight by gender\nCompare \\(SS_{fit}\\) vs \\(SS_{fit}\\) weight by gender\n\n# fitting a line weight vs intercept + gender\nfit_WvG &lt;- lm(formula = weight ~ 1 + gender , data = biochem)\n\n# augment (i.e. add fitted and residual values)\nbiochem_WvG &lt;- augment(fit_WvG, data = biochem)\n\n# plot of data with mean and colored by residuals\np_WvG_res &lt;- ggplot(biochem_WvG, aes(x = gender, y = weight)) +\n  geom_point(position = position_jitter(), aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(x=.5, xend=1.5, y=fit_WvG$coefficients[1], yend=fit_WvG$coefficients[1]), color=\"red\") +\n    geom_segment(aes(x=1.5, xend=2.5, y=sum(fit_WvG$coefficients)), yend=sum(fit_WvG$coefficients), color=\"red\") +\n  theme_minimal()\n\n\n\n\n\n\nNOTE: We are fitting 2 lines to the data\nFor the weight by intercept model (right) we fit 1 line.\nFor the weight by gender model (left) we fit 2 lines (i.e. male and female).\nExceptions to the fit\nCheck the residuals!\nMatrices Interlude Begin\nHow do we go from 2 fit lines to 1 equation\nSince we don’t want to calculate any of this by hand, the framework needs to be flexible such that a computer can execute for different flavors of comparison (cont y vs cont x, cont y vs 2 or more categorical x, …).\nLet’s break this down and focus on just a few players.\n\np_exc\n\n\n\n# ggplot(data = biochem_WvG, aes(.resid, color=gender)) +\n#   geom_density() + \n#   theme_minimal()\n\nRemember that:\n\\(weight\\) is \\(y\\)\n\\(F_{avg}\\) is the average \\(weight\\) of \\(females\\)\n\\(M_{avg}\\) is the average \\(weight\\) of \\(males\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#need-a-volunteer",
    "href": "content/bootcamp/stats/class-04.slides.html#need-a-volunteer",
    "title": "Bootcamp: Stats class 4",
    "section": "Need a volunteer",
    "text": "Need a volunteer\nMe: Ooohh my, imagine how tedious it would be to do this for all 1782 mice…\nVolunteer: Wait a sec…isn’t there a way to formulate this as a matrix algebra problem.\nMe: You’re right - I’m so glad you asked! Let’s wield our matrix-magic at this problem and see what happens.\n\\(f_{avg} = \\beta_0\\) is the average \\(weight\\) of \\(female\\) mice\n\\(m_{avg} = \\beta_1\\) is the average \\(weight\\) of \\(male\\) mice\n\\(\\begin{bmatrix} y_{85} \\\\ y_{71} \\\\ y_{51} \\\\y_{62} \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{85} \\\\ e_{71} \\\\ e_{51} \\\\e_{62} \\end{bmatrix}\\)\nSo basically this looks like the same equation for fitting a line we’ve been discussing, just w/a few more dimensions :)\nThis is a conceptual peak into the underbelly of how the \\(\\beta\\) cofficients and least squares is performed using matrix operations (remember linear algebra, maybe?). We will not go any deeper in this course, but if you are interested in learning more I recommend the following resources:\n\nLinear Models Pt.3 - Design Matrices\n\nA Matrix Formulation of the Multiple Regression Model\nMatrices Interlude FIN\nClass exercise 2:\nWhich of these are valid ways to calculate \\(SS_{mean}\\) from biochem_W?\n\\(SS_{fit}\\) — sum of squares around the least-squares fit\n\\(SS_{fit} = \\sum_{i=1}^{n} (data - line)^2\\)\nWhich of these are valid ways to calculate \\(SS_{fit}\\) from biochem_WvG?\n\n# A. sum(biochem_WvG$.resid^2)  \n# B. sum((biochem_WvG$weight - biochem_WvG$.resid)^2)    \n# C. sum((biochem_WvG$weight - biochem_WvG$.fitted)^2)  \n# D. sum((biochem_WvG$weight - mean(biochem_WvG$weight))^2)\n\np_WvG_res"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#calculate-f-statistic",
    "href": "content/bootcamp/stats/class-04.slides.html#calculate-f-statistic",
    "title": "Bootcamp: Stats class 4",
    "section": "3. Calculate \\(F-statistic\\)",
    "text": "3. Calculate \\(F-statistic\\)\nF-statistic — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\\(F = \\displaystyle \\frac{SS_{fit}/(p_{fit}-p_{mean})} {SS_{mean}/(n-p_{fit})}\\)\n\\(p_{fit}\\) — number of parameters in the fit line\n\\(p_{mean}\\) — number of parameters in the mean line\n\\(n\\) — number of data points"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#sanity-check",
    "href": "content/bootcamp/stats/class-04.slides.html#sanity-check",
    "title": "Bootcamp: Stats class 4",
    "section": "4. Sanity check",
    "text": "4. Sanity check\n\n# sum of squares of the residuals for the simple model\nss.mean &lt;- sum(biochem_W$.resid^2)\n\n# sum of squares of the residuals for the complex model\nss.fit &lt;- sum(biochem_WvG$.resid^2)\n\n#### COEFFICIENTS NEEL\n\n# number of paramters in simple model\npmean &lt;- 1 \n\n# number of paramters in complex model\npfit &lt;- 2\n\n# F-value\nbiochem_WvG_F &lt;- ((ss.mean - ss.fit) / (pfit - 1)) / \n  (ss.fit / (nrow(biochem_WvG) - pfit))\n\nbiochem_WvG_F \n\n[1] 1296.507\n\nglance(fit_WvG) %&gt;% pull(statistic)\n\n   value \n1296.507"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#one-way-anova",
    "href": "content/bootcamp/stats/class-04.slides.html#one-way-anova",
    "title": "Bootcamp: Stats class 4",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nLet’s compare the \\(weight\\) by \\(family\\), but only for a few selected families.\n\n# biochem %&gt;% \n#   group_by(family) %&gt;%\n#   count(family) %&gt;% \n#   arrage(-n) %&gt;%\n#   View()\n\nbigfams &lt;- biochem %&gt;% \n  group_by(family) %&gt;%\n  count(family) %&gt;%\n  filter(n&gt;10) %&gt;%\n  pull(family)\n  \nbiochem_bigfams &lt;- biochem %&gt;%\n  filter(family %in% bigfams)\n\n# i have pre-selected some families to compare\nmyfams &lt;- c(\"B1.5:E1.4(4) B1.5:A1.4(5)\",\n\"F1.3:A1.2(3) F1.3:E2.2(3)\",\n\"A1.3:D1.2(3) A1.3:H1.2(3)\",\n\"D5.4:G2.3(4) D5.4:C4.3(4)\")\n\n# only keep the familys in myfams\nfam_data &lt;- biochem_bigfams %&gt;%\n  filter(family %in% myfams) %&gt;%\n  droplevels() \n\n# simplify family names and make factor\nfam_data$family &lt;- gsub(pattern = \"\\\\..*\", \n                        replacement = \"\",\n                        x = fam_data$family) %&gt;%\n  as.factor\n\n\n# make B1 the reference (most similar to overall mean)\nfam_data$family &lt;- relevel(x = fam_data$family, ref = \"B1\")\n\nModel: \\(y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(family\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\\(\\mathcal{H}_1:\\) mouse \\(family\\) does explain \\(weight\\)\nImportant: \\(x_{i}\\) is an indicator (0 or 1) saying which group point \\(i\\) was sampled from using the matrix encoding of 0s and 1s.\nBelow is an example depicting 6 observations with 2 from each of 3 families:\n\\(\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3} \\\\y_{4} \\\\y_{5} \\\\y_{5} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{1} \\\\ e_{2} \\\\ e_{3} \\\\e_{4} \\\\e_{5} \\\\e_{6} \\end{bmatrix}\\)\n\n# fitting a line weight vs intercept + family\nfit_WvFam &lt;- lm(formula = weight ~ family, data = fam_data)\n\n\n# augment (i.e. add fitted and residual values)\nbiochem_WvFam &lt;- augment(fit_WvFam, fam_data)\n\nmean_B1 &lt;- fit_WvFam$coefficients[1]\nmean_A1 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[2]\nmean_D5 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[3]\nmean_F1 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[4]\n\n# plot of data with mean and colored by residuals\np_WvFam_res &lt;- ggplot(biochem_WvFam, aes(x = family, y = weight)) +\n  geom_point(position = position_jitter(), aes(color = .resid)) +\n  geom_segment(aes(x=.5, xend=1.5, y=mean_B1, yend=mean_B1), color=\"red\") +\n  geom_segment(aes(x=1.5, xend=2.5, y=mean_A1, yend=mean_A1), color=\"red\") +\n  geom_segment(aes(x=2.5, xend=3.5, y=mean_D5, yend=mean_D5), color=\"red\") +\n  geom_segment(aes(x=3.5, xend=4.5, y=mean_F1, yend=mean_F1), color=\"red\") +\n  geom_segment(aes(x=.5, xend=4.5, y=mean(weight), yend=mean(weight)), color=\"black\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  theme_minimal()  \n\np_WvFam_res"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.slides.html#ancova-analysis-of-covariance",
    "href": "content/bootcamp/stats/class-04.slides.html#ancova-analysis-of-covariance",
    "title": "Bootcamp: Stats class 4",
    "section": "ANCOVA, Analysis of Covariance",
    "text": "ANCOVA, Analysis of Covariance\nANOVA with more than one independent variable. In this case, what is the impact of mouse age on mouse weight for males vs females.\n\n# more info here https://towardsdatascience.com/doing-and-reporting-your-first-anova-and-ancova-in-r-1d820940f2ef\n\np_WvA_gender &lt;- ggplot(data = biochem, aes(y = weight, x = age, color=gender)) +\n  geom_point(size=.5) +\n  geom_smooth(method=lm) +\n  theme_minimal()\n\np_WvA_gender\n\nWvA_gender &lt;- lm(formula = weight ~ 1 + age + gender, data = biochem)\n\nWvA_gender %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.475         0.475  2.42      805. 8.37e-250     2 -4100. 8209. 8231.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\naov(formula = weight ~ 1 + age + gender, data = biochem) %&gt;% glance()\n\n# A tibble: 1 × 6\n  logLik   AIC   BIC deviance  nobs r.squared\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1 -4100. 8209. 8231.   10402.  1782     0.475"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#provide-a-simple-and-flexible-framework",
    "href": "content/bootcamp/stats/class-04.html#provide-a-simple-and-flexible-framework",
    "title": "Bootcamp: Stats class 4",
    "section": "Provide a simple and flexible framework",
    "text": "Provide a simple and flexible framework"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#variables-definitions",
    "href": "content/bootcamp/stats/class-04.html#variables-definitions",
    "title": "Bootcamp: Stats class 4",
    "section": "Variables definitions",
    "text": "Variables definitions\n\nRandom variables (x, y)\nResponse Variable ( y - aka dependent or outcome variable): this variable is predicted or its variation is explained by the explanatory variable. In an experiment, this is the outcome that is measured following manipulation of the explanatory variable.\nExplanatory Variable ( x - aka independent or predictor variable): explains variations in the response variable. In an experiment, it is manipulated by the researcher.\n\n\nQuantitative Variables\nDiscrete variable: numeric variables that have a countable number of values between any two values - integer in R (e.g., number of mice, read counts).\nContinuous variable: numeric variables that have an infinite number of values between any two values - numeric in R (e.g., normalized expression values, fluorescent intensity).\n\n\nCategorical Variables\nNominal variable: (unordered) random variables have categories where order doesn’t matter - factor in R (e.g., country, type of gene, genotype).\nOrdinal variable: (ordered) random variables have ordered categories - order of levels in R ( e.g. grade of tumor)."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#hypothesis-testing-definitions",
    "href": "content/bootcamp/stats/class-04.html#hypothesis-testing-definitions",
    "title": "Bootcamp: Stats class 4",
    "section": "Hypothesis testing definitions",
    "text": "Hypothesis testing definitions\nHypothesis testing is a statistical analysis that uses sample data to assess two mutually exclusive theories about the properties of a population. Statisticians call these theories the null hypothesis and the alternative hypothesis. A hypothesis test assesses your sample statistic and factors in an estimate of the sample error to determine which hypothesis the data support.\nWhen you can reject the null hypothesis, the results are statistically significant, and your data support the theory that an effect exists at the population level.\nA legal analogy: Guilty or not guilty?\nThe statistical concept of ‘significant’ vs. ‘not significant’ can be understood by comparing to the legal concept of ‘guilty’ vs. ‘not guilty’.\nIn the American legal system (and much of the world) a criminal defendant is presumed innocent until proven guilty. If the evidence proves the defendant guilty beyond a reasonable doubt, the verdict is ‘guilty’. Otherwise the verdict is ‘not guilty’. In some countries, this verdict is ‘not proven’, which is a better description. A ‘not guilty’ verdict does not mean the judge or jury concluded that the defendant is innocent -- it just means that the evidence was not strong enough to persuade the judge or jury that the defendant was guilty.\nIn statistical hypothesis testing, you start with the null hypothesis (usually that there is no difference between groups). If the evidence produces a small enough P value, you reject that null hypothesis, and conclude that the difference is real. If the P value is higher than your threshold (usually 0.05), you don’t reject the null hypothesis. This doesn’t mean the evidence convinced you that the treatment had no effect, only that the evidence was not persuasive enough to convince you that there is an effect.\nEffect — the difference between the population value and the null hypothesis value. The effect is also known as population effect or the difference. Typically, you do not know the size of the actual effect. However, you can use a hypothesis test to help you determine whether an effect exists and to estimate its size.\nNull Hypothesis or \\(\\mathcal{H}_0\\) — one of two mutually exclusive theories about the properties of the population in hypothesis testing. Typically, the null hypothesis states that there is no effect (i.e., the effect size equals zero).\nAlternative Hypothesis or \\(\\mathcal{H}_1\\) — the other theory about the properties of the population in hypothesis testing. Typically, the alternative hypothesis states that a population parameter does not equal the null hypothesis value. In other words, there is a non-zero effect. If your sample contains sufficient evidence, you can reject the null and favor the alternative hypothesis.\nP-values — the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. Lower p-values represent stronger evidence against the null. P-values in conjunction with the significance level determines whether your data favor the null or alternative hypothesis.\nStatQuest: P Values, clearly explained\nStatQuest: How to calculate p-values\nSignificance Level or \\(a\\) — an evidentiary standard set before the study. It is the probability that you say there is an effect when there is no effect (the probability of rejecting the null hypothesis given that it is true). Lower significance levels indicate that you require stronger evidence before you will reject the null.It is usually set at or below .05.\n\n\n\nGuinness"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#null-hypothesis-testing",
    "href": "content/bootcamp/stats/class-04.html#null-hypothesis-testing",
    "title": "Bootcamp: Stats class 4",
    "section": "Null hypothesis testing",
    "text": "Null hypothesis testing\n\nSpecify the variables\nDeclare null hypothesis \\(\\mathcal{H}_0\\)\nCalculate test-statistic, exact p-value\nGenerate and visualize data reflecting null-distribution\nCalculate the p-value from the test statistic and null distribution\n\n*4-5: For calculating empirical p-value"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#the-simplicity-underlying-common-tests",
    "href": "content/bootcamp/stats/class-04.html#the-simplicity-underlying-common-tests",
    "title": "Bootcamp: Stats class 4",
    "section": "The simplicity underlying common tests",
    "text": "The simplicity underlying common tests\nMost of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. In particular, it all comes down to:\n\\(y = a \\cdot x + b\\)\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#parametric-vs-non-parametric-tests",
    "href": "content/bootcamp/stats/class-04.html#parametric-vs-non-parametric-tests",
    "title": "Bootcamp: Stats class 4",
    "section": "Parametric vs Non-Parametric tests",
    "text": "Parametric vs Non-Parametric tests\nParametric tests are suitable for normally distributed data.\nNon-Parametric tests are suitable for any continuous data. For the sake of simplicity and sticking with a consistent framework, we will consider Non-Parametric tests as the ranked versions of the corresponding parametric tests.\nMore on choosing Parametric vs Non-Parametric\n\n\n\n\n\nInfo\nParametric\nNon-Parametric\n\n\n\n\nbetter descriptor\nmean\nmedian\n\n\n# of samples (N)\nmany\nfew"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#equation-for-a-line-stats-version",
    "href": "content/bootcamp/stats/class-04.html#equation-for-a-line-stats-version",
    "title": "Bootcamp: Stats class 4",
    "section": "Equation for a line (Stats version)",
    "text": "Equation for a line (Stats version)\nModel: the recipe for \\(y\\) is a slope (\\(\\beta_1\\)) times \\(x\\) plus an intercept (\\(\\beta_0\\), aka a straight line).\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x \\qquad \\mathcal{H}_0: \\beta_1 = 0 \\qquad y = \\beta_0 \\cdot 1\\)\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x \\qquad \\mathcal{H}_0: \\beta_1 \\neq 0\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#find-the-best-beta-coefficients",
    "href": "content/bootcamp/stats/class-04.html#find-the-best-beta-coefficients",
    "title": "Bootcamp: Stats class 4",
    "section": "Find the best \\(\\beta\\) coefficients",
    "text": "Find the best \\(\\beta\\) coefficients\nThese \\(\\beta\\) coefficients are also called the paramters of the model. The \\(\\beta\\) coefficients returned are for the lineear model that best fits the data."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis",
    "href": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis",
    "title": "Bootcamp: Stats class 4",
    "section": "Specify variables and hypothesis",
    "text": "Specify variables and hypothesis\nRemember: \\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(cholesterol\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\nSimple model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(\\mathcal{H}_1:\\) mouse \\(cholesterol\\) does explain \\(weight\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#relationship-between-mouse-weight-and-cholesterol",
    "href": "content/bootcamp/stats/class-04.html#relationship-between-mouse-weight-and-cholesterol",
    "title": "Bootcamp: Stats class 4",
    "section": "Relationship between mouse weight and cholesterol",
    "text": "Relationship between mouse weight and cholesterol\n\n# fitting a line weight vs intercept (mean weight)\nfit_W &lt;- lm(formula = weight ~ 1 , data = biochem)\n\n# augment data to add fit/residuals\nbiochem_W &lt;- augment(fit_W, data = biochem)\n\n#plot data\np_wch &lt;- ggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size=.5) +\n  geom_smooth(method=lm, col = \"red\") +\n  scale_color_manual() +\n  theme_minimal()\n\np_WvInt_res &lt;- ggplot(data = biochem_W, aes(x = tot_cholesterol, y = weight)) +\n  geom_hline(yintercept = biochem_W$.fitted, col = \"red\", size=.5) + # plot linear model fit\n  geom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) + # plot line representing residuals\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# fitting a line weight vs icholesterol\nfit_WvC &lt;- lm(data = biochem,\n              formula = weight ~ 1 + tot_cholesterol)\n\n# augment data to add fit/residuals\nbiochem_WvC &lt;- augment(fit_WvC, data = biochem)\n\n# plot data\np_WvC_res &lt;- ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_point(size=.5, aes(color = .resid)) +\n  geom_smooth(method=lm, col = \"red\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\n\nbiochem_WvC_rsq &lt;- fit_WvC %&gt;% glance() %&gt;% pull(r.squared)\n\nbiochem_WvC_pval &lt;- fit_WvC %&gt;% glance() %&gt;% pull(p.value)\n\n\nplot_grid(p_WvInt_res, p_WvC_res, ncol = 2, labels = c(\"weight by intercept\",\"weight by cholesterol\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nTo what extent does mouse cholesterol predict mouse weight?\n\\(R^2\\) or coefficient of determination — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n13%\n\n\nProbability that relationship is due to chance?\n\\(p-value\\) — the probability of obtaining an \\(F-statistic\\) in the null distribution at least as extreme as our observed \\(F-statistic\\).\n8.9241551^{-54}"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#guinness-brewery-in-dublin",
    "href": "content/bootcamp/stats/class-04.html#guinness-brewery-in-dublin",
    "title": "Bootcamp: Stats class 4",
    "section": "Guinness Brewery in Dublin",
    "text": "Guinness Brewery in Dublin\n \nWe will compare mouse \\(weight\\) by \\(gender\\)."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis-1",
    "href": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis-1",
    "title": "Bootcamp: Stats class 4",
    "section": "Specify variables and hypothesis",
    "text": "Specify variables and hypothesis\nModel: \\(y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(gender\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\\(\\mathcal{H}_1:\\) mouse \\(gender\\) does explain \\(weight\\)\nImportant: \\(x_{i}\\) is an indicator (0 or 1) saying whether data point i was sampled from one or the other group (female or male).\nWe will explore this in more detail soon."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#calculate-ss_mean-for-weight-by-gender-female-vs-male",
    "href": "content/bootcamp/stats/class-04.html#calculate-ss_mean-for-weight-by-gender-female-vs-male",
    "title": "Bootcamp: Stats class 4",
    "section": "1. Calculate \\(SS_{mean}\\) for weight by gender (female vs male)",
    "text": "1. Calculate \\(SS_{mean}\\) for weight by gender (female vs male)\n\nCompare \\(SS_{mean}\\) for weight by cholesterol versus weight by gender\n\\(SS_{mean}\\) — sum of squares around the overall mean of \\(y\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (data - mean)^2\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (y_{i} - \\overline{y})^2\\)\n\nResiduals, \\(e\\) — the difference between the observed value of the dependent variable \\(y\\) and the predicted value \\(\\widehat{y}\\) is called the residual. Each data point has one residual.\n\\(e = y_{i} - \\widehat{y}\\)\n\n\nClass exercise 1:\nWhich of these are valid ways to calculate \\(SS_{mean}\\) from biochem_W?\n\n# A. sum((biochem_W$weight - biochem_W$.fitted)^2)  \n# B. sum((biochem_W$weight - biochem_W$.resid)^2)    \n# C. sum(biochem_W$.resid^2)  \n# D. sum((biochem_W$weight - mean(biochem_W$weight))^2)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#calculate-ss_fit-for-weight-by-gender",
    "href": "content/bootcamp/stats/class-04.html#calculate-ss_fit-for-weight-by-gender",
    "title": "Bootcamp: Stats class 4",
    "section": "2. Calculate \\(SS_{fit}\\) for weight by gender",
    "text": "2. Calculate \\(SS_{fit}\\) for weight by gender\n\nCompare \\(SS_{fit}\\) vs \\(SS_{fit}\\) weight by gender\n\n# fitting a line weight vs intercept + gender\nfit_WvG &lt;- lm(formula = weight ~ 1 + gender , data = biochem)\n\n# augment (i.e. add fitted and residual values)\nbiochem_WvG &lt;- augment(fit_WvG, data = biochem)\n\n# plot of data with mean and colored by residuals\np_WvG_res &lt;- ggplot(biochem_WvG, aes(x = gender, y = weight)) +\n  geom_point(position = position_jitter(), aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(x=.5, xend=1.5, y=fit_WvG$coefficients[1], yend=fit_WvG$coefficients[1]), color=\"red\") +\n    geom_segment(aes(x=1.5, xend=2.5, y=sum(fit_WvG$coefficients)), yend=sum(fit_WvG$coefficients), color=\"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNOTE: We are fitting 2 lines to the data\nFor the weight by intercept model (right) we fit 1 line.\nFor the weight by gender model (left) we fit 2 lines (i.e. male and female).\n\n\nExceptions to the fit\nCheck the residuals!\n\n\nMatrices Interlude Begin\n\nHow do we go from 2 fit lines to 1 equation\nSince we don’t want to calculate any of this by hand, the framework needs to be flexible such that a computer can execute for different flavors of comparison (cont y vs cont x, cont y vs 2 or more categorical x, …).\nLet’s break this down and focus on just a few players.\n\np_exc\n\n\n\n# ggplot(data = biochem_WvG, aes(.resid, color=gender)) +\n#   geom_density() + \n#   theme_minimal()\n\nRemember that:\n\\(weight\\) is \\(y\\)\n\\(F_{avg}\\) is the average \\(weight\\) of \\(females\\)\n\\(M_{avg}\\) is the average \\(weight\\) of \\(males\\)\n\nA048054885, female\n\\(y_{85}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{85}\\)\nA067109771, female\n\\(y_{71}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{71}\\)\n\nA066822351, male\n\\(y_{51}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{51}\\)\nA048274362, male\n\\(y_{62}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{62}\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#need-a-volunteer",
    "href": "content/bootcamp/stats/class-04.html#need-a-volunteer",
    "title": "Bootcamp: Stats class 4",
    "section": "Need a volunteer",
    "text": "Need a volunteer\nMe: Ooohh my, imagine how tedious it would be to do this for all 1782 mice…\nVolunteer: Wait a sec…isn’t there a way to formulate this as a matrix algebra problem.\nMe: You’re right - I’m so glad you asked! Let’s wield our matrix-magic at this problem and see what happens.\n\\(f_{avg} = \\beta_0\\) is the average \\(weight\\) of \\(female\\) mice\n\\(m_{avg} = \\beta_1\\) is the average \\(weight\\) of \\(male\\) mice\n\\(\\begin{bmatrix} y_{85} \\\\ y_{71} \\\\ y_{51} \\\\y_{62} \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{85} \\\\ e_{71} \\\\ e_{51} \\\\e_{62} \\end{bmatrix}\\)\nSo basically this looks like the same equation for fitting a line we’ve been discussing, just w/a few more dimensions :)\nThis is a conceptual peak into the underbelly of how the \\(\\beta\\) cofficients and least squares is performed using matrix operations (remember linear algebra, maybe?). We will not go any deeper in this course, but if you are interested in learning more I recommend the following resources:\n\nLinear Models Pt.3 - Design Matrices\n\nA Matrix Formulation of the Multiple Regression Model\n\nMatrices Interlude FIN\n\n\nClass exercise 2:\nWhich of these are valid ways to calculate \\(SS_{mean}\\) from biochem_W?\n\\(SS_{fit}\\) — sum of squares around the least-squares fit\n\\(SS_{fit} = \\sum_{i=1}^{n} (data - line)^2\\)\nWhich of these are valid ways to calculate \\(SS_{fit}\\) from biochem_WvG?\n\n# A. sum(biochem_WvG$.resid^2)  \n# B. sum((biochem_WvG$weight - biochem_WvG$.resid)^2)    \n# C. sum((biochem_WvG$weight - biochem_WvG$.fitted)^2)  \n# D. sum((biochem_WvG$weight - mean(biochem_WvG$weight))^2)\n\np_WvG_res"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#calculate-f-statistic",
    "href": "content/bootcamp/stats/class-04.html#calculate-f-statistic",
    "title": "Bootcamp: Stats class 4",
    "section": "3. Calculate \\(F-statistic\\)",
    "text": "3. Calculate \\(F-statistic\\)\nF-statistic — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\\(F = \\displaystyle \\frac{SS_{fit}/(p_{fit}-p_{mean})} {SS_{mean}/(n-p_{fit})}\\)\n\\(p_{fit}\\) — number of parameters in the fit line\n\\(p_{mean}\\) — number of parameters in the mean line\n\\(n\\) — number of data points"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#sanity-check",
    "href": "content/bootcamp/stats/class-04.html#sanity-check",
    "title": "Bootcamp: Stats class 4",
    "section": "4. Sanity check",
    "text": "4. Sanity check\n\n# sum of squares of the residuals for the simple model\nss.mean &lt;- sum(biochem_W$.resid^2)\n\n# sum of squares of the residuals for the complex model\nss.fit &lt;- sum(biochem_WvG$.resid^2)\n\n#### COEFFICIENTS NEEL\n\n# number of paramters in simple model\npmean &lt;- 1 \n\n# number of paramters in complex model\npfit &lt;- 2\n\n# F-value\nbiochem_WvG_F &lt;- ((ss.mean - ss.fit) / (pfit - 1)) / \n  (ss.fit / (nrow(biochem_WvG) - pfit))\n\nbiochem_WvG_F \n\n[1] 1296.507\n\nglance(fit_WvG) %&gt;% pull(statistic)\n\n   value \n1296.507 \n\n\n\nLet’s take a look at the statistic and p-values:\n\nfit_WvG_stats &lt;- tidy(fit_WvG) %&gt;% filter(term==\"genderM\") %&gt;% select(statistic, p.value)\n\n\n# to run a t.test in R we need numeric vectors for each of our groups of interest\nmale_weight_t &lt;- biochem_WvG %&gt;% filter(gender==\"M\") %&gt;% pull(weight)\n  \nfemale_weight_t &lt;- biochem_WvG %&gt;% filter(gender==\"F\") %&gt;% pull(weight)\n\n\ntrad_t &lt;- t.test(male_weight_t, female_weight_t, var.equal = T)\n\ntrad_WvG_stats &lt;- tidy(trad_t) %&gt;% select(statistic, p.value)\n\nfit_WvG_stats\n\n# A tibble: 1 × 2\n  statistic   p.value\n      &lt;dbl&gt;     &lt;dbl&gt;\n1      36.0 9.25e-214\n\ntrad_WvG_stats\n\n# A tibble: 1 × 2\n  statistic   p.value\n      &lt;dbl&gt;     &lt;dbl&gt;\n1      36.0 9.25e-214"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#one-way-anova",
    "href": "content/bootcamp/stats/class-04.html#one-way-anova",
    "title": "Bootcamp: Stats class 4",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nLet’s compare the \\(weight\\) by \\(family\\), but only for a few selected families.\n\n# biochem %&gt;% \n#   group_by(family) %&gt;%\n#   count(family) %&gt;% \n#   arrage(-n) %&gt;%\n#   View()\n\nbigfams &lt;- biochem %&gt;% \n  group_by(family) %&gt;%\n  count(family) %&gt;%\n  filter(n&gt;10) %&gt;%\n  pull(family)\n  \nbiochem_bigfams &lt;- biochem %&gt;%\n  filter(family %in% bigfams)\n\n# i have pre-selected some families to compare\nmyfams &lt;- c(\"B1.5:E1.4(4) B1.5:A1.4(5)\",\n\"F1.3:A1.2(3) F1.3:E2.2(3)\",\n\"A1.3:D1.2(3) A1.3:H1.2(3)\",\n\"D5.4:G2.3(4) D5.4:C4.3(4)\")\n\n# only keep the familys in myfams\nfam_data &lt;- biochem_bigfams %&gt;%\n  filter(family %in% myfams) %&gt;%\n  droplevels() \n\n# simplify family names and make factor\nfam_data$family &lt;- gsub(pattern = \"\\\\..*\", \n                        replacement = \"\",\n                        x = fam_data$family) %&gt;%\n  as.factor\n\n\n# make B1 the reference (most similar to overall mean)\nfam_data$family &lt;- relevel(x = fam_data$family, ref = \"B1\")\n\nModel: \\(y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(family\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\\(\\mathcal{H}_1:\\) mouse \\(family\\) does explain \\(weight\\)\nImportant: \\(x_{i}\\) is an indicator (0 or 1) saying which group point \\(i\\) was sampled from using the matrix encoding of 0s and 1s.\nBelow is an example depicting 6 observations with 2 from each of 3 families:\n\\(\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3} \\\\y_{4} \\\\y_{5} \\\\y_{5} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{1} \\\\ e_{2} \\\\ e_{3} \\\\e_{4} \\\\e_{5} \\\\e_{6} \\end{bmatrix}\\)\n\n# fitting a line weight vs intercept + family\nfit_WvFam &lt;- lm(formula = weight ~ family, data = fam_data)\n\n\n# augment (i.e. add fitted and residual values)\nbiochem_WvFam &lt;- augment(fit_WvFam, fam_data)\n\nmean_B1 &lt;- fit_WvFam$coefficients[1]\nmean_A1 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[2]\nmean_D5 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[3]\nmean_F1 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[4]\n\n# plot of data with mean and colored by residuals\np_WvFam_res &lt;- ggplot(biochem_WvFam, aes(x = family, y = weight)) +\n  geom_point(position = position_jitter(), aes(color = .resid)) +\n  geom_segment(aes(x=.5, xend=1.5, y=mean_B1, yend=mean_B1), color=\"red\") +\n  geom_segment(aes(x=1.5, xend=2.5, y=mean_A1, yend=mean_A1), color=\"red\") +\n  geom_segment(aes(x=2.5, xend=3.5, y=mean_D5, yend=mean_D5), color=\"red\") +\n  geom_segment(aes(x=3.5, xend=4.5, y=mean_F1, yend=mean_F1), color=\"red\") +\n  geom_segment(aes(x=.5, xend=4.5, y=mean(weight), yend=mean(weight)), color=\"black\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  theme_minimal()  \n\np_WvFam_res\n\n\n\n\n\n\nanova_WvFam &lt;- aov(formula = weight ~ family, data = fam_data)\n\n# are the coefficients the same?\nbind_cols(tidy(fit_WvFam) %&gt;% select(term,estimate),\n          ANOVA_coef=anova_WvFam$coefficients\n          )\n\n# A tibble: 4 × 3\n  term        estimate ANOVA_coef\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   18.1       18.1  \n2 familyA1      -3.66      -3.66 \n3 familyD5       5.86       5.86 \n4 familyF1      -0.682     -0.682\n\nglance(fit_WvFam) %&gt;% select(p.value)\n\n# A tibble: 1 × 1\n   p.value\n     &lt;dbl&gt;\n1 7.62e-13\n\ntidy(anova_WvFam) %&gt;% select(p.value)\n\n# A tibble: 2 × 1\n    p.value\n      &lt;dbl&gt;\n1  7.62e-13\n2 NA       \n\n\n\n\nClass exercise 3:\n\n\n\n\n\nWhat are the \\(p_{mean}\\) and \\(p_{fit}\\) for the models above?\n\n\n\n\n\nParameter\nfit cholesterol\nfit gender\nfit family\n\n\n\n\n\\(p_{mean}\\)\n?\n?\n?\n\n\n\\(p_{fit}\\)\n?\n?\n?"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#ancova-analysis-of-covariance",
    "href": "content/bootcamp/stats/class-04.html#ancova-analysis-of-covariance",
    "title": "Bootcamp: Stats class 4",
    "section": "ANCOVA, Analysis of Covariance",
    "text": "ANCOVA, Analysis of Covariance\nANOVA with more than one independent variable. In this case, what is the impact of mouse age on mouse weight for males vs females.\n\n# more info here https://towardsdatascience.com/doing-and-reporting-your-first-anova-and-ancova-in-r-1d820940f2ef\n\np_WvA_gender &lt;- ggplot(data = biochem, aes(y = weight, x = age, color=gender)) +\n  geom_point(size=.5) +\n  geom_smooth(method=lm) +\n  theme_minimal()\n\np_WvA_gender\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nWvA_gender &lt;- lm(formula = weight ~ 1 + age + gender, data = biochem)\n\nWvA_gender %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.475         0.475  2.42      805. 8.37e-250     2 -4100. 8209. 8231.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\naov(formula = weight ~ 1 + age + gender, data = biochem) %&gt;% glance()\n\n# A tibble: 1 × 6\n  logLik   AIC   BIC deviance  nobs r.squared\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1 -4100. 8209. 8231.   10402.  1782     0.475"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.slides.html#class-objectives",
    "href": "content/bootcamp/stats/class-02.slides.html#class-objectives",
    "title": "Stats Class 02",
    "section": "Class Objectives",
    "text": "Class Objectives\n\nUnderstand how to determine if two samples come from different distributions\n\nUsing theoretical distributions\nUsing resampling/bootstrap\nUsing non-parametric tests\n\nLearn how to determine if overlap of two categories is significant"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.slides.html#continuing-from-last-class",
    "href": "content/bootcamp/stats/class-02.slides.html#continuing-from-last-class",
    "title": "Stats Class 02",
    "section": "Continuing from last class",
    "text": "Continuing from last class\nLet us start by reading in the dataset:\n\n# read dataset\ndata &lt;- read.csv(file = \"data/cell_dimensions.tsv\", sep = \"\\t\", stringsAsFactors = T)\nhead(data)\n\n   Cell_Rg Cell_Len   Cell_Type\n1 2.441927 4.143111 Fibroblasts\n2 2.832714 6.648487 Fibroblasts\n3 3.085169 7.822642 Fibroblasts\n4 3.243857 7.997854 Fibroblasts\n5 3.359737 7.448261 Fibroblasts\n6 3.434748 7.800078 Fibroblasts\n\n\nWe generated observed and theoretical distributions of Rg. First muscle:\n\n# Get muscle Rg as a vector:\nmuscle_rg &lt;- data[data$Cell_Type == \"Muscle\", ]$Cell_Rg\n# generate histogram\nmuscle_rg_hist &lt;- hist(muscle_rg, breaks = 30, plot = F)\n# Use seq to create a range of numbers\ncell_size_list &lt;- seq(0, 12, 0.2)\n# Use dnorm to calculate probability at each point of the vector above\ndist_cell_size_muscle &lt;- dnorm(cell_size_list, mean = 7, sd = 1)\nplot(muscle_rg_hist$mids, muscle_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_muscle, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\nNext, fibroblasts:\n\n# Get Fib Rg as a vector:\nfib_rg &lt;- data[data$Cell_Type == \"Fibroblasts\", ]$Cell_Rg\n# generate histogram\nfib_rg_hist &lt;- hist(fib_rg, breaks = 30, plot = F)\n# Use dnorm to calculate probability at each point of the cell_size_list vector\ndist_cell_size_fib &lt;- dnorm(cell_size_list, mean = 5, sd = 1)\nplot(fib_rg_hist$mids, fib_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_fib, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\nWe are confident that Rg is sampled from Normal distributions, so we can take advantage of knowing exact details of the theoretical distributions. We could work out the math using formulae, or we could sample the distribution to get an intuitive feel.\nThe question we are asking is: Is the Rg of muscle different from Rg of fibroblasts?\nWe now know that the Rg is sampled from normal distribution as follows:\n\nMuscle - mean = 7, sd = 1\nFibroblasts - mean = 5, sd = 1\n\nThe advantage of having a theoretical distribution is that we can calculate exact differences. We will use 10,000 points sampled from each distribution to illustrate this. Let us generate 10,000 points from these distributions:\n\ntheoretical_fib &lt;- rnorm(10000, mean = 5, sd = 1)\ntheoretical_muscle &lt;- rnorm(10000, mean = 7, sd = 1)\n\nThe difference in Rg between the two cell types would be simply:\n\ndiff_cell_Rg &lt;- theoretical_muscle - theoretical_fib\n\nWhat is the distribution of the difference?\n\nhist(diff_cell_Rg, breaks = 100, freq = F)\n\n\n\n\nThe difference is also a Normal distribution!\nThe mean of this distribution is simply difference in mean of the starting distributions: 7-5 = 2. The sd is the square root of sum of squares of the sd of the starting distributions:\n\nsqrt(1^2 + 1^2)\n\n[1] 1.414214\n\n\nLet us see if this is the case, by overlaying the theoretical normal distribution over the histogram of differences:\n\nhist(diff_cell_Rg, breaks = 100, freq = F, xlim = c(-3, 10))\nlines(x = seq(-3, 10, 0.1), y = dnorm(seq(-3, 10, 0.1), m = 2, sd = 1.414), lwd = 3, col = \"red\", xlim = c(-3, 10))\n\n\n\n\nThe probability that muscle has larger Rg than fibroblasts is the probability that the difference is &gt; 0.\nWe can use the theoritical distribution to figure this out:\n\npnorm(0, mean = 2, sd = 1.414, lower.tail = F)\n\n[1] 0.9213817\n\n\nSo 92% of muscle cells will have bigger Rg than fibroblasts.\nNow, here again are the theeoretical distributions of the two cell types:\n\nplot(cell_size_list, dist_cell_size_muscle, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5), type = \"l\")\nlines(cell_size_list, dist_cell_size_fib, col = \"blue\", xlim = c(0, 12), ylim = c(0, 0.5))\nabline(v = 7, col = \"red\")\nabline(v = 5, col = \"blue\")\n\n\n\n\nTo ask if muscle has significantly higher Rg, you could ask, what is the probability of seeing an Rg in fibroblasts that this higher than mean Rg of muscle:\n\npnorm(7, mean = 5, sd = 1, lower.tail = F)\n\n[1] 0.02275013\n\n\n2.2% of fibroblasts have Rg greater than or equal to the mean Rg of muscle.\n(Is this the p-value?)\nBelow we will go through two methods to determine the statistical significance, or p-value of this comparison."
  },
  {
    "objectID": "content/bootcamp/stats/class-02.slides.html#bootstrapping",
    "href": "content/bootcamp/stats/class-02.slides.html#bootstrapping",
    "title": "Stats Class 02",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nTo get statistical significance, we ask: “what is the probability of the observed measure occuring in a null distribution?”\nWe could either use a theoretical null distribution or create our own using existing data.\nWhat if we didn’t know the underlying distribution of the data or we didn’t care? Biological systems are complex and many times, the standard theoretical distributions will not capture the observed distribution.\nWith increasing number of data points and increasing computing power, we don’t need to rely on theoretical distributions.\nWe can create our own multiverse (a.k.a the null distribution) with resampling/bootstrapping.\nHere are the steps involved in bootstrapping:\n\nWe will aggregate our observations into a new vector. This is the starting point of creating our multiverse.\n\n\nall_obs &lt;- c(muscle_rg, fib_rg)\n\n\nWhen we sample 100 observations from “all_obs” randomly, we create a new Rg dataset sampled from the combined data:\n\n\ntmp_muscle_rg &lt;- sample(all_obs, 100, replace = T)\ntmp_fib_rg &lt;- sample(all_obs, 100, replace = T)\n\nWe could do this without replacement too:\n\nidx_b &lt;- sample(1:200, 100)\nidx_a &lt;- setdiff(1:200, idx_b)\ntmp_muscle_rg &lt;- all_obs[idx_a]\ntmp_fib_rg &lt;- all_obs[idx_b]\n\n\nWe then calculate the difference in mean of the two sampled datasets. Here, we just performed an experiment where we sampled two sets of Rg from a single distribution and calculated the differences in mean between the two sets.\n\n\ntmp_diff &lt;- mean(tmp_muscle_rg) - mean(tmp_fib_rg)\ntmp_diff\n\n[1] 0.3208043\n\n\n\nWe repeat this many times (here let us do 1000 repetitions) to get the null distribution of the differences in mean. The null distribution assumes that the Rg for muscle and fibroblasts were drawn from the same distribution:\n\n\n# initialize a vector with 0s\nbs_mean_diff &lt;- rep(0, 1000)\nfor (i in 1:1000) {\n  tmp_muscle_rg &lt;- sample(all_obs, 100, replace = T)\n  tmp_fib_rg &lt;- sample(all_obs, 100, replace = T)\n  bs_mean_diff[i] &lt;- mean(tmp_muscle_rg) - mean(tmp_fib_rg)\n  # if you want to sample without replacement:\n  # idx_b &lt;- sample(1:200,100)\n  # idx_a &lt;- setdiff(1:200,idx_b)\n  # bs_mean_diff[i] = mean(all_obs[idx_a]) - mean(all_obs[idx_b])\n}\n\nLet us see the distribution of the mean difference of random sampling of Rg with the observed difference in red:\n\nhist(bs_mean_diff, freq = F, xlim = c(-3, 3))\nabline(v = mean(muscle_rg) - mean(fib_rg), col = \"red\", lwd = 3, xlim = c(-3, 3))\n\n\n\n\nThe observed difference is way higher than anything we saw in 1000 samples from the comobined Rg’s.\nHow often do we see a mean difference in sampling that is higher than observed?\n\nmean(bs_mean_diff &gt; (mean(muscle_rg) - mean(fib_rg)))\n\n[1] 0\n\n\nNever! So the difference in mean Rg we observe between muscle and fibroblasts is highly statistically significant. In other words, p=0\nBootstrapping requires no assumptions and can be applied to any dataset succesfully to determine if the same measure from two samples are significantly different.\nLet us do the same compaarison between fibroblasts and neurons:\n\nneuron_rg &lt;- data[data$Cell_Type == \"Neuron\", ]$Cell_Rg\nall_obs &lt;- c(neuron_rg, fib_rg)\nbs_mean_diff &lt;- rep(0, 10000)\nfor (i in 1:10000) {\n  # tmp_neuron_rg &lt;- sample(all_obs,100,replace = T)\n  # tmp_fib_rg &lt;- sample(all_obs,100,replace = T)\n  # bs_mean_diff[i] = mean(tmp_neuron_rg) - mean(tmp_fib_rg)\n  idx_b &lt;- sample(1:200, 100)\n  idx_a &lt;- setdiff(1:200, idx_b)\n  bs_mean_diff[i] &lt;- mean(all_obs[idx_a]) - mean(all_obs[idx_b])\n}\nhist(bs_mean_diff, freq = F, xlim = c(-1, 1))\nabline(v = mean(neuron_rg) - mean(fib_rg), col = \"red\", lwd = 3, xlim = c(-1, 1))\n\n\n\n\n\n# mean(neuron_rg)-mean(fib_rg)\nmean(bs_mean_diff &lt; (mean(neuron_rg) - mean(fib_rg)))\n\n[1] 0.0842\n\n\nHere, the probability of the difference in mean observed in the null distribution is 0.0824, higher than accepted level of significance (0.05)."
  },
  {
    "objectID": "content/bootcamp/stats/class-02.slides.html#non-parametric-tests",
    "href": "content/bootcamp/stats/class-02.slides.html#non-parametric-tests",
    "title": "Stats Class 02",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nAnother way to ask if the two samples are drawn from different distributions is to use non parametric statistical tests that have inbuilt null distributions. The two that will be useful in a wide range of scenarios are Kolmogrov Smirnov test and Wilcoxon test, both of which are easy to implement in R.\nThe “p.value” variable gives the probability that the two samples were drawn from the same underlying distribution. Hence, if p.value is less than 0.05 (or a more stringent cutoff), it means the two samples are significantly different.\nKS test:\n\nks.test(muscle_rg, fib_rg)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  muscle_rg and fib_rg\nD = 0.7, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\nks.test(neuron_rg, fib_rg)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  neuron_rg and fib_rg\nD = 0.1, p-value = 0.6994\nalternative hypothesis: two-sided\n\n\nWilcoxon test:\n\nwilcox.test(muscle_rg, fib_rg)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  muscle_rg and fib_rg\nW = 9227, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\nwilcox.test(neuron_rg, fib_rg)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  neuron_rg and fib_rg\nW = 4444, p-value = 0.1747\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.slides.html#overlaps",
    "href": "content/bootcamp/stats/class-02.slides.html#overlaps",
    "title": "Stats Class 02",
    "section": "Overlaps",
    "text": "Overlaps\nWe come across the question of overlaps quite a bit in biological datasets.\nA typical question would be: “Does my list of upregulated genes overlap with [my favorite] category of genes?”\n(Or the dreaded Reviewer comment “You should do a GO analysis” which means you have to add a supplementary figure no one cares about.)\nLet us take an experiment where you measured change in gene expression and categorized each gene as either “Upregulaated” or “Not_Upregulated”.\nYou hypothesize that your intervention affects cell cycle genes, so you get a list of cell cycle genes from a database and categorize each gene in your gene list as either cell cycle gene or not.\nIf the two categories have nothing to do with each other, then they co-occur just by chance. They would be “independent” of each other and we can calculate the joint probability for such a scenario as follows:\nIf Cell_Cycle and Upregulation are independent:\nP(Cell_Cycle \\(\\bigcap\\) Upregulation) = P(Cell_Cycle)*P(Upregulation)\nWe now have an observed probability of intetrsection from your experiment. So we can determine the ratio of observed over expected:\nP(Cell_Cycle \\(\\bigcap\\) Upregulation) / ( P(Cell_Cycle)*P(Upregulation) )\nMore the ratio greater than 1, more significant the intersection between the two categories.\nLet us look at such an experiment:\n\noverlap_dat &lt;- read.csv(file = \"data/Gene_Association.csv\", sep = \",\")\nhead(overlap_dat)\n\n    Gene Cell_Cycle_Gene  Exp_Change\n1 Gene_1      Cell_Cycle Upregulated\n2 Gene_2      Cell_Cycle Upregulated\n3 Gene_3      Cell_Cycle Upregulated\n4 Gene_4      Cell_Cycle Upregulated\n5 Gene_5      Cell_Cycle Upregulated\n6 Gene_6      Cell_Cycle Upregulated\n\n\nLet us calculate individual probabilities and joint probability:\n\nct_total &lt;- nrow(overlap_dat)\nct_cell_cycle &lt;- nrow(overlap_dat[overlap_dat$Cell_Cycle_Gene == \"Cell_Cycle\", ])\n\np_cell_cycle &lt;- ct_cell_cycle / ct_total\n\nct_upreg &lt;- nrow(overlap_dat[overlap_dat$Exp_Change == \"Upregulated\", ])\n\np_upreg &lt;- ct_upreg / ct_total\n\nct_both &lt;- nrow(overlap_dat[overlap_dat$Cell_Cycle_Gene == \"Cell_Cycle\" &\n  overlap_dat$Exp_Change == \"Upregulated\", ])\np_joint &lt;- ct_both / ct_total\n\nnoquote(paste0(\"p_cell_cycle \", p_cell_cycle))\n\n[1] p_cell_cycle 0.15\n\nnoquote(paste0(\"p_upreg \", p_upreg))\n\n[1] p_upreg 0.19\n\nnoquote(paste0(\"p_joint \", p_joint))\n\n[1] p_joint 0.11\n\nexcess &lt;- p_joint / (p_cell_cycle * p_upreg)\nnoquote(paste0(\"Observed/Expected = \", excess))\n\n[1] Observed/Expected = 3.85964912280702\n\n\nIs this statistically significant?\nWe will use the Fisher exact test* to get the significance of this observation, or what is the probability of observing this overlap from two randomly distributed variables.\n*(chi squared test and hypergeometric test can also be used)\nFirst we will generate a contingency table:\n\nc_table &lt;- table(overlap_dat$Cell_Cycle_Gene, overlap_dat$Exp_Change)\nc_table\n\n                \n                 Not_Upregulated Upregulated\n  Cell_Cycle                   4          11\n  Not_Cell_Cycle              77           8\n\n\nYou can see that off diagonal seems enriched already.\n\nfisher.test(c_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  c_table\np-value = 5.137e-07\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.007467239 0.172362221\nsample estimates:\nodds ratio \n0.04022862 \n\n\nThe overlap is highly significant.\nUsing bootstrap\nWe can estimate significance alternatively using bootstrapping:\n\n# make a copy of the overlap_dat dataframe so we can shuffle\n# the cell_cycle column each repetition of the bootstrap.\n\ntemp_overlap_df &lt;- overlap_dat\n\n# initialize vector that will contain number of genes that\n# have both a cell cycle category and are upregulated\n\noverlap_ct &lt;- rep(0, 1000)\nfor (i in 1:1000) {\n  # shuffle Cell_Cycle_Gene column\n  temp_overlap_df$Cell_Cycle_Gene &lt;- sample(overlap_dat$Cell_Cycle_Gene, nrow(overlap_dat))\n  # count overlap\n  overlap_ct[i] &lt;- nrow(temp_overlap_df[temp_overlap_df$Cell_Cycle_Gene == \"Cell_Cycle\" &\n    temp_overlap_df$Exp_Change == \"Upregulated\", ])\n}\n\nWe have generated a distribution of number of genes that would have overlapping cell cycle category and upregulation category by chance. Now let us compare the distribution with what we observe:\n\nhist(overlap_ct, freq = F, xlim = c(0, 12), breaks = 10)\n\nabline(v = ct_both, col = \"red\", lwd = 3, xlim = c(0, 12))\n\n\nAnd the p-value from bootstrapping:\n\nmean(overlap_ct &gt; ct_both)\n\n[1] 0\n\n\nSo the bootstrap p-value is 0."
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html",
    "href": "content/bootcamp/stats/class-02.html",
    "title": "Stats Class 02",
    "section": "",
    "text": "Understand how to determine if two samples come from different distributions\n\nUsing theoretical distributions\nUsing resampling/bootstrap\nUsing non-parametric tests\n\nLearn how to determine if overlap of two categories is significant"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#class-objectives",
    "href": "content/bootcamp/stats/class-02.html#class-objectives",
    "title": "Stats Class 02",
    "section": "",
    "text": "Understand how to determine if two samples come from different distributions\n\nUsing theoretical distributions\nUsing resampling/bootstrap\nUsing non-parametric tests\n\nLearn how to determine if overlap of two categories is significant"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#continuing-from-last-class",
    "href": "content/bootcamp/stats/class-02.html#continuing-from-last-class",
    "title": "Stats Class 02",
    "section": "Continuing from last class",
    "text": "Continuing from last class\nLet us start by reading in the dataset:\n\n# read dataset\ndata &lt;- read.csv(file = \"data/cell_dimensions.tsv\", sep = \"\\t\", stringsAsFactors = T)\nhead(data)\n\n   Cell_Rg Cell_Len   Cell_Type\n1 2.441927 4.143111 Fibroblasts\n2 2.832714 6.648487 Fibroblasts\n3 3.085169 7.822642 Fibroblasts\n4 3.243857 7.997854 Fibroblasts\n5 3.359737 7.448261 Fibroblasts\n6 3.434748 7.800078 Fibroblasts\n\n\nWe generated observed and theoretical distributions of Rg. First muscle:\n\n# Get muscle Rg as a vector:\nmuscle_rg &lt;- data[data$Cell_Type == \"Muscle\", ]$Cell_Rg\n# generate histogram\nmuscle_rg_hist &lt;- hist(muscle_rg, breaks = 30, plot = F)\n# Use seq to create a range of numbers\ncell_size_list &lt;- seq(0, 12, 0.2)\n# Use dnorm to calculate probability at each point of the vector above\ndist_cell_size_muscle &lt;- dnorm(cell_size_list, mean = 7, sd = 1)\nplot(muscle_rg_hist$mids, muscle_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_muscle, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\nNext, fibroblasts:\n\n# Get Fib Rg as a vector:\nfib_rg &lt;- data[data$Cell_Type == \"Fibroblasts\", ]$Cell_Rg\n# generate histogram\nfib_rg_hist &lt;- hist(fib_rg, breaks = 30, plot = F)\n# Use dnorm to calculate probability at each point of the cell_size_list vector\ndist_cell_size_fib &lt;- dnorm(cell_size_list, mean = 5, sd = 1)\nplot(fib_rg_hist$mids, fib_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_fib, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\nWe are confident that Rg is sampled from Normal distributions, so we can take advantage of knowing exact details of the theoretical distributions. We could work out the math using formulae, or we could sample the distribution to get an intuitive feel.\nThe question we are asking is: Is the Rg of muscle different from Rg of fibroblasts?\nWe now know that the Rg is sampled from normal distribution as follows:\n\nMuscle - mean = 7, sd = 1\nFibroblasts - mean = 5, sd = 1\n\nThe advantage of having a theoretical distribution is that we can calculate exact differences. We will use 10,000 points sampled from each distribution to illustrate this. Let us generate 10,000 points from these distributions:\n\ntheoretical_fib &lt;- rnorm(10000, mean = 5, sd = 1)\ntheoretical_muscle &lt;- rnorm(10000, mean = 7, sd = 1)\n\nThe difference in Rg between the two cell types would be simply:\n\ndiff_cell_Rg &lt;- theoretical_muscle - theoretical_fib\n\nWhat is the distribution of the difference?\n\nhist(diff_cell_Rg, breaks = 100, freq = F)\n\n\n\n\nThe difference is also a Normal distribution!\nThe mean of this distribution is simply difference in mean of the starting distributions: 7-5 = 2. The sd is the square root of sum of squares of the sd of the starting distributions:\n\nsqrt(1^2 + 1^2)\n\n[1] 1.414214\n\n\nLet us see if this is the case, by overlaying the theoretical normal distribution over the histogram of differences:\n\nhist(diff_cell_Rg, breaks = 100, freq = F, xlim = c(-3, 10))\nlines(x = seq(-3, 10, 0.1), y = dnorm(seq(-3, 10, 0.1), m = 2, sd = 1.414), lwd = 3, col = \"red\", xlim = c(-3, 10))\n\n\n\n\nThe probability that muscle has larger Rg than fibroblasts is the probability that the difference is &gt; 0.\nWe can use the theoritical distribution to figure this out:\n\npnorm(0, mean = 2, sd = 1.414, lower.tail = F)\n\n[1] 0.9213817\n\n\nSo 92% of muscle cells will have bigger Rg than fibroblasts.\nNow, here again are the theeoretical distributions of the two cell types:\n\nplot(cell_size_list, dist_cell_size_muscle, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5), type = \"l\")\nlines(cell_size_list, dist_cell_size_fib, col = \"blue\", xlim = c(0, 12), ylim = c(0, 0.5))\nabline(v = 7, col = \"red\")\nabline(v = 5, col = \"blue\")\n\n\n\n\nTo ask if muscle has significantly higher Rg, you could ask, what is the probability of seeing an Rg in fibroblasts that this higher than mean Rg of muscle:\n\npnorm(7, mean = 5, sd = 1, lower.tail = F)\n\n[1] 0.02275013\n\n\n2.2% of fibroblasts have Rg greater than or equal to the mean Rg of muscle.\n(Is this the p-value?)\nBelow we will go through two methods to determine the statistical significance, or p-value of this comparison."
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#bootstrapping",
    "href": "content/bootcamp/stats/class-02.html#bootstrapping",
    "title": "Stats Class 02",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nTo get statistical significance, we ask: “what is the probability of the observed measure occuring in a null distribution?”\nWe could either use a theoretical null distribution or create our own using existing data.\nWhat if we didn’t know the underlying distribution of the data or we didn’t care? Biological systems are complex and many times, the standard theoretical distributions will not capture the observed distribution.\nWith increasing number of data points and increasing computing power, we don’t need to rely on theoretical distributions.\nWe can create our own multiverse (a.k.a the null distribution) with resampling/bootstrapping.\nHere are the steps involved in bootstrapping:\n\nWe will aggregate our observations into a new vector. This is the starting point of creating our multiverse.\n\n\nall_obs &lt;- c(muscle_rg, fib_rg)\n\n\nWhen we sample 100 observations from “all_obs” randomly, we create a new Rg dataset sampled from the combined data:\n\n\ntmp_muscle_rg &lt;- sample(all_obs, 100, replace = T)\ntmp_fib_rg &lt;- sample(all_obs, 100, replace = T)\n\nWe could do this without replacement too:\n\nidx_b &lt;- sample(1:200, 100)\nidx_a &lt;- setdiff(1:200, idx_b)\ntmp_muscle_rg &lt;- all_obs[idx_a]\ntmp_fib_rg &lt;- all_obs[idx_b]\n\n\nWe then calculate the difference in mean of the two sampled datasets. Here, we just performed an experiment where we sampled two sets of Rg from a single distribution and calculated the differences in mean between the two sets.\n\n\ntmp_diff &lt;- mean(tmp_muscle_rg) - mean(tmp_fib_rg)\ntmp_diff\n\n[1] 0.02240372\n\n\n\nWe repeat this many times (here let us do 1000 repetitions) to get the null distribution of the differences in mean. The null distribution assumes that the Rg for muscle and fibroblasts were drawn from the same distribution:\n\n\n# initialize a vector with 0s\nbs_mean_diff &lt;- rep(0, 1000)\nfor (i in 1:1000) {\n  tmp_muscle_rg &lt;- sample(all_obs, 100, replace = T)\n  tmp_fib_rg &lt;- sample(all_obs, 100, replace = T)\n  bs_mean_diff[i] &lt;- mean(tmp_muscle_rg) - mean(tmp_fib_rg)\n  # if you want to sample without replacement:\n  # idx_b &lt;- sample(1:200,100)\n  # idx_a &lt;- setdiff(1:200,idx_b)\n  # bs_mean_diff[i] = mean(all_obs[idx_a]) - mean(all_obs[idx_b])\n}\n\nLet us see the distribution of the mean difference of random sampling of Rg with the observed difference in red:\n\nhist(bs_mean_diff, freq = F, xlim = c(-3, 3))\nabline(v = mean(muscle_rg) - mean(fib_rg), col = \"red\", lwd = 3, xlim = c(-3, 3))\n\n\n\n\nThe observed difference is way higher than anything we saw in 1000 samples from the comobined Rg’s.\nHow often do we see a mean difference in sampling that is higher than observed?\n\nmean(bs_mean_diff &gt; (mean(muscle_rg) - mean(fib_rg)))\n\n[1] 0\n\n\nNever! So the difference in mean Rg we observe between muscle and fibroblasts is highly statistically significant. In other words, p=0\nBootstrapping requires no assumptions and can be applied to any dataset succesfully to determine if the same measure from two samples are significantly different.\nLet us do the same compaarison between fibroblasts and neurons:\n\nneuron_rg &lt;- data[data$Cell_Type == \"Neuron\", ]$Cell_Rg\nall_obs &lt;- c(neuron_rg, fib_rg)\nbs_mean_diff &lt;- rep(0, 10000)\nfor (i in 1:10000) {\n  # tmp_neuron_rg &lt;- sample(all_obs,100,replace = T)\n  # tmp_fib_rg &lt;- sample(all_obs,100,replace = T)\n  # bs_mean_diff[i] = mean(tmp_neuron_rg) - mean(tmp_fib_rg)\n  idx_b &lt;- sample(1:200, 100)\n  idx_a &lt;- setdiff(1:200, idx_b)\n  bs_mean_diff[i] &lt;- mean(all_obs[idx_a]) - mean(all_obs[idx_b])\n}\nhist(bs_mean_diff, freq = F, xlim = c(-1, 1))\nabline(v = mean(neuron_rg) - mean(fib_rg), col = \"red\", lwd = 3, xlim = c(-1, 1))\n\n\n\n\n\n# mean(neuron_rg)-mean(fib_rg)\nmean(bs_mean_diff &lt; (mean(neuron_rg) - mean(fib_rg)))\n\n[1] 0.081\n\n\nHere, the probability of the difference in mean observed in the null distribution is 0.0824, higher than accepted level of significance (0.05)."
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#non-parametric-tests",
    "href": "content/bootcamp/stats/class-02.html#non-parametric-tests",
    "title": "Stats Class 02",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nAnother way to ask if the two samples are drawn from different distributions is to use non parametric statistical tests that have inbuilt null distributions. The two that will be useful in a wide range of scenarios are Kolmogrov Smirnov test and Wilcoxon test, both of which are easy to implement in R.\nThe “p.value” variable gives the probability that the two samples were drawn from the same underlying distribution. Hence, if p.value is less than 0.05 (or a more stringent cutoff), it means the two samples are significantly different.\nKS test:\n\nks.test(muscle_rg, fib_rg)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  muscle_rg and fib_rg\nD = 0.7, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\nks.test(neuron_rg, fib_rg)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  neuron_rg and fib_rg\nD = 0.1, p-value = 0.6994\nalternative hypothesis: two-sided\n\n\nWilcoxon test:\n\nwilcox.test(muscle_rg, fib_rg)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  muscle_rg and fib_rg\nW = 9227, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\nwilcox.test(neuron_rg, fib_rg)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  neuron_rg and fib_rg\nW = 4444, p-value = 0.1747\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#overlaps",
    "href": "content/bootcamp/stats/class-02.html#overlaps",
    "title": "Stats Class 02",
    "section": "Overlaps",
    "text": "Overlaps\nWe come across the question of overlaps quite a bit in biological datasets.\nA typical question would be: “Does my list of upregulated genes overlap with [my favorite] category of genes?”\n(Or the dreaded Reviewer comment “You should do a GO analysis” which means you have to add a supplementary figure no one cares about.)\nLet us take an experiment where you measured change in gene expression and categorized each gene as either “Upregulaated” or “Not_Upregulated”.\nYou hypothesize that your intervention affects cell cycle genes, so you get a list of cell cycle genes from a database and categorize each gene in your gene list as either cell cycle gene or not.\nIf the two categories have nothing to do with each other, then they co-occur just by chance. They would be “independent” of each other and we can calculate the joint probability for such a scenario as follows:\nIf Cell_Cycle and Upregulation are independent:\nP(Cell_Cycle \\(\\bigcap\\) Upregulation) = P(Cell_Cycle)*P(Upregulation)\nWe now have an observed probability of intetrsection from your experiment. So we can determine the ratio of observed over expected:\nP(Cell_Cycle \\(\\bigcap\\) Upregulation) / ( P(Cell_Cycle)*P(Upregulation) )\nMore the ratio greater than 1, more significant the intersection between the two categories.\nLet us look at such an experiment:\n\noverlap_dat &lt;- read.csv(file = \"data/Gene_Association.csv\", sep = \",\")\nhead(overlap_dat)\n\n    Gene Cell_Cycle_Gene  Exp_Change\n1 Gene_1      Cell_Cycle Upregulated\n2 Gene_2      Cell_Cycle Upregulated\n3 Gene_3      Cell_Cycle Upregulated\n4 Gene_4      Cell_Cycle Upregulated\n5 Gene_5      Cell_Cycle Upregulated\n6 Gene_6      Cell_Cycle Upregulated\n\n\nLet us calculate individual probabilities and joint probability:\n\nct_total &lt;- nrow(overlap_dat)\nct_cell_cycle &lt;- nrow(overlap_dat[overlap_dat$Cell_Cycle_Gene == \"Cell_Cycle\", ])\n\np_cell_cycle &lt;- ct_cell_cycle / ct_total\n\nct_upreg &lt;- nrow(overlap_dat[overlap_dat$Exp_Change == \"Upregulated\", ])\n\np_upreg &lt;- ct_upreg / ct_total\n\nct_both &lt;- nrow(overlap_dat[overlap_dat$Cell_Cycle_Gene == \"Cell_Cycle\" &\n  overlap_dat$Exp_Change == \"Upregulated\", ])\np_joint &lt;- ct_both / ct_total\n\nnoquote(paste0(\"p_cell_cycle \", p_cell_cycle))\n\n[1] p_cell_cycle 0.15\n\nnoquote(paste0(\"p_upreg \", p_upreg))\n\n[1] p_upreg 0.19\n\nnoquote(paste0(\"p_joint \", p_joint))\n\n[1] p_joint 0.11\n\nexcess &lt;- p_joint / (p_cell_cycle * p_upreg)\nnoquote(paste0(\"Observed/Expected = \", excess))\n\n[1] Observed/Expected = 3.85964912280702\n\n\nIs this statistically significant?\nWe will use the Fisher exact test* to get the significance of this observation, or what is the probability of observing this overlap from two randomly distributed variables.\n*(chi squared test and hypergeometric test can also be used)\nFirst we will generate a contingency table:\n\nc_table &lt;- table(overlap_dat$Cell_Cycle_Gene, overlap_dat$Exp_Change)\nc_table\n\n                \n                 Not_Upregulated Upregulated\n  Cell_Cycle                   4          11\n  Not_Cell_Cycle              77           8\n\n\nYou can see that off diagonal seems enriched already.\n\nfisher.test(c_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  c_table\np-value = 5.137e-07\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.007467239 0.172362221\nsample estimates:\nodds ratio \n0.04022862 \n\n\nThe overlap is highly significant.\n\nUsing bootstrap\nWe can estimate significance alternatively using bootstrapping:\n\n# make a copy of the overlap_dat dataframe so we can shuffle\n# the cell_cycle column each repetition of the bootstrap.\n\ntemp_overlap_df &lt;- overlap_dat\n\n# initialize vector that will contain number of genes that\n# have both a cell cycle category and are upregulated\n\noverlap_ct &lt;- rep(0, 1000)\nfor (i in 1:1000) {\n  # shuffle Cell_Cycle_Gene column\n  temp_overlap_df$Cell_Cycle_Gene &lt;- sample(overlap_dat$Cell_Cycle_Gene, nrow(overlap_dat))\n  # count overlap\n  overlap_ct[i] &lt;- nrow(temp_overlap_df[temp_overlap_df$Cell_Cycle_Gene == \"Cell_Cycle\" &\n    temp_overlap_df$Exp_Change == \"Upregulated\", ])\n}\n\nWe have generated a distribution of number of genes that would have overlapping cell cycle category and upregulation category by chance. Now let us compare the distribution with what we observe:\n\nhist(overlap_ct, freq = F, xlim = c(0, 12), breaks = 10)\n\nabline(v = ct_both, col = \"red\", lwd = 3, xlim = c(0, 12))\n\n\n\n\nAnd the p-value from bootstrapping:\n\nmean(overlap_ct &gt; ct_both)\n\n[1] 0\n\n\nSo the bootstrap p-value is 0."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#genomics---lots-of-data---lots-of-hypothesis-tests",
    "href": "content/bootcamp/stats/class-05.slides.html#genomics---lots-of-data---lots-of-hypothesis-tests",
    "title": "Bootcamp: Stats class 5",
    "section": "Genomics -> Lots of Data -> Lots of Hypothesis Tests",
    "text": "Genomics -&gt; Lots of Data -&gt; Lots of Hypothesis Tests\nIn a typical RNA-seq experiment, we test ~10K different hypotheses. For example, you have 10K genes and for each gene you test whether the mean expression changed in condition A vs condition B. Using a standard p-value cut-off of 0.05, we’d expect 500 genes to be deemed “significant” by chance. Thus, we concerned about False Positives or Type I Errors.\n\nSo, we want to control our type 1 error. We can approach this in two different ways.\n\nControl overall α (also known as family-wise error rate or FWER), which will affect the α* for each test. That is, we are controlling the overall probability of making at least one false discovery. Bonferroni and Sidak corrections all control FWER.\nControl false discovery rate (FDR). Where FWER controls for the probability for making a type 1 error at all, these procedures allow for type 1 errors (false positives) but control the proportion of these false positives in relation to true positives. This is done by adjusting the decision made for the p-value associated with each individual test to decide rejection or not. Because this will result in a higher type 1 error rate, it has higher power. This affords a higher probability of true discoveries. The step procedures control for FDR.\n\nBonferroni Correction\n**The most conservative of corrections, the Bonferroni correction is also perhaps the most straightforward in its approach. Simply divide α by the number of tests (m).\nHowever, with many tests, α* will become very small. This reduces power, which means that we are very unlikely to make any true discoveries.\nSidak Correction\n**α* = 1-(1-α)^(1/m)\nHolm’s Step-Down Procedure\n**An update of the Bonferroni correction, this procedure is more powerful. Rather than controlling the FWER, Holm’s procedure controls for the false discovery rate (FDR) and is performed after conducting all hypothesis tests and finding associated p-values at α within a set.\nThe step-down procedure is best illustrated with an example. Say we have three hypotheses, each with the associated p-value:\nH1: 0.025\nH2: 0.003\nH3: 0.01\nStep 1: Order p-values from smallest to greatest\nH2: 0.003\nH3: 0.01\nH1: 0.025\nStep 2: Use the Holm-Bonferroni formula for the first-ranked (smallest) p-value\nα* = α/(n-rank+1)\nα* = 0.05/(3–1+1) = 0.0167\nStep 3: Compare the first-ranked p-value with the α* calculated from Step 2\n0.003 &lt; 0.0167\nBecause the p-value for H2 is less than the calculated α*, we can reject H2.\nMove onto the next ranked p-value and repeat steps 2–3, calculating the α* for its respective rank and comparing it to that p-value. Continue until you reach the first non-rejected hypothesis. You would then fail to reject all following hypotheses.\nHochberg’s Step-Up Procedure\n**More powerful than Holm’s step-down procedure, Hochberg’s step-up procedure also seeks to control the FDR and follows a similar process, only p-values are ranked from largest to smallest.\nFor each ranked p-value, it is compared to the α* calculated for its respective rank (same formula as Holm’s procedure). Testing continues until you reach the first non-rejected hypothesis. You would then fail to reject all following hypotheses.\n\nrna &lt;- read_csv(file = \"data/deltaRNA_test.csv\", show_col_types = FALSE) %&gt;%\n  select(gene_id, pvalue) %&gt;%\n  na.omit()\n\nrna$fdr &lt;- p.adjust(p = rna$pvalue, method = \"fdr\", n = nrow(rna))\n\nrna$BH &lt;- p.adjust(p = rna$pvalue, method = \"BH\", n = nrow(rna))\n\nrna$bon &lt;- p.adjust(p = rna$pvalue, method = \"bonferroni\", n = nrow(rna))\n\n\nrna_long &lt;- rna %&gt;% pivot_longer(cols = pvalue:bon, names_to = \"type\")\n\n\n\np_none &lt;- ggplot(data = rna, aes(x = pvalue, y = pvalue)) +\n  geom_point(size = .1) +\n  ggtitle(\"None\") +\n  theme_minimal()\n\n\np_FDR &lt;- ggplot(data = rna, aes(x = pvalue, y = fdr)) +\n  geom_point(size = .1) +\n  ggtitle(\"FDR\") +\n  theme_minimal()\n\np_BH &lt;- ggplot(data = rna, aes(x = pvalue, y = BH)) +\n  geom_point(size = .1) +\n  ggtitle(\"BH\") +\n  theme_minimal()\n\np_bon &lt;- ggplot(data = rna, aes(x = pvalue, y = bon)) +\n  geom_point(size = .1) +\n  ggtitle(\"Bonferroni\") +\n  theme_minimal()\n\nplot_grid(p_none, p_bon, p_BH, p_FDR, ncol = 2, nrow = 2)\n\n\n\nggplot(data = rna_long, aes(x = value, color = type)) +\n  stat_ecdf() +\n  theme_minimal() +\n  xlab(\"p-values\") +\n  ylab(\"cumulative fraction\")"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#bayes-rule",
    "href": "content/bootcamp/stats/class-05.slides.html#bayes-rule",
    "title": "Bootcamp: Stats class 5",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nThe conditional probability of the event \\(A\\) conditional on the event \\(B\\) is given by \\[\n  P(A \\mid B) = \\frac{P(A \\,\\&\\, B)}{P(B)}.\n\\]\nThis section introduces how the Bayes’ rule is applied to calculating conditional probability, and several real-life examples are demonstrated. Finally, we compare the Bayesian and frequentist definition of probability.\nBayes’ Rule and Diagnostic Testing\nTo better understand conditional probabilities and their importance, let us consider an example involving the human immunodeficiency virus (HIV). In the early 1980s, HIV had just been discovered and was rapidly expanding. There was major concern with the safety of the blood supply. Also, virtually no cure existed making an HIV diagnosis basically a death sentence, in addition to the stigma that was attached to the disease.\nThese made false positives and false negatives in HIV testing highly undesirable. A false positive is when a test returns postive while the truth is negative. That would for instance be that someone without HIV is wrongly diagnosed with HIV, wrongly telling that person they are going to die and casting the stigma on them. A false negative is when a test returns negative while the truth is positive. That is when someone with HIV undergoes an HIV test which wrongly comes back negative. The latter poses a threat to the blood supply if that person is about to donate blood.\nThe probability of a false positive if the truth is negative is called the false positive rate. Similarly, the false negative rate is the probability of a false negative if the truth is positive. Note that both these rates are conditional probabilities: The false positive rate of an HIV test is the probability of a positive result conditional on the person tested having no HIV.\nThe HIV test we consider is an enzyme-linked immunosorbent assay, commonly known as an ELISA. We would like to know the probability that someone (in the early 1980s) has HIV if ELISA tests positive. For this, we need the following information. ELISA’s true positive rate (one minus the false negative rate), also referred to as sensitivity, recall, or probability of detection, is estimated as\n\\[\n  P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 93\\% = 0.93.\n\\]\nIts true negative rate (one minus the false positive rate), also referred to as specificity, is estimated as\n\\[\n  P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV}) = 99\\% = 0.99.\n\\]\nAlso relevant to our question is the prevalence of HIV in the overall population, which is estimated to be 1.48 out of every 1000 American adults. We therefore assume\n\\[\\begin{equation}\n  P(\\text{Person tested has HIV}) = \\frac{1.48}{1000} = 0.00148.\n  (\\#eq:HIVpositive)\n\\end{equation}\\]\nNote that the above numbers are estimates. For our purposes, however, we will treat them as if they were exact.\nOur goal is to compute the probability of HIV if ELISA is positive, that is \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})\\). In none of the above numbers did we condition on the outcome of ELISA. Fortunately, Bayes’ rule allows is to use the above numbers to compute the probability we seek. Bayes’ rule states that\n\\[\\begin{equation}\n  P(\\text{Person tested has HIV}  \\mid \\text{ELISA is positive}) = \\frac{P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive})}{P(\\text{ELISA is positive})}.\n   (\\#eq:HIVconditional)\n\\end{equation}\\]\nThis can be derived as follows. For someone to test positive and be HIV positive, that person first needs to be HIV positive and then secondly test positive. The probability of the first thing happening is \\(P(\\text{HIV positive}) = 0.00148\\). The probability of then testing positive is \\(P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 0.93\\), the true positive rate. This yields for the numerator\n\\[\\begin{multline}\n  P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) \\\\\n  &= 0.00148 \\cdot 0.93\n  = 0.0013764.\n  \\end{split}\n  (\\#eq:HIVjoint)\n\\end{multline}\\]\nThe first step in the above equation is implied by Bayes’ rule: By multiplying the left- and right-hand side of Bayes’ rule by \\(P(B)\\), we obtain \\[\n  P(A \\mid B) P(B) = P(A \\,\\&\\, B).\n\\]\nThe denominator in @ref(eq:HIVconditional) can be expanded as\n\\[\\begin{multline*}\n  P(\\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) + P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  &= 0.0013764 + 0.0099852 = 0.0113616\n  \\end{split}\n\\end{multline*}\\]\nwhere we used @ref(eq:HIVjoint) and\n\\[\\begin{multline*}\n  P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has no HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has no HIV}) \\\\\n  &= \\left(1 - P(\\text{Person tested has HIV})\\right) \\cdot \\left(1 - P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV})\\right) \\\\\n  &= \\left(1 - 0.00148\\right) \\cdot \\left(1 - 0.99\\right) = 0.0099852.\n  \\end{split}\n\\end{multline*}\\]\nPutting this all together and inserting into @ref(eq:HIVconditional) reveals \\[\\begin{equation}\n  P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) = \\frac{0.0013764}{0.0113616} \\approx 0.12.\n  (\\#eq:HIVresult)\n\\end{equation}\\] So even when the ELISA returns positive, the probability of having HIV is only 12%. An important reason why this number is so low is due to the prevalence of HIV. Before testing, one’s probability of HIV was 0.148%, so the positive test changes that probability dramatically, but it is still below 50%. That is, it is more likely that one is HIV negative rather than positive after one positive ELISA test.\nQuestions like the one we just answered (What is the probability of a disease if a test returns positive?) are crucial to make medical diagnoses. As we saw, just the true positive and true negative rates of a test do not tell the full story, but also a disease’s prevalence plays a role. Bayes’ rule is a tool to synthesize such numbers into a more useful probability of having a disease after a test result.\n\nWhat is the probability that someone who tests positive does not actually have HIV?\n\nWe found in @ref(eq:HIVresult) that someone who tests positive has a \\(0.12\\) probability of having HIV. That implies that the same person has a \\(1-0.12=0.88\\) probability of not having HIV, despite testing positive.\n\nIf the individual is at a higher risk for having HIV than a randomly sampled person from the population considered, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n\nIf the person has a priori a higher risk for HIV and tests positive, then the probability of having HIV must be higher than for someone not at increased risk who also tests positive. Therefore, \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &gt; 0.12\\) where \\(0.12\\) comes from @ref(eq:HIVresult).\nOne can derive this mathematically by plugging in a larger number in @ref(eq:HIVpositive) than 0.00148, as that number represents the prior risk of HIV. Changing the calculations accordingly shows \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &gt; 0.12\\).\n\nIf the false positive rate of the test is higher than 1%, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n\nIf the false positive rate increases, the probability of a wrong positive result increases. That means that a positive test result is more likely to be wrong and thus less indicative of HIV. Therefore, the probability of HIV after a positive ELISA goes down such that \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &lt; 0.12\\).\nBayes Updating\nIn the previous section, we saw that one positive ELISA test yields a probability of having HIV of 12%. To obtain a more convincing probability, one might want to do a second ELISA test after a first one comes up positive. What is the probability of being HIV positive if also the second ELISA test comes back positive?\nTo solve this problem, we will assume that the correctness of this second test is not influenced by the first ELISA, that is, the tests are independent from each other. This assumption probably does not hold true as it is plausible that if the first test was a false positive, it is more likely that the second one will be one as well. Nonetheless, we stick with the independence assumption for simplicity.\nIn the last section, we used \\(P(\\text{Person tested has HIV}) = 0.00148\\), see @ref(eq:HIVpositive), to compute the probability of HIV after one positive test. If we repeat those steps but now with \\(P(\\text{Person tested has HIV}) = 0.12\\), the probability that a person with one positive test has HIV, we exactly obtain the probability of HIV after two positive tests. Repeating the maths from the previous section, involving Bayes’ rule, gives\n\\[\\begin{multline}\n  P(\\text{Person tested has HIV} \\mid \\text{Second ELISA is also positive}) \\\\\n  \\begin{split}\n  &= \\frac{P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Person tested has HIV})}{P(\\text{Second ELISA is also positive})} \\\\\n  &= \\frac{0.12 \\cdot 0.93}{\n  \\begin{split}\n  &P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has HIV}) \\\\\n  &+ P(\\text{Person tested has no HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has no HIV})\n  \\end{split}\n  } \\\\\n  &= \\frac{0.1116}{0.12 \\cdot 0.93 + (1 - 0.12)\\cdot (1 - 0.99)} \\approx 0.93.\n  \\end{split}\n  (\\#eq:Bayes-updating)\n\\end{multline}\\]\nSince we are considering the same ELISA test, we used the same true positive and true negative rates as in Section @ref(sec:diagnostic-testing). We see that two positive tests makes it much more probable for someone to have HIV than when only one test comes up positive.\nThis process, of using Bayes’ rule to update a probability based on an event affecting it, is called Bayes’ updating. More generally, the what one tries to update can be considered ‘prior’ information, sometimes simply called the prior. The event providing information about this can also be data. Then, updating this prior using Bayes’ rule gives the information conditional on the data, also known as the posterior, as in the information after having seen the data. Going from the prior to the posterior is Bayes updating.\nThe probability of HIV after one positive ELISA, 0.12, was the posterior in the previous section as it was an update of the overall prevalence of HIV, @ref(eq:HIVpositive). However, in this section we answered a question where we used this posterior information as the prior. This process of using a posterior as prior in a new problem is natural in the Bayesian framework of updating knowledge based on the data."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#fathers-of-statistics",
    "href": "content/bootcamp/stats/class-05.slides.html#fathers-of-statistics",
    "title": "Bootcamp: Stats class 5",
    "section": "Fathers of statistics",
    "text": "Fathers of statistics\nThe torch was passed within the triumvirate of Galton, Pearson, and Fisher.\nSir Francis Galton (1822-1911)\n\n\n\nfrom galton.org\n\n\n\nDiscovered regression to the mean\nRe-discovered correlation and regression and discovered how to apply these in anthropology, psychology, and more\nDefined the concept of standard deviation\nEstablished the field of Eugenics in 1883\nDarwin’s cousin.\n\nGalton’s reasoning for coining the term eugenics:\n\n“We greatly want a brief word to express the science of improving stock, which…takes cognisance of all influences that tend in however remote a degree to give the more suitable races or strains of blood a better chance of prevailing speedily over the less suitable than they otherwise would have had.”\n\nKarl Pearson (1857-1936)\n\n\n\nhttps://www.britannica.com/biography/Karl-Pearson\n\n\nKarl Pearson was Galton’s protégé and directly or contributed to:\n\nDeveloped hypothesis testing\nDeveloped the use of p-values\nDefined the Chi-Squared test\nCorrelation coefficient\nPrinciple components analysis\n\nAlso authors of timeless “classics” such as:\nThe Woman’s Question\nNational Life from the standpoint of science\nIn the year Mein Kampf was published, Pearson wrote an article called:\nTHE PROBLEM OF ALIEN IMMIGRATION INTO GREAT BRITAIN, ILLUSTRATED BY AN EXAMINATION OF RUSSIAN AND POLISH JEWISH CHILDREN\nHere is an excerpt:\n\n“[they] will develop into a parasitic race…Taken on the average, and regarding both sexes, this alien Jewish population is somewhat inferior physically and mentally to the native population.”\n\nSir Ronald Aylmer Fisher (1890-1962)\n\n\n\nhttps://www.42evolution.org/ronald-a-fisher/\n\n\nFisher’s work in statistics established and promoted many important methods of statistical inference. His contributions include:\n\nEstablishing p = 0.05 as the normal threshold for significant p-values\nPromoting Maximum Likelihood Estimation\nDeveloping the ANalysis Of VAriance (ANOVA) The iris dataset (this seems an incredibly minor contribution but I use it daily)\nThe Genetical Theory of Natural Selection, which blended the work of Mendel and Darwin.\n\nThere is no lack of Fisher’s strong and consistent support for eugenics. Here is an example from as late as 1954.\n\n\n\nLetter from R.A. Fisher to R. Ruggles Gates. Ronald Fisher Archive. University of Adelaide."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#storytime-galton-laboratory",
    "href": "content/bootcamp/stats/class-05.slides.html#storytime-galton-laboratory",
    "title": "Bootcamp: Stats class 5",
    "section": "Storytime: Galton Laboratory",
    "text": "Storytime: Galton Laboratory\nGalton founded the Eugenics Record Office (1904)\nGalton Eugenics Laboratory as part of University College London (UCL). Created by Pearson and funded by Galton. (1907)\nGalton left UCL enough money to create a Chair in National Eugenics, filled by Pearson and then Fisher. Hell of a name for an endowed chair!\nAnnals of Human Genetics: It was established in 1925 Pearson as the Annals of Eugenics, and obtained its current name in 1954.\nGalton laboratory was incorporated into the Department of Eugenics, Biometry and Genetic at UCL in 1944.\nRenamed to the Department of Human Genetics and Biometry in 1966.\nBecame part of the Department of Biology at UCL in 1996.\nIn 2020: UCL renames three facilities that honoured prominent eugenicists\nThese horrendous views did not appear to be common at UCL in the 1930s. For example, they were not held by JBS Haldane, Egon Pearson (son of Karl), and Lionel Penrose.\nWhat about in the US?\nCSHL - Eugenics Archive\nU.S. Scientists’ Role in the Eugenics Movement (1907–1939): A Contemporary Biologist’s Perspective\nCharles Davenport (first director of CSHL) and the Carnegie Insitution\nCold Spring Harbor and German Eugenics in the 1930s\nEugenics and the history of Science and AAAS"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#section",
    "href": "content/bootcamp/stats/class-05.slides.html#section",
    "title": "Bootcamp: Stats class 5",
    "section": "",
    "text": "from “America’s Shameful History of Eugenics and Forced Sterilizations”"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#modern-day-eugenics-and-beyond",
    "href": "content/bootcamp/stats/class-05.slides.html#modern-day-eugenics-and-beyond",
    "title": "Bootcamp: Stats class 5",
    "section": "Modern day: Eugenics and beyond",
    "text": "Modern day: Eugenics and beyond\n[Sordid genealogies: a conjectural history of Cambridge Analytica’s\neugenic roots](https://www.nature.com/articles/s41599-020-0505-5)\nAmerican Renaissance\n\n‘Race’ cannot be biologically defined due to genetic variation among human individuals and populations. (A) The old concept of the “five races:” African, Asian, European, Native American, and Oceanian. (B) Actual genetic variation in humans.\nPolygenic Traits, Human Embryos, and Eugenic Dreams\nAn academic study debunked the idea of “Screening Human Embryos for Polygenic Traits,” but the CEO of the company Stephen Hsu cofounded announced that they had screened human embryos for polygenic traits.\n\nThe amoral nonsense of Orchid’s embryo selection\n\n“Superior: The Return of Race Science”\n\n\n\nhttps://en.wikipedia.org/wiki/Superior:_The_Return_of_Race_Science\n\n\n“Weapons of Math Destruction”\n\n\n\nhttps://en.wikipedia.org/wiki/Weapons_of_Math_Destruction"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#multiple-testing",
    "href": "content/bootcamp/stats/class-05.slides.html#multiple-testing",
    "title": "Bootcamp: Stats class 5",
    "section": "Multiple testing",
    "text": "Multiple testing\n\nHow does multiple testing correction work?\n\nMultiple Testing — How Should You Adjust?\n\nMultiple Comparisons\n\nAn Overview of Methods to Address the Multiple Comparison Problem"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#bayesian-statistics-1",
    "href": "content/bootcamp/stats/class-05.slides.html#bayesian-statistics-1",
    "title": "Bootcamp: Stats class 5",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nHigh-speed intro to Bayes’s rule\nAn Introduction to Bayesian Thinking\nBayes’ Theorem, Clearly Explained!!!!"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.slides.html#modern-statistics-beer-and-eugenics-1",
    "href": "content/bootcamp/stats/class-05.slides.html#modern-statistics-beer-and-eugenics-1",
    "title": "Bootcamp: Stats class 5",
    "section": "Modern Statistics, Beer, and Eugenics",
    "text": "Modern Statistics, Beer, and Eugenics\n\nWhy We Might Not Have Statistics Without Guinness Brewery\nStatistics, Eugenics, and Me\nIs Statistics Racist?\nBeer Vs. Eugenics: The Good And The Bad Uses Of Statistics\nEngineering American society: the lesson of eugenics\nEugenics – journey to the dark side at the dawn of statistics\nHow Eugenics Shaped Statistics\nFrancis Galton’s Statistical Ideas: The Influence of Eugenics\nR. A. Fisher: a faith fit for eugenics\nSordid genealogies: a conjectural history of Cambridge Analytica’s eugenic roots\nU.S. Scientists’ Role in the Eugenics Movement (1907–1939): A Contemporary Biologist’s Perspective\nBiomedical centre memorial to victims of Nazi research\nBerlin Wild—and the Max Delbrück Center for Molecular Medicine\nEugenics timeline\nKarl Pearson praised Hitler and Nazi Race Hygiene\nRonald Fisher Is Not Being ‘Cancelled’, But His Eugenic Advocacy Should Have Consequences"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#genomics---lots-of-data---lots-of-hypothesis-tests",
    "href": "content/bootcamp/stats/class-05.html#genomics---lots-of-data---lots-of-hypothesis-tests",
    "title": "Bootcamp: Stats class 5",
    "section": "Genomics -> Lots of Data -> Lots of Hypothesis Tests",
    "text": "Genomics -&gt; Lots of Data -&gt; Lots of Hypothesis Tests\nIn a typical RNA-seq experiment, we test ~10K different hypotheses. For example, you have 10K genes and for each gene you test whether the mean expression changed in condition A vs condition B. Using a standard p-value cut-off of 0.05, we’d expect 500 genes to be deemed “significant” by chance. Thus, we concerned about False Positives or Type I Errors.\n\nSo, we want to control our type 1 error. We can approach this in two different ways.\n\nControl overall α (also known as family-wise error rate or FWER), which will affect the α* for each test. That is, we are controlling the overall probability of making at least one false discovery. Bonferroni and Sidak corrections all control FWER.\nControl false discovery rate (FDR). Where FWER controls for the probability for making a type 1 error at all, these procedures allow for type 1 errors (false positives) but control the proportion of these false positives in relation to true positives. This is done by adjusting the decision made for the p-value associated with each individual test to decide rejection or not. Because this will result in a higher type 1 error rate, it has higher power. This affords a higher probability of true discoveries. The step procedures control for FDR.\n\n\nBonferroni Correction\n**The most conservative of corrections, the Bonferroni correction is also perhaps the most straightforward in its approach. Simply divide α by the number of tests (m).\nHowever, with many tests, α* will become very small. This reduces power, which means that we are very unlikely to make any true discoveries.\n\n\nSidak Correction\n**α* = 1-(1-α)^(1/m)\n\n\nHolm’s Step-Down Procedure\n**An update of the Bonferroni correction, this procedure is more powerful. Rather than controlling the FWER, Holm’s procedure controls for the false discovery rate (FDR) and is performed after conducting all hypothesis tests and finding associated p-values at α within a set.\nThe step-down procedure is best illustrated with an example. Say we have three hypotheses, each with the associated p-value:\nH1: 0.025\nH2: 0.003\nH3: 0.01\nStep 1: Order p-values from smallest to greatest\nH2: 0.003\nH3: 0.01\nH1: 0.025\nStep 2: Use the Holm-Bonferroni formula for the first-ranked (smallest) p-value\nα* = α/(n-rank+1)\nα* = 0.05/(3–1+1) = 0.0167\nStep 3: Compare the first-ranked p-value with the α* calculated from Step 2\n0.003 &lt; 0.0167\nBecause the p-value for H2 is less than the calculated α*, we can reject H2.\nMove onto the next ranked p-value and repeat steps 2–3, calculating the α* for its respective rank and comparing it to that p-value. Continue until you reach the first non-rejected hypothesis. You would then fail to reject all following hypotheses.\n\n\nHochberg’s Step-Up Procedure\n**More powerful than Holm’s step-down procedure, Hochberg’s step-up procedure also seeks to control the FDR and follows a similar process, only p-values are ranked from largest to smallest.\nFor each ranked p-value, it is compared to the α* calculated for its respective rank (same formula as Holm’s procedure). Testing continues until you reach the first non-rejected hypothesis. You would then fail to reject all following hypotheses.\n\nrna &lt;- read_csv(file = \"data/deltaRNA_test.csv\", show_col_types = FALSE) %&gt;%\n  select(gene_id, pvalue) %&gt;%\n  na.omit()\n\nrna$fdr &lt;- p.adjust(p = rna$pvalue, method = \"fdr\", n = nrow(rna))\n\nrna$BH &lt;- p.adjust(p = rna$pvalue, method = \"BH\", n = nrow(rna))\n\nrna$bon &lt;- p.adjust(p = rna$pvalue, method = \"bonferroni\", n = nrow(rna))\n\n\nrna_long &lt;- rna %&gt;% pivot_longer(cols = pvalue:bon, names_to = \"type\")\n\n\n\np_none &lt;- ggplot(data = rna, aes(x = pvalue, y = pvalue)) +\n  geom_point(size = .1) +\n  ggtitle(\"None\") +\n  theme_minimal()\n\n\np_FDR &lt;- ggplot(data = rna, aes(x = pvalue, y = fdr)) +\n  geom_point(size = .1) +\n  ggtitle(\"FDR\") +\n  theme_minimal()\n\np_BH &lt;- ggplot(data = rna, aes(x = pvalue, y = BH)) +\n  geom_point(size = .1) +\n  ggtitle(\"BH\") +\n  theme_minimal()\n\np_bon &lt;- ggplot(data = rna, aes(x = pvalue, y = bon)) +\n  geom_point(size = .1) +\n  ggtitle(\"Bonferroni\") +\n  theme_minimal()\n\nplot_grid(p_none, p_bon, p_BH, p_FDR, ncol = 2, nrow = 2)\n\n\n\nggplot(data = rna_long, aes(x = value, color = type)) +\n  stat_ecdf() +\n  theme_minimal() +\n  xlab(\"p-values\") +\n  ylab(\"cumulative fraction\")"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#bayes-rule",
    "href": "content/bootcamp/stats/class-05.html#bayes-rule",
    "title": "Bootcamp: Stats class 5",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nThe conditional probability of the event \\(A\\) conditional on the event \\(B\\) is given by \\[\n  P(A \\mid B) = \\frac{P(A \\,\\&\\, B)}{P(B)}.\n\\]\nThis section introduces how the Bayes’ rule is applied to calculating conditional probability, and several real-life examples are demonstrated. Finally, we compare the Bayesian and frequentist definition of probability.\n\nBayes’ Rule and Diagnostic Testing\nTo better understand conditional probabilities and their importance, let us consider an example involving the human immunodeficiency virus (HIV). In the early 1980s, HIV had just been discovered and was rapidly expanding. There was major concern with the safety of the blood supply. Also, virtually no cure existed making an HIV diagnosis basically a death sentence, in addition to the stigma that was attached to the disease.\nThese made false positives and false negatives in HIV testing highly undesirable. A false positive is when a test returns postive while the truth is negative. That would for instance be that someone without HIV is wrongly diagnosed with HIV, wrongly telling that person they are going to die and casting the stigma on them. A false negative is when a test returns negative while the truth is positive. That is when someone with HIV undergoes an HIV test which wrongly comes back negative. The latter poses a threat to the blood supply if that person is about to donate blood.\nThe probability of a false positive if the truth is negative is called the false positive rate. Similarly, the false negative rate is the probability of a false negative if the truth is positive. Note that both these rates are conditional probabilities: The false positive rate of an HIV test is the probability of a positive result conditional on the person tested having no HIV.\nThe HIV test we consider is an enzyme-linked immunosorbent assay, commonly known as an ELISA. We would like to know the probability that someone (in the early 1980s) has HIV if ELISA tests positive. For this, we need the following information. ELISA’s true positive rate (one minus the false negative rate), also referred to as sensitivity, recall, or probability of detection, is estimated as\n\\[\n  P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 93\\% = 0.93.\n\\]\nIts true negative rate (one minus the false positive rate), also referred to as specificity, is estimated as\n\\[\n  P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV}) = 99\\% = 0.99.\n\\]\nAlso relevant to our question is the prevalence of HIV in the overall population, which is estimated to be 1.48 out of every 1000 American adults. We therefore assume\n\\[\\begin{equation}\n  P(\\text{Person tested has HIV}) = \\frac{1.48}{1000} = 0.00148.\n  (\\#eq:HIVpositive)\n\\end{equation}\\]\nNote that the above numbers are estimates. For our purposes, however, we will treat them as if they were exact.\nOur goal is to compute the probability of HIV if ELISA is positive, that is \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})\\). In none of the above numbers did we condition on the outcome of ELISA. Fortunately, Bayes’ rule allows is to use the above numbers to compute the probability we seek. Bayes’ rule states that\n\\[\\begin{equation}\n  P(\\text{Person tested has HIV}  \\mid \\text{ELISA is positive}) = \\frac{P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive})}{P(\\text{ELISA is positive})}.\n   (\\#eq:HIVconditional)\n\\end{equation}\\]\nThis can be derived as follows. For someone to test positive and be HIV positive, that person first needs to be HIV positive and then secondly test positive. The probability of the first thing happening is \\(P(\\text{HIV positive}) = 0.00148\\). The probability of then testing positive is \\(P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 0.93\\), the true positive rate. This yields for the numerator\n\\[\\begin{multline}\n  P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) \\\\\n  &= 0.00148 \\cdot 0.93\n  = 0.0013764.\n  \\end{split}\n  (\\#eq:HIVjoint)\n\\end{multline}\\]\nThe first step in the above equation is implied by Bayes’ rule: By multiplying the left- and right-hand side of Bayes’ rule by \\(P(B)\\), we obtain \\[\n  P(A \\mid B) P(B) = P(A \\,\\&\\, B).\n\\]\nThe denominator in @ref(eq:HIVconditional) can be expanded as\n\\[\\begin{multline*}\n  P(\\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) + P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  &= 0.0013764 + 0.0099852 = 0.0113616\n  \\end{split}\n\\end{multline*}\\]\nwhere we used @ref(eq:HIVjoint) and\n\\[\\begin{multline*}\n  P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has no HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has no HIV}) \\\\\n  &= \\left(1 - P(\\text{Person tested has HIV})\\right) \\cdot \\left(1 - P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV})\\right) \\\\\n  &= \\left(1 - 0.00148\\right) \\cdot \\left(1 - 0.99\\right) = 0.0099852.\n  \\end{split}\n\\end{multline*}\\]\nPutting this all together and inserting into @ref(eq:HIVconditional) reveals \\[\\begin{equation}\n  P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) = \\frac{0.0013764}{0.0113616} \\approx 0.12.\n  (\\#eq:HIVresult)\n\\end{equation}\\] So even when the ELISA returns positive, the probability of having HIV is only 12%. An important reason why this number is so low is due to the prevalence of HIV. Before testing, one’s probability of HIV was 0.148%, so the positive test changes that probability dramatically, but it is still below 50%. That is, it is more likely that one is HIV negative rather than positive after one positive ELISA test.\nQuestions like the one we just answered (What is the probability of a disease if a test returns positive?) are crucial to make medical diagnoses. As we saw, just the true positive and true negative rates of a test do not tell the full story, but also a disease’s prevalence plays a role. Bayes’ rule is a tool to synthesize such numbers into a more useful probability of having a disease after a test result.\n\nWhat is the probability that someone who tests positive does not actually have HIV?\n\nWe found in @ref(eq:HIVresult) that someone who tests positive has a \\(0.12\\) probability of having HIV. That implies that the same person has a \\(1-0.12=0.88\\) probability of not having HIV, despite testing positive.\n\nIf the individual is at a higher risk for having HIV than a randomly sampled person from the population considered, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n\nIf the person has a priori a higher risk for HIV and tests positive, then the probability of having HIV must be higher than for someone not at increased risk who also tests positive. Therefore, \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &gt; 0.12\\) where \\(0.12\\) comes from @ref(eq:HIVresult).\nOne can derive this mathematically by plugging in a larger number in @ref(eq:HIVpositive) than 0.00148, as that number represents the prior risk of HIV. Changing the calculations accordingly shows \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &gt; 0.12\\).\n\nIf the false positive rate of the test is higher than 1%, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n\nIf the false positive rate increases, the probability of a wrong positive result increases. That means that a positive test result is more likely to be wrong and thus less indicative of HIV. Therefore, the probability of HIV after a positive ELISA goes down such that \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &lt; 0.12\\).\n\n\nBayes Updating\nIn the previous section, we saw that one positive ELISA test yields a probability of having HIV of 12%. To obtain a more convincing probability, one might want to do a second ELISA test after a first one comes up positive. What is the probability of being HIV positive if also the second ELISA test comes back positive?\nTo solve this problem, we will assume that the correctness of this second test is not influenced by the first ELISA, that is, the tests are independent from each other. This assumption probably does not hold true as it is plausible that if the first test was a false positive, it is more likely that the second one will be one as well. Nonetheless, we stick with the independence assumption for simplicity.\nIn the last section, we used \\(P(\\text{Person tested has HIV}) = 0.00148\\), see @ref(eq:HIVpositive), to compute the probability of HIV after one positive test. If we repeat those steps but now with \\(P(\\text{Person tested has HIV}) = 0.12\\), the probability that a person with one positive test has HIV, we exactly obtain the probability of HIV after two positive tests. Repeating the maths from the previous section, involving Bayes’ rule, gives\n\\[\\begin{multline}\n  P(\\text{Person tested has HIV} \\mid \\text{Second ELISA is also positive}) \\\\\n  \\begin{split}\n  &= \\frac{P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Person tested has HIV})}{P(\\text{Second ELISA is also positive})} \\\\\n  &= \\frac{0.12 \\cdot 0.93}{\n  \\begin{split}\n  &P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has HIV}) \\\\\n  &+ P(\\text{Person tested has no HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has no HIV})\n  \\end{split}\n  } \\\\\n  &= \\frac{0.1116}{0.12 \\cdot 0.93 + (1 - 0.12)\\cdot (1 - 0.99)} \\approx 0.93.\n  \\end{split}\n  (\\#eq:Bayes-updating)\n\\end{multline}\\]\nSince we are considering the same ELISA test, we used the same true positive and true negative rates as in Section @ref(sec:diagnostic-testing). We see that two positive tests makes it much more probable for someone to have HIV than when only one test comes up positive.\nThis process, of using Bayes’ rule to update a probability based on an event affecting it, is called Bayes’ updating. More generally, the what one tries to update can be considered ‘prior’ information, sometimes simply called the prior. The event providing information about this can also be data. Then, updating this prior using Bayes’ rule gives the information conditional on the data, also known as the posterior, as in the information after having seen the data. Going from the prior to the posterior is Bayes updating.\nThe probability of HIV after one positive ELISA, 0.12, was the posterior in the previous section as it was an update of the overall prevalence of HIV, @ref(eq:HIVpositive). However, in this section we answered a question where we used this posterior information as the prior. This process of using a posterior as prior in a new problem is natural in the Bayesian framework of updating knowledge based on the data."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#fathers-of-statistics",
    "href": "content/bootcamp/stats/class-05.html#fathers-of-statistics",
    "title": "Bootcamp: Stats class 5",
    "section": "Fathers of statistics",
    "text": "Fathers of statistics\nThe torch was passed within the triumvirate of Galton, Pearson, and Fisher.\n\nSir Francis Galton (1822-1911)\n\n\n\nfrom galton.org\n\n\n\nDiscovered regression to the mean\nRe-discovered correlation and regression and discovered how to apply these in anthropology, psychology, and more\nDefined the concept of standard deviation\nEstablished the field of Eugenics in 1883\nDarwin’s cousin.\n\nGalton’s reasoning for coining the term eugenics:\n\n“We greatly want a brief word to express the science of improving stock, which…takes cognisance of all influences that tend in however remote a degree to give the more suitable races or strains of blood a better chance of prevailing speedily over the less suitable than they otherwise would have had.”\n\n\n\nKarl Pearson (1857-1936)\n\n\n\nhttps://www.britannica.com/biography/Karl-Pearson\n\n\nKarl Pearson was Galton’s protégé and directly or contributed to:\n\nDeveloped hypothesis testing\nDeveloped the use of p-values\nDefined the Chi-Squared test\nCorrelation coefficient\nPrinciple components analysis\n\nAlso authors of timeless “classics” such as:\nThe Woman’s Question\nNational Life from the standpoint of science\nIn the year Mein Kampf was published, Pearson wrote an article called:\nTHE PROBLEM OF ALIEN IMMIGRATION INTO GREAT BRITAIN, ILLUSTRATED BY AN EXAMINATION OF RUSSIAN AND POLISH JEWISH CHILDREN\nHere is an excerpt:\n\n“[they] will develop into a parasitic race…Taken on the average, and regarding both sexes, this alien Jewish population is somewhat inferior physically and mentally to the native population.”\n\n\n\nSir Ronald Aylmer Fisher (1890-1962)\n\n\n\nhttps://www.42evolution.org/ronald-a-fisher/\n\n\nFisher’s work in statistics established and promoted many important methods of statistical inference. His contributions include:\n\nEstablishing p = 0.05 as the normal threshold for significant p-values\nPromoting Maximum Likelihood Estimation\nDeveloping the ANalysis Of VAriance (ANOVA) The iris dataset (this seems an incredibly minor contribution but I use it daily)\nThe Genetical Theory of Natural Selection, which blended the work of Mendel and Darwin.\n\nThere is no lack of Fisher’s strong and consistent support for eugenics. Here is an example from as late as 1954.\n\n\n\nLetter from R.A. Fisher to R. Ruggles Gates. Ronald Fisher Archive. University of Adelaide."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#storytime-galton-laboratory",
    "href": "content/bootcamp/stats/class-05.html#storytime-galton-laboratory",
    "title": "Bootcamp: Stats class 5",
    "section": "Storytime: Galton Laboratory",
    "text": "Storytime: Galton Laboratory\nGalton founded the Eugenics Record Office (1904)\nGalton Eugenics Laboratory as part of University College London (UCL). Created by Pearson and funded by Galton. (1907)\nGalton left UCL enough money to create a Chair in National Eugenics, filled by Pearson and then Fisher. Hell of a name for an endowed chair!\nAnnals of Human Genetics: It was established in 1925 Pearson as the Annals of Eugenics, and obtained its current name in 1954.\nGalton laboratory was incorporated into the Department of Eugenics, Biometry and Genetic at UCL in 1944.\nRenamed to the Department of Human Genetics and Biometry in 1966.\nBecame part of the Department of Biology at UCL in 1996.\nIn 2020: UCL renames three facilities that honoured prominent eugenicists\nThese horrendous views did not appear to be common at UCL in the 1930s. For example, they were not held by JBS Haldane, Egon Pearson (son of Karl), and Lionel Penrose.\n\nWhat about in the US?\nCSHL - Eugenics Archive\nU.S. Scientists’ Role in the Eugenics Movement (1907–1939): A Contemporary Biologist’s Perspective\nCharles Davenport (first director of CSHL) and the Carnegie Insitution\nCold Spring Harbor and German Eugenics in the 1930s\nEugenics and the history of Science and AAAS"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#section",
    "href": "content/bootcamp/stats/class-05.html#section",
    "title": "Bootcamp: Stats class 5",
    "section": "",
    "text": "from “America’s Shameful History of Eugenics and Forced Sterilizations”"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#modern-day-eugenics-and-beyond",
    "href": "content/bootcamp/stats/class-05.html#modern-day-eugenics-and-beyond",
    "title": "Bootcamp: Stats class 5",
    "section": "Modern day: Eugenics and beyond",
    "text": "Modern day: Eugenics and beyond\n\n[Sordid genealogies: a conjectural history of Cambridge Analytica’s\neugenic roots](https://www.nature.com/articles/s41599-020-0505-5)\n\n\nAmerican Renaissance\n\n‘Race’ cannot be biologically defined due to genetic variation among human individuals and populations. (A) The old concept of the “five races:” African, Asian, European, Native American, and Oceanian. (B) Actual genetic variation in humans.\n\n\nPolygenic Traits, Human Embryos, and Eugenic Dreams\nAn academic study debunked the idea of “Screening Human Embryos for Polygenic Traits,” but the CEO of the company Stephen Hsu cofounded announced that they had screened human embryos for polygenic traits.\n\nThe amoral nonsense of Orchid’s embryo selection\n\n\n\n“Superior: The Return of Race Science”\n\n\n\nhttps://en.wikipedia.org/wiki/Superior:_The_Return_of_Race_Science\n\n\n\n\n“Weapons of Math Destruction”\n\n\n\nhttps://en.wikipedia.org/wiki/Weapons_of_Math_Destruction"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#multiple-testing",
    "href": "content/bootcamp/stats/class-05.html#multiple-testing",
    "title": "Bootcamp: Stats class 5",
    "section": "Multiple testing",
    "text": "Multiple testing\n\nHow does multiple testing correction work?\n\nMultiple Testing — How Should You Adjust?\n\nMultiple Comparisons\n\nAn Overview of Methods to Address the Multiple Comparison Problem"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#bayesian-statistics-1",
    "href": "content/bootcamp/stats/class-05.html#bayesian-statistics-1",
    "title": "Bootcamp: Stats class 5",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nHigh-speed intro to Bayes’s rule\nAn Introduction to Bayesian Thinking\nBayes’ Theorem, Clearly Explained!!!!"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#modern-statistics-beer-and-eugenics-1",
    "href": "content/bootcamp/stats/class-05.html#modern-statistics-beer-and-eugenics-1",
    "title": "Bootcamp: Stats class 5",
    "section": "Modern Statistics, Beer, and Eugenics",
    "text": "Modern Statistics, Beer, and Eugenics\n\nWhy We Might Not Have Statistics Without Guinness Brewery\nStatistics, Eugenics, and Me\nIs Statistics Racist?\nBeer Vs. Eugenics: The Good And The Bad Uses Of Statistics\nEngineering American society: the lesson of eugenics\nEugenics – journey to the dark side at the dawn of statistics\nHow Eugenics Shaped Statistics\nFrancis Galton’s Statistical Ideas: The Influence of Eugenics\nR. A. Fisher: a faith fit for eugenics\nSordid genealogies: a conjectural history of Cambridge Analytica’s eugenic roots\nU.S. Scientists’ Role in the Eugenics Movement (1907–1939): A Contemporary Biologist’s Perspective\nBiomedical centre memorial to victims of Nazi research\nBerlin Wild—and the Max Delbrück Center for Molecular Medicine\nEugenics timeline\nKarl Pearson praised Hitler and Nazi Race Hygiene\nRonald Fisher Is Not Being ‘Cancelled’, But His Eugenic Advocacy Should Have Consequences"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#provide-a-simple-and-flexible-framework",
    "href": "content/bootcamp/stats/class-03.slides.html#provide-a-simple-and-flexible-framework",
    "title": "Bootcamp: Stats class 3",
    "section": "Provide a simple and flexible framework",
    "text": "Provide a simple and flexible framework"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#variables-definitions",
    "href": "content/bootcamp/stats/class-03.slides.html#variables-definitions",
    "title": "Bootcamp: Stats class 3",
    "section": "Variables definitions",
    "text": "Variables definitions\nRandom variables (x, y)\nResponse Variable ( y - aka dependent or outcome variable): this variable is predicted or its variation is explained by the explanatory variable. In an experiment, this is the outcome that is measured following manipulation of the explanatory variable.\nExplanatory Variable ( x - aka independent or predictor variable): explains variations in the response variable. In an experiment, it is manipulated by the researcher.\nQuantitative Variables\nDiscrete variable: numeric variables that have a countable number of values between any two values - integer in R (e.g., number of mice, read counts).\nContinuous variable: numeric variables that have an infinite number of values between any two values - numeric in R (e.g., normalized expression values, fluorescent intensity).\nCategorical Variables\nNominal variable: (unordered) random variables have categories where order doesn’t matter - factor in R (e.g., country, type of gene, genotype).\nOrdinal variable: (ordered) random variables have ordered categories - order of levels in R ( e.g. grade of tumor)."
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#hypothesis-testing-definitions",
    "href": "content/bootcamp/stats/class-03.slides.html#hypothesis-testing-definitions",
    "title": "Bootcamp: Stats class 3",
    "section": "Hypothesis testing definitions",
    "text": "Hypothesis testing definitions\nHypothesis testing is a statistical analysis that uses sample data to assess two mutually exclusive theories about the properties of a population. Statisticians call these theories the null hypothesis and the alternative hypothesis. A hypothesis test assesses your sample statistic and factors in an estimate of the sample error to determine which hypothesis the data support.\nWhen you can reject the null hypothesis, the results are statistically significant, and your data support the theory that an effect exists at the population level.\nA legal analogy: Guilty or not guilty?\nThe statistical concept of ‘significant’ vs. ‘not significant’ can be understood by comparing to the legal concept of ‘guilty’ vs. ‘not guilty’.\nIn the American legal system (and much of the world) a criminal defendant is presumed innocent until proven guilty. If the evidence proves the defendant guilty beyond a reasonable doubt, the verdict is ‘guilty’. Otherwise the verdict is ‘not guilty’. In some countries, this verdict is ‘not proven’, which is a better description. A ‘not guilty’ verdict does not mean the judge or jury concluded that the defendant is innocent -- it just means that the evidence was not strong enough to persuade the judge or jury that the defendant was guilty.\nIn statistical hypothesis testing, you start with the null hypothesis (usually that there is no difference between groups). If the evidence produces a small enough P value, you reject that null hypothesis, and conclude that the difference is real. If the P value is higher than your threshold (usually 0.05), you don’t reject the null hypothesis. This doesn’t mean the evidence convinced you that the treatment had no effect, only that the evidence was not persuasive enough to convince you that there is an effect.\nEffect — the difference between the population value and the null hypothesis value. The effect is also known as population effect or the difference. Typically, you do not know the size of the actual effect. However, you can use a hypothesis test to help you determine whether an effect exists and to estimate its size.\nNull Hypothesis or \\(\\mathcal{H}_0\\) — one of two mutually exclusive theories about the properties of the population in hypothesis testing. Typically, the null hypothesis states that there is no effect (i.e., the effect size equals zero).\nAlternative Hypothesis or \\(\\mathcal{H}_1\\) — the other theory about the properties of the population in hypothesis testing. Typically, the alternative hypothesis states that a population parameter does not equal the null hypothesis value. In other words, there is a non-zero effect. If your sample contains sufficient evidence, you can reject the null and favor the alternative hypothesis.\nP-values — the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. Lower p-values represent stronger evidence against the null. P-values in conjunction with the significance level determines whether your data favor the null or alternative hypothesis.\nStatQuest: P Values, clearly explained\nStatQuest: How to calculate p-values\nSignificance Level or \\(a\\) — an evidentiary standard set before the study. It is the probability that you say there is an effect when there is no effect (the probability of rejecting the null hypothesis given that it is true). Lower significance levels indicate that you require stronger evidence before you will reject the null.It is usually set at or below .05.\n\nGuinness"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#null-hypothesis-testing",
    "href": "content/bootcamp/stats/class-03.slides.html#null-hypothesis-testing",
    "title": "Bootcamp: Stats class 3",
    "section": "Null hypothesis testing",
    "text": "Null hypothesis testing\n\nSpecify the variables\nDeclare null hypothesis \\(\\mathcal{H}_0\\)\nCalculate test-statistic, exact p-value\nGenerate and visualize data reflecting null-distribution\nCalculate the p-value from the test statistic and null distribution\n\n*4-5: For calculating empirical p-value"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#the-simplicity-underlying-common-tests",
    "href": "content/bootcamp/stats/class-03.slides.html#the-simplicity-underlying-common-tests",
    "title": "Bootcamp: Stats class 3",
    "section": "The simplicity underlying common tests",
    "text": "The simplicity underlying common tests\nMost of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. In particular, it all comes down to:\n\\(y = a \\cdot x + b\\)\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model."
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#parametric-vs-non-parametric-tests",
    "href": "content/bootcamp/stats/class-03.slides.html#parametric-vs-non-parametric-tests",
    "title": "Bootcamp: Stats class 3",
    "section": "Parametric vs Non-Parametric tests",
    "text": "Parametric vs Non-Parametric tests\nParametric tests are suitable for normally distributed data.\nNon-Parametric tests are suitable for any continuous data. For the sake of simplicity and sticking with a consistent framework, we will consider Non-Parametric tests as the ranked versions of the corresponding parametric tests.\nMore on choosing Parametric vs Non-Parametric\n\n\n\n\n\nInfo\nParametric\nNon-Parametric\n\n\n\n\nbetter descriptor\nmean\nmedian\n\n\n# of samples (N)\nmany\nfew"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#correlation-vs-regression",
    "href": "content/bootcamp/stats/class-03.slides.html#correlation-vs-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Correlation vs Regression",
    "text": "Correlation vs Regression\nCorrelation is primarily used to quickly and concisely summarize the direction and strength of the relationships between a set of 2 or more numeric variables. \nRegression is primarily used to build models/equations to predict a key response, Y, from a set of predictor (X) variables.\n\n\n\n\n\n\n\n\n\nCorrelation\nRegression\n\n\n\n\nDescription\nAssociation between 2 or more variables\nHow an independent variable is numerically related to the dependent variable\n\n\nUsage\nTo represent linear relationship between two variables\nTo fit a best line and estimate one variable on the basis of another variable\n\n\nDependent vs Independent variables\nDoesn’t matter\nmust define (i.e. order of relationship matters)\n\n\nInterpretation\nCorrelation coefficient indicates the extent to which two variables move together\nRegression indicates the impact of a unit change in the known variable (x) on the estimated variable (y)\n\n\nGoal\nTo find a numerical value expressing the relationship between variables\nTo estimate values of random variable on the basis of the values of fixed variable\n\n\n\n*Borrowed from and more info available here and here.\nPearson Correlation\nIt was developed by Karl Pearson from a related idea introduced by Francis Galton in the 1880s, and for which the mathematical formula was derived and published by Auguste Bravais in 1844.[a][6][7][8][9] The naming of the coefficient is thus an example of Stigler’s Law (see list of examples here).\nInterpretation of coefficient:\n1 = perfect linear correlation\n0 = no correlation\n-1 = perfect linear anti-correlation\n\\(Corr(x,y) = \\displaystyle \\frac {\\sum_{i=1}^{n} (x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i=1}^{n} \\sqrt(x_{i} - \\overline{x})^2 \\sqrt(y_{i} - \\overline{y})^2}\\)\n\\(x_{i}\\) = the “i-th” observation of the variable \\(x\\)\n\\(\\overline{x}\\) = mean of all observations of \\(x\\)\n\\(y_{i}\\) = the “i-th” observation of the variable \\(y\\)\n\\(\\overline{y}\\) = mean of all observations of \\(y\\)\n\nr_pearson &lt;- cor(x = biochem$tot_cholesterol, y = biochem$weight)\n\nr_pearson\n\n[1] 0.3540731\n\n# average total cholesterol\navg_chol &lt;- mean(biochem$tot_cholesterol)\n\n# average weight\navg_weight &lt;- mean(biochem$weight)\n\n# difference from mean total cholesterol\ndiff_chol &lt;- biochem$tot_cholesterol - avg_chol\n\n# difference from mean total cholesterol\ndiff_weight &lt;- biochem$weight - avg_weight\n\n# follow formula above\nmanual_pearson &lt;- sum(diff_chol*diff_weight)/(\nsqrt(sum(diff_chol^2))*sqrt(sum(diff_weight^2)))\n\nmanual_pearson\n\n[1] 0.3540731\n\nidentical(manual_pearson, r_pearson)\n\n[1] TRUE\n\n# cov(x = biochem$tot_cholesterol, y = biochem$weight)/(sd(biochem$tot_cholesterol)*sd(biochem$weight))"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#regression",
    "href": "content/bootcamp/stats/class-03.slides.html#regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Regression",
    "text": "Regression"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#equation-for-a-line",
    "href": "content/bootcamp/stats/class-03.slides.html#equation-for-a-line",
    "title": "Bootcamp: Stats class 3",
    "section": "Equation for a line",
    "text": "Equation for a line\nRemember:\n\\(y = a \\cdot x + b\\)\nOR\n\\(y = b + a \\cdot x\\)\n\\(a\\) is the SLOPE\n\\(b\\) is the y-intercept\n\n\\(a\\) = 2 (the slope)\n\\(b\\) = 0.5 (the y-intercept)"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#stats-equation-for-a-line",
    "href": "content/bootcamp/stats/class-03.slides.html#stats-equation-for-a-line",
    "title": "Bootcamp: Stats class 3",
    "section": "Stats equation for a line",
    "text": "Stats equation for a line\nModel: the recipe for \\(y\\) is a slope (\\(\\beta_1\\)) times \\(x\\) plus an intercept (\\(\\beta_0\\)).\n\\(y = \\beta_0 + \\beta_1 x \\qquad \\qquad \\mathcal{H}_0: \\beta_1 = 0\\)\n… which is the same has \\(y = a \\cdot x + b\\) (here ordered as \\(y = b + a \\cdot x\\)). In R we are lazy and write y ~ 1 + x which R reads like y = 1*number + x*othernumber and the task of t-tests, lm, etc., is simply to find the numbers that best predict \\(y\\).\nEither way you write it, it’s an intercept (\\(\\beta_0\\)) and a slope (\\(\\beta_1\\)) yielding a straight line:\n\n\\(\\beta_0\\) = 0.5 (the y-intercept)\n\\(\\beta_1\\) = 2 (the slope)\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(y = .5 \\cdot 1 + 2 \\cdot x\\)\nOur mission: FIND THE BEST \\(\\beta\\) coefficients"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#linear-regression",
    "href": "content/bootcamp/stats/class-03.slides.html#linear-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nSTEP 1: Make a scatter plot visualize the linear relationship between x and y.\nSTEP 2: Perform the regression\nSTEP 3: Look at the \\(R^2\\), \\(F\\)-value and \\(p\\)-value\nSTEP 4: Visualize fit and errors\nSTEP 5: Calculate \\(R^2\\), \\(F\\)-value and \\(p\\)-value ourselves"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#multiple-regression",
    "href": "content/bootcamp/stats/class-03.slides.html#multiple-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Multiple regression",
    "text": "Multiple regression\nRemember:\n\\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nLet’s add an explanatory variable:\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z\\)\nlinear model equation: \\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium\\)\n\\(\\mathcal{H}_0:\\) Mouse \\(cholesterol\\) and \\(sodium\\) does NOT explain \\(weight\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1, \\beta_2 = 0\\)\n\\(weight = \\beta_0 \\cdot 1 + 0 \\cdot cholesterol + 0 \\cdot sodium\\)\n\\(weight = \\beta_0 \\cdot 1\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1,\\beta_2 \\neq 0\\)\n\nFull model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z\\)\n\\(\\mathcal{H}_1:\\) Mouse \\(cholesterol\\) and \\(sodium\\) does explain \\(weight\\)\n\\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium\\)\n\n# how do different variables correlate with weight?\nbiochem %&gt;%\n  select_if(is.numeric) %&gt;%\n  cor() %&gt;%\n  as.data.frame %&gt;%\n  select(weight) %&gt;%\n  arrange(-weight) %&gt;% rownames()\n\n[1] \"weight\"          \"tot_cholesterol\" \"age\"             \"sodium\"         \n[5] \"glucose\"         \"calcium\"         \"litter\"          \"cage_density\"   \n\n# does sodium + cholesterol predict weight better than cholesterol alone?\nfit_WvC_S &lt;-  lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + sodium)  \n\n\nfit_WvC_S %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.171         0.170  3.04      183. 3.72e-73     2 -4508. 9024. 9046.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nbiochem_WvC_S &lt;- augment(fit_WvC_S, data = biochem)\n\nss.fit_C_S &lt;- sum(biochem_WvC_S$.resid^2)\n\nvar.fit_C_S &lt;- ss.fit_C_S/nrow(biochem_WvC_S)\n \n\nss.fit_C &lt;- sum(biochem_WvC$.resid^2)\n\nvar.fit_C &lt;- ss.fit_C/nrow(biochem_WvC)\n\n\n\nfit_Wvall &lt;-  lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + age + sodium + glucose + calcium + litter + cage_density)  \n\nfit_Wvall %&gt;% glance() \n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.253         0.250  2.89      85.7 1.33e-107     7 -4415. 8849. 8898.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nfit_Wvall %&gt;% tidy() \n\n# A tibble: 8 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      -6.53      1.68       -3.90 9.98e- 5\n2 tot_cholesterol   1.97      0.109      18.2  9.08e-68\n3 age               0.133     0.0161      8.28 2.49e-16\n4 sodium            0.179     0.0145     12.4  1.12e-33\n5 glucose           0.159     0.0284      5.59 2.59e- 8\n6 calcium          -5.65      0.692      -8.16 6.13e-16\n7 litter           -0.0557    0.0523     -1.06 2.88e- 1\n8 cage_density     -0.316     0.0648     -4.87 1.22e- 6\n\nggplot(data = biochem, aes(y = weight, x = calcium)) +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_minimal()"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.slides.html#non-linear-regression",
    "href": "content/bootcamp/stats/class-03.slides.html#non-linear-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Non-linear regression",
    "text": "Non-linear regression\nNon-linear regression — observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.\nDANGER OVERFITTING\nPerform loess regression\nLoess regression — a non-parametric technique that uses local weighted regression to fit a smooth curve through points in a scatter plot. LOESS combines much of the simplicity of linear least squares regression with the flexibility of nonlinear regression. It does this by fitting simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point. In fact, one of the chief attractions of this method is that the data analyst is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data.\n\n# local-weighted regression fit\nloessfit_WvC &lt;- loess(formula = weight ~  tot_cholesterol, data = biochem)\n\nloess_biochem_WvC &lt;- augment(loessfit_WvC, biochem) \n\n\nsummary(loessfit_WvC)\n\nCall:\nloess(formula = weight ~ tot_cholesterol, data = biochem)\n\nNumber of Observations: 1782 \nEquivalent Number of Parameters: 5.24 \nResidual Standard Error: 3.112 \nTrace of smoother matrix: 5.73  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate     cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n\nsummary(fit_WvC)\n\n\nCall:\nlm(formula = weight ~ 1 + tot_cholesterol, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9624 -2.1349 -0.2627  2.0113 10.2927 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      14.5560     0.3635   40.04   &lt;2e-16 ***\ntot_cholesterol   1.8516     0.1159   15.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 1780 degrees of freedom\nMultiple R-squared:  0.1254,    Adjusted R-squared:  0.1249 \nF-statistic: 255.1 on 1 and 1780 DF,  p-value: &lt; 2.2e-16\n\n\nPlot loess fit depicting residuals\n\nggplot(data = loess_biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_smooth(method=lm, col = \"black\", se = F, size=.25, linetype=\"dashed\") + # linear fit black dashed line\n  geom_smooth(method=loess, col = \"red\", se = F, size=.25) + # loess fit red line\n  geom_point(size=.5, aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") +\n  # guides(color = FALSE) +\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) +\n  theme_minimal()"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#provide-a-simple-and-flexible-framework",
    "href": "content/bootcamp/stats/class-03.html#provide-a-simple-and-flexible-framework",
    "title": "Bootcamp: Stats class 3",
    "section": "Provide a simple and flexible framework",
    "text": "Provide a simple and flexible framework"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#variables-definitions",
    "href": "content/bootcamp/stats/class-03.html#variables-definitions",
    "title": "Bootcamp: Stats class 3",
    "section": "Variables definitions",
    "text": "Variables definitions\n\nRandom variables (x, y)\nResponse Variable ( y - aka dependent or outcome variable): this variable is predicted or its variation is explained by the explanatory variable. In an experiment, this is the outcome that is measured following manipulation of the explanatory variable.\nExplanatory Variable ( x - aka independent or predictor variable): explains variations in the response variable. In an experiment, it is manipulated by the researcher.\n\n\nQuantitative Variables\nDiscrete variable: numeric variables that have a countable number of values between any two values - integer in R (e.g., number of mice, read counts).\nContinuous variable: numeric variables that have an infinite number of values between any two values - numeric in R (e.g., normalized expression values, fluorescent intensity).\n\n\nCategorical Variables\nNominal variable: (unordered) random variables have categories where order doesn’t matter - factor in R (e.g., country, type of gene, genotype).\nOrdinal variable: (ordered) random variables have ordered categories - order of levels in R ( e.g. grade of tumor)."
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#hypothesis-testing-definitions",
    "href": "content/bootcamp/stats/class-03.html#hypothesis-testing-definitions",
    "title": "Bootcamp: Stats class 3",
    "section": "Hypothesis testing definitions",
    "text": "Hypothesis testing definitions\nHypothesis testing is a statistical analysis that uses sample data to assess two mutually exclusive theories about the properties of a population. Statisticians call these theories the null hypothesis and the alternative hypothesis. A hypothesis test assesses your sample statistic and factors in an estimate of the sample error to determine which hypothesis the data support.\nWhen you can reject the null hypothesis, the results are statistically significant, and your data support the theory that an effect exists at the population level.\nA legal analogy: Guilty or not guilty?\nThe statistical concept of ‘significant’ vs. ‘not significant’ can be understood by comparing to the legal concept of ‘guilty’ vs. ‘not guilty’.\nIn the American legal system (and much of the world) a criminal defendant is presumed innocent until proven guilty. If the evidence proves the defendant guilty beyond a reasonable doubt, the verdict is ‘guilty’. Otherwise the verdict is ‘not guilty’. In some countries, this verdict is ‘not proven’, which is a better description. A ‘not guilty’ verdict does not mean the judge or jury concluded that the defendant is innocent -- it just means that the evidence was not strong enough to persuade the judge or jury that the defendant was guilty.\nIn statistical hypothesis testing, you start with the null hypothesis (usually that there is no difference between groups). If the evidence produces a small enough P value, you reject that null hypothesis, and conclude that the difference is real. If the P value is higher than your threshold (usually 0.05), you don’t reject the null hypothesis. This doesn’t mean the evidence convinced you that the treatment had no effect, only that the evidence was not persuasive enough to convince you that there is an effect.\nEffect — the difference between the population value and the null hypothesis value. The effect is also known as population effect or the difference. Typically, you do not know the size of the actual effect. However, you can use a hypothesis test to help you determine whether an effect exists and to estimate its size.\nNull Hypothesis or \\(\\mathcal{H}_0\\) — one of two mutually exclusive theories about the properties of the population in hypothesis testing. Typically, the null hypothesis states that there is no effect (i.e., the effect size equals zero).\nAlternative Hypothesis or \\(\\mathcal{H}_1\\) — the other theory about the properties of the population in hypothesis testing. Typically, the alternative hypothesis states that a population parameter does not equal the null hypothesis value. In other words, there is a non-zero effect. If your sample contains sufficient evidence, you can reject the null and favor the alternative hypothesis.\nP-values — the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. Lower p-values represent stronger evidence against the null. P-values in conjunction with the significance level determines whether your data favor the null or alternative hypothesis.\nStatQuest: P Values, clearly explained\nStatQuest: How to calculate p-values\nSignificance Level or \\(a\\) — an evidentiary standard set before the study. It is the probability that you say there is an effect when there is no effect (the probability of rejecting the null hypothesis given that it is true). Lower significance levels indicate that you require stronger evidence before you will reject the null.It is usually set at or below .05.\n\n\n\nGuinness"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#null-hypothesis-testing",
    "href": "content/bootcamp/stats/class-03.html#null-hypothesis-testing",
    "title": "Bootcamp: Stats class 3",
    "section": "Null hypothesis testing",
    "text": "Null hypothesis testing\n\nSpecify the variables\nDeclare null hypothesis \\(\\mathcal{H}_0\\)\nCalculate test-statistic, exact p-value\nGenerate and visualize data reflecting null-distribution\nCalculate the p-value from the test statistic and null distribution\n\n*4-5: For calculating empirical p-value"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#the-simplicity-underlying-common-tests",
    "href": "content/bootcamp/stats/class-03.html#the-simplicity-underlying-common-tests",
    "title": "Bootcamp: Stats class 3",
    "section": "The simplicity underlying common tests",
    "text": "The simplicity underlying common tests\nMost of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. In particular, it all comes down to:\n\\(y = a \\cdot x + b\\)\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model."
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#parametric-vs-non-parametric-tests",
    "href": "content/bootcamp/stats/class-03.html#parametric-vs-non-parametric-tests",
    "title": "Bootcamp: Stats class 3",
    "section": "Parametric vs Non-Parametric tests",
    "text": "Parametric vs Non-Parametric tests\nParametric tests are suitable for normally distributed data.\nNon-Parametric tests are suitable for any continuous data. For the sake of simplicity and sticking with a consistent framework, we will consider Non-Parametric tests as the ranked versions of the corresponding parametric tests.\nMore on choosing Parametric vs Non-Parametric\n\n\n\n\n\nInfo\nParametric\nNon-Parametric\n\n\n\n\nbetter descriptor\nmean\nmedian\n\n\n# of samples (N)\nmany\nfew"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#correlation-vs-regression",
    "href": "content/bootcamp/stats/class-03.html#correlation-vs-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Correlation vs Regression",
    "text": "Correlation vs Regression\nCorrelation is primarily used to quickly and concisely summarize the direction and strength of the relationships between a set of 2 or more numeric variables. \nRegression is primarily used to build models/equations to predict a key response, Y, from a set of predictor (X) variables.\n\n\n\n\n\n\n\n\n\nCorrelation\nRegression\n\n\n\n\nDescription\nAssociation between 2 or more variables\nHow an independent variable is numerically related to the dependent variable\n\n\nUsage\nTo represent linear relationship between two variables\nTo fit a best line and estimate one variable on the basis of another variable\n\n\nDependent vs Independent variables\nDoesn’t matter\nmust define (i.e. order of relationship matters)\n\n\nInterpretation\nCorrelation coefficient indicates the extent to which two variables move together\nRegression indicates the impact of a unit change in the known variable (x) on the estimated variable (y)\n\n\nGoal\nTo find a numerical value expressing the relationship between variables\nTo estimate values of random variable on the basis of the values of fixed variable\n\n\n\n*Borrowed from and more info available here and here.\n\nPearson Correlation\nIt was developed by Karl Pearson from a related idea introduced by Francis Galton in the 1880s, and for which the mathematical formula was derived and published by Auguste Bravais in 1844.[a][6][7][8][9] The naming of the coefficient is thus an example of Stigler’s Law (see list of examples here).\nInterpretation of coefficient:\n1 = perfect linear correlation\n0 = no correlation\n-1 = perfect linear anti-correlation\n\\(Corr(x,y) = \\displaystyle \\frac {\\sum_{i=1}^{n} (x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i=1}^{n} \\sqrt(x_{i} - \\overline{x})^2 \\sqrt(y_{i} - \\overline{y})^2}\\)\n\\(x_{i}\\) = the “i-th” observation of the variable \\(x\\)\n\\(\\overline{x}\\) = mean of all observations of \\(x\\)\n\\(y_{i}\\) = the “i-th” observation of the variable \\(y\\)\n\\(\\overline{y}\\) = mean of all observations of \\(y\\)\n\nr_pearson &lt;- cor(x = biochem$tot_cholesterol, y = biochem$weight)\n\nr_pearson\n\n[1] 0.3540731\n\n# average total cholesterol\navg_chol &lt;- mean(biochem$tot_cholesterol)\n\n# average weight\navg_weight &lt;- mean(biochem$weight)\n\n# difference from mean total cholesterol\ndiff_chol &lt;- biochem$tot_cholesterol - avg_chol\n\n# difference from mean total cholesterol\ndiff_weight &lt;- biochem$weight - avg_weight\n\n# follow formula above\nmanual_pearson &lt;- sum(diff_chol*diff_weight)/(\nsqrt(sum(diff_chol^2))*sqrt(sum(diff_weight^2)))\n\nmanual_pearson\n\n[1] 0.3540731\n\nidentical(manual_pearson, r_pearson)\n\n[1] TRUE\n\n# cov(x = biochem$tot_cholesterol, y = biochem$weight)/(sd(biochem$tot_cholesterol)*sd(biochem$weight))\n\n\n\n\nSpearman Correlation (nonparametric)\nSpearman’s rank correlation coefficientor Spearman’s ρ, named after Charles Spearman is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a monotonic function.\nMore info here.\n\nx &lt;- seq(1,30, 1)\ny &lt;- 2^x\n\nplot(x,y, type = \"l\", las=2)\n\n\n\ncor(x,y, method = \"pearson\")\n\n[1] 0.5199042\n\ncor(x,y, method = \"spearman\")\n\n[1] 1\n\ncor(x = biochem$tot_cholesterol, y = biochem$weight, method = \"spearman\")\n\n[1] 0.3596281"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#regression",
    "href": "content/bootcamp/stats/class-03.html#regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Regression",
    "text": "Regression"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#equation-for-a-line",
    "href": "content/bootcamp/stats/class-03.html#equation-for-a-line",
    "title": "Bootcamp: Stats class 3",
    "section": "Equation for a line",
    "text": "Equation for a line\nRemember:\n\\(y = a \\cdot x + b\\)\nOR\n\\(y = b + a \\cdot x\\)\n\\(a\\) is the SLOPE\n\\(b\\) is the y-intercept\n\n\n\n\n\n\\(a\\) = 2 (the slope)\n\\(b\\) = 0.5 (the y-intercept)"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#stats-equation-for-a-line",
    "href": "content/bootcamp/stats/class-03.html#stats-equation-for-a-line",
    "title": "Bootcamp: Stats class 3",
    "section": "Stats equation for a line",
    "text": "Stats equation for a line\nModel: the recipe for \\(y\\) is a slope (\\(\\beta_1\\)) times \\(x\\) plus an intercept (\\(\\beta_0\\)).\n\\(y = \\beta_0 + \\beta_1 x \\qquad \\qquad \\mathcal{H}_0: \\beta_1 = 0\\)\n… which is the same has \\(y = a \\cdot x + b\\) (here ordered as \\(y = b + a \\cdot x\\)). In R we are lazy and write y ~ 1 + x which R reads like y = 1*number + x*othernumber and the task of t-tests, lm, etc., is simply to find the numbers that best predict \\(y\\).\nEither way you write it, it’s an intercept (\\(\\beta_0\\)) and a slope (\\(\\beta_1\\)) yielding a straight line:\n\n\n\n\n\n\\(\\beta_0\\) = 0.5 (the y-intercept)\n\\(\\beta_1\\) = 2 (the slope)\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(y = .5 \\cdot 1 + 2 \\cdot x\\)\nOur mission: FIND THE BEST \\(\\beta\\) coefficients"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#linear-regression",
    "href": "content/bootcamp/stats/class-03.html#linear-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nSTEP 1: Make a scatter plot visualize the linear relationship between x and y.\nSTEP 2: Perform the regression\nSTEP 3: Look at the \\(R^2\\), \\(F\\)-value and \\(p\\)-value\nSTEP 4: Visualize fit and errors\nSTEP 5: Calculate \\(R^2\\), \\(F\\)-value and \\(p\\)-value ourselves\n\n\n\nSTEP 1: Can mouse cholesterol levels help explain mouse weight?\nPlot weight (y, response variable) and cholesterol levels (x, explanatory variable) of the mice.\n\nggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_minimal()\n\n\n\n\n\n\n\nSTEP 2: Do the regression\nLet’s fit a line (linear model).\nRemember: \\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nlinear model equation: \\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol\\)\n\\(\\mathcal{H}_0:\\) Mouse \\(cholesterol\\) does NOT explain \\(weight\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(weight = \\beta_0 \\cdot 1 + 0 \\cdot cholesterol\\)\n\\(weight = \\beta_0 \\cdot 1\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\nFull model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(\\mathcal{H}_1:\\) Mouse \\(cholesterol\\) does explain \\(weight\\)\n\\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol\\)\nThe cool thing here is that we can assess and compare our null and alternative hypothesis by learning and examining the model coefficients (intercept and slope). Essentially, we are comparing a complex model (including cholesterol) to a simple model (weight).\nhttps://statisticsbyjim.com/regression/interpret-constant-y-intercept-regression/\n\n\nSTEP 4: Look at the \\(R^2\\), \\(F\\)-value and \\(p\\)-value\n\n# fitting a line\nfit_WvC &lt;- lm(data = biochem,\n              formula = weight ~ 1 + tot_cholesterol)\n\n\n# base R summary of fit\nsummary(fit_WvC)\n\n\nCall:\nlm(formula = weight ~ 1 + tot_cholesterol, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9624 -2.1349 -0.2627  2.0113 10.2927 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      14.5560     0.3635   40.04   &lt;2e-16 ***\ntot_cholesterol   1.8516     0.1159   15.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 1780 degrees of freedom\nMultiple R-squared:  0.1254,    Adjusted R-squared:  0.1249 \nF-statistic: 255.1 on 1 and 1780 DF,  p-value: &lt; 2.2e-16\n\n\nThat’s a lot of info, but how would I access it? Time to meet your new best friend — Broom\n\n# information about the model fit\nglance(fit_WvC) \n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.125         0.125  3.12      255. 8.92e-54     1 -4556. 9117. 9134.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# information about the intercept and coefficients\ntidy(fit_WvC)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        14.6      0.363      40.0 1.48e-250\n2 tot_cholesterol     1.85     0.116      16.0 8.92e- 54\n\nchol_intercept &lt;- tidy(fit_WvC)[1,2]\n\nchol_slope &lt;- tidy(fit_WvC)[2,2]\n# for every 1 unit increase in cholesterol there is a 1.85 unit increase weight \n\n\n# add residuals and other information\naugment(fit_WvC) \n\n# A tibble: 1,782 × 8\n   weight tot_cholesterol .fitted .resid     .hat .sigma     .cooksd .std.resid\n    &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1   20.3            3.01    20.1  0.171 0.000566   3.12 0.000000846     0.0547\n 2   16.1            2.46    19.1 -3.01  0.00107    3.12 0.000501       -0.965 \n 3   19.5            3.57    21.2 -1.67  0.000906   3.12 0.000129       -0.534 \n 4   22.2            2.61    19.4  2.81  0.000853   3.12 0.000347        0.901 \n 5   17.3            2.04    18.3 -1.03  0.00203    3.12 0.000111       -0.331 \n 6   18.1            2.86    19.9 -1.75  0.000622   3.12 0.0000981      -0.561 \n 7   25.6            3.22    20.5  5.08  0.000592   3.12 0.000786        1.63  \n 8   18.6            3.47    21.0 -2.38  0.000782   3.12 0.000228       -0.763 \n 9   23.1            3.35    20.8  2.34  0.000669   3.12 0.000189        0.750 \n10   17.3            2.29    18.8 -1.50  0.00140    3.12 0.000161       -0.480 \n# ℹ 1,772 more rows\n\n# add residuals and other information into the biochem object\nbiochem_WvC &lt;- augment(fit_WvC, data = biochem)\n\nFor every 1 unit increase in \\(cholesterol\\) there is a 1.8516281 increase in \\(weight\\).\n\n\n\nSTEP 5: Visualize fit and errors\n\nggplot(data = biochem_WvC, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size=.5, col=\"white\") +\n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"pink\", size=1) +\n  geom_smooth(method=lm, col = \"black\", se = F, size=1, linetype=\"dashed\") +\n  scale_color_manual() +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_point(size=.5, aes(color = .resid)) + \n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"red\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\n\n\n\nSTEP 6: Calculate \\(R^2\\), \\(F\\)-value and \\(p\\)-value ourselves\n\nDEFINITIONS\n\\(SS_{mean}\\) — sum of squared errors around the mean of \\(y\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (data - mean)^2\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (y_{i} - \\overline{y})^2\\)\n\\(Var_{mean}\\) — think of it like the average of the sum of squared error around the mean of \\(y\\)\n\\(Var_{mean} = \\displaystyle \\frac {\\sum_{i=1}^{n} (y_{i} - \\overline{y})^2}{n}\\)\n\\(SS_{fit}\\) — sum of squared errors around the least-squares fit\n\\(SS_{fit} = \\sum_{i=1}^{n} (data - line)^2\\)\n\\(SS_{fit} = \\sum_{i=1}^{n} (y_{i} - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot x)^2\\)\nResiduals, \\(e\\) — the difference between the observed value of the dependent variable \\(y\\) and the predicted value \\(\\widehat{y}\\) is called the residual. Each data point has one residual.\n\\(e = y_{i} - \\widehat{y}\\)\n\\(Var_{fit}\\) — think of it like the average of the sum of squared errors around the least-squares fit\n\\(Var_{fit} = \\displaystyle \\frac {\\sum_{i=1}^{n} (y_{i} - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot x))}{n}\\)\n\nfit_W &lt;- lm(formula = weight ~ 1, data = biochem)\n\nsummary(fit_W)\n\n\nCall:\nlm(formula = weight ~ 1, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3409 -2.4409 -0.4409  2.2591  9.9591 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20.24091    0.07903   256.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.336 on 1781 degrees of freedom\n\nbiochem_W &lt;- augment(fit_W, data = biochem)\n\n\n\nSuper nice way of visualizing\nFirst, let’s look at \\(SS_{mean}\\) for avg weight:\n\np_W &lt;- ggplot(data = biochem_W, aes(x = tot_cholesterol, y = weight)) +\n  geom_hline(yintercept = biochem_W$.fitted, col = \"red\", size=.5) + # plot linear model fit\n  geom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) + # plot line representing residuals\n  theme_minimal()\n\nNow, let’s look at \\(SS_{fit}\\) for the \\(weight\\) ~ \\(cholesterol\\) :\n\np_WvC &lt;- ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"red\") +\n  geom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  # guides(color = FALSE) + # no legend for color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\nplot_grid(p_W, p_WvC, ncol = 2, labels = c(\"weight by intercept\",\"weight by cholesterol\"))\n\n\n\n\n\n\n\nFor which mice does the model fit perform the most poorly?\nGotta check residuals!\n\n# make new variable exception = absolute value of resid &gt; 9 then subject id\n\n\n\nbiochem_WvC$exceptions &lt;- if_else(\n  condition = abs(biochem_WvC$.resid) &lt; 9,\n  true = \"\",\n  false = biochem_WvC$subject_name\n  )\n\nggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight, label = exceptions)) +\n  geom_point(color = ifelse(biochem_WvC$exceptions == \"\", \"grey50\", \"red\")) +\n  geom_text_repel() +\ngeom_point(size=.5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  # guides(color = FALSE) + # no legend for color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\n\n\n\n\n\nBig Baby and KryptoNate\nNate Robinson aka “KryptoNate”\n5’ 9”\n180 lbs\n\nGlen Davis aka “Big Baby”\n6’ 9”\n280 lbs\n\n\n\\(R^2\\) or coefficient of determination — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\\(R^2 = \\displaystyle \\frac {SS_{mean} - SS_{fit}}{SS_{mean}}\\)\n\\(R^2 = \\displaystyle \\frac {SS_{w} - SS_{wc}}{SS_{w}}\\)\n\nss.fit &lt;- sum(biochem_WvC$.resid^2)\n\n\nss.mean &lt;- sum(biochem_W$.resid^2)\n\n\n# Calc R^2 value\nbiochem_WvC_rsq &lt;- (ss.mean - ss.fit) / ss.mean \nbiochem_WvC_rsq\n\n[1] 0.1253678\n\nglance(fit_WvC) %&gt;% pull(r.squared)\n\n[1] 0.1253678\n\n\n\n\nInterpretation of \\(R^2\\)\nThere is a 13% reduction in the variance when we take mouse \\(cholesterol\\) into account\nOR\nMouse \\(cholesterol\\) explains 13% in player \\(weight\\)\nBy the way, this is the same \\(R\\) as from the Pearson correlation:\n\n# Pearson correlation R value\ncor(biochem$weight, biochem$tot_cholesterol, method = \"pearson\")\n\n[1] 0.3540731\n\n# Pearson correlation R^2 value\ncor(biochem$weight, biochem$tot_cholesterol, method = \"pearson\")^2\n\n[1] 0.1253678\n\n\n\n\n\nThe F-statistic\nF-statistic — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\\(F = \\displaystyle \\frac{SS_{fit}/(p_{fit}-p_{mean})} {SS_{mean}/(n-p_{fit})}\\)\n\\(p_{fit}\\) — number of parameters in the fit line\n\\(p_{mean}\\) — number of parameters in the mean line\n\\(n\\) — number of data points\n\n# F-value\nbiochem_WvC_F &lt;- ((ss.mean - ss.fit) / (2 - 1)) / \n  (ss.fit / (nrow(biochem_WvC) - 2))\n\nbiochem_WvC_F \n\n[1] 255.1411\n\nglance(fit_WvC) %&gt;% pull(statistic)\n\n   value \n255.1411 \n\n\n\n\nP-value from the F-statistic\nWe need to generate a null distribution of \\(F-statistic\\) values to compare to our observed \\(F-statistic\\).\nTherefore, we will randomize the player Height and Weight and then calculate the \\(F-statistic\\).\n\n\nWe will do this many many times to generate a null distribution of \\(F-statistic\\)s.\n\n \n  \nThe p-value will be the probability of obtaining an \\(F-statistic\\) in the null distribution at least as extreme as our observed \\(F-statistic\\).\nAnother beautiful Statquest explaining how to go from F-statistic to p-value\n\n# set up an empty tibble to hold our null distribution\nfake_biochem &lt;- tribble()\n\n\n# sample function to randomize/permute data\nsample(x = 1:10)\n\n [1]  4  1  5  2  8  3  7  6 10  9\n\n# we will perform 100 permutations\nmyPerms &lt;- 100\n\nfor (i in 1:myPerms) {\n\n  tmp &lt;- bind_cols(\n    biochem_WvC[sample(nrow(biochem_WvC)), \"weight\"],\n    biochem_WvC[sample(nrow(biochem_WvC)), \"tot_cholesterol\"],\n    \"perm\"=factor(rep(i,nrow(biochem_WvC)))\n    )\n\n  fake_biochem &lt;- bind_rows(fake_biochem,tmp)\n  rm(tmp)\n\n}\n\n\n# let's look at permutations 1 and 2\nggplot(fake_biochem %&gt;% filter(perm %in% c(1:2)), aes(x=weight, y=tot_cholesterol, color=perm)) +\n  geom_point(size=.1) +\n  theme_minimal()\n\n\n\n\n\n\nRemember your best friend\nBROOM\n\n# here we will calculate and extract linear model results for each permutation individualy using nest, mutate, and map functions\n\nfake_biochem_lms &lt;- fake_biochem %&gt;%\n  nest(data = -perm) %&gt;%\n  mutate(\n    fit = map(data, ~ lm(weight ~ tot_cholesterol, data = .x)),\n    glanced = map(fit, glance)\n  ) %&gt;%\n  unnest(glanced)\n\n\n\nLet’s take a look at the null distribution of F-statistics from the randomized values\n\nfake_biochem_lms %&gt;%\n  ggplot(., aes(x = statistic)) +\n  geom_density(color=\"red\") +\n  theme_minimal()\n\n\n\n\nremember that the \\(F-statistic\\) we observed was ~255!\n\nfake_biochem_lms %&gt;%\n  ggplot(., aes(x = statistic)) +\n  xlim(0,biochem_WvC_F*1.1) +\n  geom_density(color=\"red\") +\n  geom_vline(xintercept = biochem_WvC_F, color = \"blue\") +\n#  scale_x_log10() +\n  theme_minimal() \n\n\n\nglance(fit_WvC) %&gt;% pull(p.value)\n\n       value \n8.924155e-54 \n\n\n\n\nInterpretation of \\(p-value\\)\nThere is no value more extreme than our observed \\(F-statistic\\).\nTherefore, the empirical \\(p-value &lt; 0.001\\) — empirical what we calculated by randomizing our data.\nThe exact \\(p-value\\) is 8.9241551^{-54} AKA “a ridiculously small number”.\nCorrect — Assuming that \\(cholesterol\\) has zero effect on \\(weight\\) in the population, you’d obtain the sample effect, or larger, in 8.9241551^{-54} AKA “a ridiculously small number” of studies because of random sample error.\nIncorrect —There’s a 8.9241551^{-54} AKA “a ridiculously small number” chance of making a mistake by rejecting the null hypothesis.\nMore on p-value (mis)interpretation\n\n\n\n\nHow to find the best (least squares) fit?\n\nRotate the line of fit\n\nFind the fit that minimizes the Sum of Squared Residuals or \\(SS_{fit}\\)\n\nThis is the derivative (slope of tangent at best point = 0) of the function describing the \\(SS_{fit}\\) and the next rotation is 0.\n\nStatQuest: Fitting a line to data, aka least squares, aka linear regression.\nStatQuest: Gradient Descent, Step-by-Step\n\n\n\nNon-parametric version\n\nTheory: rank-transformation\nrank simply takes a list of numbers and “replace” them with the integers of their rank (1st smallest, 2nd smallest, 3rd smallest, etc.). So the result of the rank-transformation rank(c(3.6, 3.4, -5.0, 8.2)) is 3, 2, 1, 4. See that in the figure above?\nA signed rank is the same, just where we rank according to absolute size first and then add in the sign second. So the signed rank here would be 2, 1, -3, 4. Or in code:\n\nnpfit_WvC &lt;- lm(formula = rank(weight) ~ 1 + rank(tot_cholesterol), data = biochem)\n\ntidy(npfit_WvC)\n\n# A tibble: 2 × 5\n  term                  estimate std.error statistic   p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)            571.      22.8         25.1 4.01e-119\n2 rank(tot_cholesterol)    0.360    0.0221      16.3 1.54e- 55\n\nglance(npfit_WvC) %&gt;% pull(r.squared)\n\n[1] 0.1293324\n\n# cor(biochem$weight, biochem$tot_cholesterol, method = \"spearman\")^2"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#multiple-regression",
    "href": "content/bootcamp/stats/class-03.html#multiple-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Multiple regression",
    "text": "Multiple regression\nRemember:\n\\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nLet’s add an explanatory variable:\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z\\)\nlinear model equation: \\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium\\)\n\\(\\mathcal{H}_0:\\) Mouse \\(cholesterol\\) and \\(sodium\\) does NOT explain \\(weight\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1, \\beta_2 = 0\\)\n\\(weight = \\beta_0 \\cdot 1 + 0 \\cdot cholesterol + 0 \\cdot sodium\\)\n\\(weight = \\beta_0 \\cdot 1\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1,\\beta_2 \\neq 0\\)\n\nFull model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z\\)\n\\(\\mathcal{H}_1:\\) Mouse \\(cholesterol\\) and \\(sodium\\) does explain \\(weight\\)\n\\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium\\)\n\n# how do different variables correlate with weight?\nbiochem %&gt;%\n  select_if(is.numeric) %&gt;%\n  cor() %&gt;%\n  as.data.frame %&gt;%\n  select(weight) %&gt;%\n  arrange(-weight) %&gt;% rownames()\n\n[1] \"weight\"          \"tot_cholesterol\" \"age\"             \"sodium\"         \n[5] \"glucose\"         \"calcium\"         \"litter\"          \"cage_density\"   \n\n# does sodium + cholesterol predict weight better than cholesterol alone?\nfit_WvC_S &lt;-  lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + sodium)  \n\n\nfit_WvC_S %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.171         0.170  3.04      183. 3.72e-73     2 -4508. 9024. 9046.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nbiochem_WvC_S &lt;- augment(fit_WvC_S, data = biochem)\n\nss.fit_C_S &lt;- sum(biochem_WvC_S$.resid^2)\n\nvar.fit_C_S &lt;- ss.fit_C_S/nrow(biochem_WvC_S)\n \n\nss.fit_C &lt;- sum(biochem_WvC$.resid^2)\n\nvar.fit_C &lt;- ss.fit_C/nrow(biochem_WvC)\n\n\n\nfit_Wvall &lt;-  lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + age + sodium + glucose + calcium + litter + cage_density)  \n\nfit_Wvall %&gt;% glance() \n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.253         0.250  2.89      85.7 1.33e-107     7 -4415. 8849. 8898.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nfit_Wvall %&gt;% tidy() \n\n# A tibble: 8 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      -6.53      1.68       -3.90 9.98e- 5\n2 tot_cholesterol   1.97      0.109      18.2  9.08e-68\n3 age               0.133     0.0161      8.28 2.49e-16\n4 sodium            0.179     0.0145     12.4  1.12e-33\n5 glucose           0.159     0.0284      5.59 2.59e- 8\n6 calcium          -5.65      0.692      -8.16 6.13e-16\n7 litter           -0.0557    0.0523     -1.06 2.88e- 1\n8 cage_density     -0.316     0.0648     -4.87 1.22e- 6\n\nggplot(data = biochem, aes(y = weight, x = calcium)) +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_minimal()"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#non-linear-regression",
    "href": "content/bootcamp/stats/class-03.html#non-linear-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Non-linear regression",
    "text": "Non-linear regression\nNon-linear regression — observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.\nDANGER OVERFITTING\n\nPerform loess regression\nLoess regression — a non-parametric technique that uses local weighted regression to fit a smooth curve through points in a scatter plot. LOESS combines much of the simplicity of linear least squares regression with the flexibility of nonlinear regression. It does this by fitting simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point. In fact, one of the chief attractions of this method is that the data analyst is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data.\n\n# local-weighted regression fit\nloessfit_WvC &lt;- loess(formula = weight ~  tot_cholesterol, data = biochem)\n\nloess_biochem_WvC &lt;- augment(loessfit_WvC, biochem) \n\n\nsummary(loessfit_WvC)\n\nCall:\nloess(formula = weight ~ tot_cholesterol, data = biochem)\n\nNumber of Observations: 1782 \nEquivalent Number of Parameters: 5.24 \nResidual Standard Error: 3.112 \nTrace of smoother matrix: 5.73  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate     cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n\nsummary(fit_WvC)\n\n\nCall:\nlm(formula = weight ~ 1 + tot_cholesterol, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9624 -2.1349 -0.2627  2.0113 10.2927 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      14.5560     0.3635   40.04   &lt;2e-16 ***\ntot_cholesterol   1.8516     0.1159   15.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 1780 degrees of freedom\nMultiple R-squared:  0.1254,    Adjusted R-squared:  0.1249 \nF-statistic: 255.1 on 1 and 1780 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nPlot loess fit depicting residuals\n\nggplot(data = loess_biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_smooth(method=lm, col = \"black\", se = F, size=.25, linetype=\"dashed\") + # linear fit black dashed line\n  geom_smooth(method=loess, col = \"red\", se = F, size=.25) + # loess fit red line\n  geom_point(size=.5, aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") +\n  # guides(color = FALSE) +\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.slides.html#class-objectives",
    "href": "content/bootcamp/stats/class-01.slides.html#class-objectives",
    "title": "Stats Class 01",
    "section": "Class Objectives",
    "text": "Class Objectives\n\nUnderstand probability distributions\nLearn to apply probabilistic thinking to biological data"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.slides.html#an-episode-in-the-life-of-a-data-scientist",
    "href": "content/bootcamp/stats/class-01.slides.html#an-episode-in-the-life-of-a-data-scientist",
    "title": "Stats Class 01",
    "section": "An Episode in the Life of a Data Scientist*^",
    "text": "An Episode in the Life of a Data Scientist*^\n*All characters are made up\n^All data is simulated\nMatt: “Here is some microscopy data. What insight can you find in it? (Yes it is not RNA and I want you to use R not python. No, my account has not been hacked.)”\nYou: “What is in the data file?”\nMatt: “I measured the radius of gyration (I made up this quantity for cells) and the largest length for a bunch of cells from different cell types.”\nYou get a brand new dataset. You are seeing it for the first time.\nFirst - unboxing\n\n#read dataset\ndata &lt;- read.csv(file='data/cell_dimensions.tsv',sep='\\t',stringsAsFactors = T)\nhead(data)\n\n   Cell_Rg Cell_Len   Cell_Type\n1 2.441927 4.143111 Fibroblasts\n2 2.832714 6.648487 Fibroblasts\n3 3.085169 7.822642 Fibroblasts\n4 3.243857 7.997854 Fibroblasts\n5 3.359737 7.448261 Fibroblasts\n6 3.434748 7.800078 Fibroblasts\n\n\nOk, so Matt gave you a clean file. There are 3 columns -\nCell_Rg - possibly radius of gyration?\nCell_len - possibly largest length?\nCell_Type - self explanatory"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.slides.html#how-many-data-points-are-there",
    "href": "content/bootcamp/stats/class-01.slides.html#how-many-data-points-are-there",
    "title": "Stats Class 01",
    "section": "How many data points are there?",
    "text": "How many data points are there?\n\nnrow(data)\n\n[1] 400\n\n\nHow do you get a feel for 400 datapoints? You cannot see trends or get “insights” directly looking at the raw data at this scale. So you need to “summarise the data” to get “Estimates of Location”\n\nsummary(data)\n\n    Cell_Rg          Cell_Len            Cell_Type  \n Min.   : 2.263   Min.   : 0.000   Fibroblasts:100  \n 1st Qu.: 4.503   1st Qu.: 5.407   mESC       :100  \n Median : 5.477   Median : 7.290   Muscle     :100  \n Mean   : 6.173   Mean   : 6.695   Neuron     :100  \n 3rd Qu.: 6.751   3rd Qu.: 8.900                    \n Max.   :15.928   Max.   :12.626                    \n\n\nLet us go through these summary statistics one-by-one.\nMinimum and Maximum - self explanatory\nMedian - midpoint when you sort that column. Let us verify.\nThere are 400 data points. Mid point will be between 200 and 201.\n\na=sort(data$Cell_Rg)[200]\nb=sort(data$Cell_Rg)[201]\nprint(c(a,b))\n\n[1] 5.475759 5.478244\n\n(a+b)/2\n\n[1] 5.477002\n\n\nA more general definition of median is that it is 0.5 quantile (or 50th percentile).\nSo then first quartile (“1st Qu.”) would be 0.25 quantile or 25th percentile, third quartile (“3rd Qu.”) would be 0.75 quantile or 75th percentile. For 400 observations, where will the first and third quartile would be around? We can use quantile function to find out:\n\nquantile(seq(1,400),0.25)\n\n   25% \n100.75 \n\nquantile(seq(1,400),0.75)\n\n   75% \n300.25 \n\n\nR will interpolate to get the quantile values.\n\nquantile(data$Cell_Rg,0.25)\n\n     25% \n4.502879 \n\nquantile(data$Cell_Rg,0.75)\n\n     75% \n6.750579 \n\n\nMean is the average: sum of all observations divided by the number of observations\n\nsum(data$Cell_Rg)/nrow(data)\n\n[1] 6.172667\n\n\nBut this is the mean of all observations. Let us find the mean for each cell type\n\nlibrary(tidyverse)\n\nSorted by mean Rg:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(mean_Rg = mean(Cell_Rg), mean_len = mean(Cell_Len)) %&gt;% \n         arrange(mean_Rg)\n\n# A tibble: 4 × 3\n  Cell_Type   mean_Rg mean_len\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Neuron         4.85     7.84\n2 Fibroblasts    5.05     6.97\n3 Muscle         7.08    10.1 \n4 mESC           7.72     1.84\n\n\nSorted by mean Cell_Len:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(mean_Rg = mean(Cell_Rg), mean_len = mean(Cell_Len)) %&gt;% \n         arrange(mean_len)\n\n# A tibble: 4 × 3\n  Cell_Type   mean_Rg mean_len\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 mESC           7.72     1.84\n2 Fibroblasts    5.05     6.97\n3 Neuron         4.85     7.84\n4 Muscle         7.08    10.1 \n\n\nSo mESC has the largest Rg and neuron has the smallest.\nmESC has the smallest length and neuron/muscle have really long lengths.\n(according to the mean…)\nMean could be skewed due to outliers. Median is a better measure in presence of outliers. If we repeat the above analysis with medians:\nSorted by median Rg:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(median_Rg = median(Cell_Rg), median_len = median(Cell_Len)) %&gt;% \n         arrange(median_Rg)\n\n# A tibble: 4 × 3\n  Cell_Type   median_Rg median_len\n  &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 Neuron           4.86       7.88\n2 Fibroblasts      5.04       6.91\n3 mESC             5.22       1   \n4 Muscle           7.10      10.1 \n\n\nSorted by median Cell_Len:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(median_Rg = median(Cell_Rg), median_len = median(Cell_Len)) %&gt;% \n         arrange(median_len)\n\n# A tibble: 4 × 3\n  Cell_Type   median_Rg median_len\n  &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 mESC             5.22       1   \n2 Fibroblasts      5.04       6.91\n3 Neuron           4.86       7.88\n4 Muscle           7.10      10.1 \n\n\nHmm, mESC seems to have changed the most. Having extreme values skew the mean is more representative of real data you will come across.\nThis big difference between mean and median is suspicious for mESC. So we need to see how the Rg of mESC is distributed. Histogram bins the data to give us a discretized distribution:\n\n#get mESC Rg:\nmesc_rg &lt;- data[data$Cell_Type==\"mESC\",]$Cell_Rg\n#plot histogram\nhist(mesc_rg,breaks=50,freq=FALSE,xlim=c(0,20))\n\n\n\n\nCan we “smooth” the histogram?\n\nhist(mesc_rg,breaks=50,freq=FALSE,xlim=c(0,20))\nlines(density(mesc_rg),xlim=c(0,20),lwd=3,col=\"blue\")\n\n\n\n\nSo obviously, there is an outlier population that is skewing the mean, but the median is not as affected by the outliers.\nThis distribution is also telling us that the trend in mean or median doesn’t account for the “spread” of the data. To compare how spread out the data is across the four cell types, we can use boxplots:\n\nlibrary(ggplot2)\nggplot(data) + \ngeom_boxplot(aes(x=Cell_Type,y=Cell_Rg),notch = T) +\ntheme_bw()\n\n\n\n\nSo the variability and two sided distribution is hidden by boxplots, violin plots help with that\n\nggplot(data) + \ngeom_violin(aes(x=Cell_Type,y=Cell_Rg)) +  \ngeom_boxplot(aes(x=Cell_Type,y=Cell_Rg),width=0.05) + \ntheme_bw()\n\n\n\n\nWith these exploratory analyses, we can hypothesize that muscle has higher Rg than fibroblasts and neurons. mESC has a complex distribution that needs further experiments, so we won’t analyze that cell type further."
  },
  {
    "objectID": "content/bootcamp/stats/class-01.slides.html#how-significant-are-differences-between-muscle-cells-and-fibroblasts",
    "href": "content/bootcamp/stats/class-01.slides.html#how-significant-are-differences-between-muscle-cells-and-fibroblasts",
    "title": "Stats Class 01",
    "section": "How significant are differences between muscle cells and fibroblasts?",
    "text": "How significant are differences between muscle cells and fibroblasts?\nWe want to ultimately answer the question: Does a given cell type have a different size than others?\nUsing statistical concepts, we can answer this question in many ways:\n\nWe can make an assumption of the underlying probability distribution - this gives us powerful tools to ask how different the size of two cell types are.\nWe could use resampling (class 2)\nWe could use non-parametric statistical tests (class 2)\n\nLet us explore underlying distribution of Rg of muscle cells\n\n#Get muscle Rg as a vector:\nmuscle_rg &lt;- data[data$Cell_Type==\"Muscle\",]$Cell_Rg\n#generate histogram\nmuscle_rg_hist &lt;- hist(muscle_rg,breaks=30,freq=F,xlim=c(0,12))\n\n\n\n\nA probability density function (pdf) is associated with continuous random variable.\n\nA pdf is greater than or equal to zero at all values of x\nA pdf integrates to 1\n\nA normal distribution is an example of a pdf that occurs commonly in the world around us.\nWe can use the normal distribution as an example to understand how to use pdfs.\nLet us assume that the muscle cell Rg is normally distributed with an average of 7 micron and a standard deviation of 1 micron. The ideal distribution would look like this:\n\n#Use seq to create a range of numbers\ncell_size_list &lt;- seq(0,12,0.2)\n#Use dnorm to calculate probability at each point of the vector above\ndist_cell_size_muscle &lt;- dnorm(cell_size_list,mean=7,sd=1)\nplot(cell_size_list,dist_cell_size_muscle,type='l')\n\n\n\n\nLet us say you fit a normal distribution to your muscle cell Rg data, this is how it might look:\n\nplot(muscle_rg_hist$mids,muscle_rg_hist$density,xlim=c(0,12),ylim=c(0,0.5))\nlines(cell_size_list,dist_cell_size_muscle,col='red',xlim=c(0,12),ylim=c(0,0.5))\n\n\n\n\nGoodness of fit\nHow do we see how good a fit this is? We will not go through rigorous measures of goodness of fit in this class, but we will do a quick visual check on how good a fit is using Q-Q plot.\nQ-Q plot compares the quantiles of two distributions. In our case, we have the observed distribution and the theoretical distribution.\nTo generate quantiles of our theoretical distribution, we will use qnorm:\n\ntheoretical_muscle &lt;- qnorm(ppoints(100),mean=7,sd=1)\n\nHere is how the theoretical and observed numbers are distributed. We are going to plot the “quintile” on y axis and the Rg on x axis:\n\nplot(y=ppoints(100),x=theoretical_muscle,xlim=c(4,10), type='l')\nlines(y=ppoints(100),x=sort(muscle_rg),xlim=c(4,10), type='l',lwd=2,col=\"red\")\n\n\n\n\nA Q-Q plot compares each point on black and red lines at each quintile:\n\nqqplot(muscle_rg,theoretical_muscle)\nabline(a = 0, b = 1, col = \"blue\")\n\n\n\n\nIf the points fall close to the diagonal, the two distributions are similar.\nPoints either falling below or above the line at the edges indicate that the tails of the distribution don’t agree.\nImportant note:\nThe two distributions being compared should be scaled to be similar. Here I generated the values of the normal distribution at the same position as that of the observed distribution.\nWe will next repeat this analysis for fibroblasts:\nSimilarly for fibbroblasts, a normal distribution could explain the underlying distribution of Rg.\nFirst the histogram of observed values:\n\n#Get fibroblast Rg as a vector:\nfib_rg &lt;- data[data$Cell_Type==\"Fibroblasts\",]$Cell_Rg\n#generate histogram\nfib_rg_hist &lt;- hist(fib_rg,breaks=30,freq=F,xlim=c(0,12))\n\n\n\n\nThen the theoretical distribution:\n\ndist_cell_size_fib &lt;- dnorm(cell_size_list,mean=5,sd=1)\n#plots:\nplot(fib_rg_hist$mids,fib_rg_hist$density,xlim=c(0,12),ylim=c(0,0.5))\nlines(cell_size_list,dist_cell_size_fib,col='red',xlim=c(0,12),ylim=c(0,0.5))\n\n\n\n\nAnd finally the Q-Q plot:\n\ntheoretical_fib &lt;- qnorm(ppoints(100),mean=5,sd=1)\nqqplot(fib_rg,theoretical_fib)\nabline(a = 0, b = 1, col = \"blue\")\n\n\n\n\nThis Q-Q plot looks good as well.\nNext class, we will start by asking how we can use these theoretical distributions to figure out if Rg of muscle and fibroblasts are different."
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html",
    "href": "content/bootcamp/stats/class-01.html",
    "title": "Stats Class 01",
    "section": "",
    "text": "Understand probability distributions\nLearn to apply probabilistic thinking to biological data"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#class-objectives",
    "href": "content/bootcamp/stats/class-01.html#class-objectives",
    "title": "Stats Class 01",
    "section": "",
    "text": "Understand probability distributions\nLearn to apply probabilistic thinking to biological data"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#an-episode-in-the-life-of-a-data-scientist",
    "href": "content/bootcamp/stats/class-01.html#an-episode-in-the-life-of-a-data-scientist",
    "title": "Stats Class 01",
    "section": "An Episode in the Life of a Data Scientist*^",
    "text": "An Episode in the Life of a Data Scientist*^\n*All characters are made up\n^All data is simulated\nMatt: “Here is some microscopy data. What insight can you find in it? (Yes it is not RNA and I want you to use R not python. No, my account has not been hacked.)”\nYou: “What is in the data file?”\nMatt: “I measured the radius of gyration (I made up this quantity for cells) and the largest length for a bunch of cells from different cell types.”\nYou get a brand new dataset. You are seeing it for the first time.\nFirst - unboxing\n\n#read dataset\ndata &lt;- read.csv(file='data/cell_dimensions.tsv',sep='\\t',stringsAsFactors = T)\nhead(data)\n\n   Cell_Rg Cell_Len   Cell_Type\n1 2.441927 4.143111 Fibroblasts\n2 2.832714 6.648487 Fibroblasts\n3 3.085169 7.822642 Fibroblasts\n4 3.243857 7.997854 Fibroblasts\n5 3.359737 7.448261 Fibroblasts\n6 3.434748 7.800078 Fibroblasts\n\n\nOk, so Matt gave you a clean file. There are 3 columns -\nCell_Rg - possibly radius of gyration?\nCell_len - possibly largest length?\nCell_Type - self explanatory"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#how-many-data-points-are-there",
    "href": "content/bootcamp/stats/class-01.html#how-many-data-points-are-there",
    "title": "Stats Class 01",
    "section": "How many data points are there?",
    "text": "How many data points are there?\n\nnrow(data)\n\n[1] 400\n\n\nHow do you get a feel for 400 datapoints? You cannot see trends or get “insights” directly looking at the raw data at this scale. So you need to “summarise the data” to get “Estimates of Location”\n\nsummary(data)\n\n    Cell_Rg          Cell_Len            Cell_Type  \n Min.   : 2.263   Min.   : 0.000   Fibroblasts:100  \n 1st Qu.: 4.503   1st Qu.: 5.407   mESC       :100  \n Median : 5.477   Median : 7.290   Muscle     :100  \n Mean   : 6.173   Mean   : 6.695   Neuron     :100  \n 3rd Qu.: 6.751   3rd Qu.: 8.900                    \n Max.   :15.928   Max.   :12.626                    \n\n\nLet us go through these summary statistics one-by-one.\nMinimum and Maximum - self explanatory\nMedian - midpoint when you sort that column. Let us verify.\nThere are 400 data points. Mid point will be between 200 and 201.\n\na=sort(data$Cell_Rg)[200]\nb=sort(data$Cell_Rg)[201]\nprint(c(a,b))\n\n[1] 5.475759 5.478244\n\n(a+b)/2\n\n[1] 5.477002\n\n\nA more general definition of median is that it is 0.5 quantile (or 50th percentile).\nSo then first quartile (“1st Qu.”) would be 0.25 quantile or 25th percentile, third quartile (“3rd Qu.”) would be 0.75 quantile or 75th percentile. For 400 observations, where will the first and third quartile would be around? We can use quantile function to find out:\n\nquantile(seq(1,400),0.25)\n\n   25% \n100.75 \n\nquantile(seq(1,400),0.75)\n\n   75% \n300.25 \n\n\nR will interpolate to get the quantile values.\n\nquantile(data$Cell_Rg,0.25)\n\n     25% \n4.502879 \n\nquantile(data$Cell_Rg,0.75)\n\n     75% \n6.750579 \n\n\nMean is the average: sum of all observations divided by the number of observations\n\nsum(data$Cell_Rg)/nrow(data)\n\n[1] 6.172667\n\n\nBut this is the mean of all observations. Let us find the mean for each cell type\n\nlibrary(tidyverse)\n\nSorted by mean Rg:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(mean_Rg = mean(Cell_Rg), mean_len = mean(Cell_Len)) %&gt;% \n         arrange(mean_Rg)\n\n# A tibble: 4 × 3\n  Cell_Type   mean_Rg mean_len\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Neuron         4.85     7.84\n2 Fibroblasts    5.05     6.97\n3 Muscle         7.08    10.1 \n4 mESC           7.72     1.84\n\n\nSorted by mean Cell_Len:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(mean_Rg = mean(Cell_Rg), mean_len = mean(Cell_Len)) %&gt;% \n         arrange(mean_len)\n\n# A tibble: 4 × 3\n  Cell_Type   mean_Rg mean_len\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 mESC           7.72     1.84\n2 Fibroblasts    5.05     6.97\n3 Neuron         4.85     7.84\n4 Muscle         7.08    10.1 \n\n\nSo mESC has the largest Rg and neuron has the smallest.\nmESC has the smallest length and neuron/muscle have really long lengths.\n(according to the mean…)\nMean could be skewed due to outliers. Median is a better measure in presence of outliers. If we repeat the above analysis with medians:\nSorted by median Rg:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(median_Rg = median(Cell_Rg), median_len = median(Cell_Len)) %&gt;% \n         arrange(median_Rg)\n\n# A tibble: 4 × 3\n  Cell_Type   median_Rg median_len\n  &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 Neuron           4.86       7.88\n2 Fibroblasts      5.04       6.91\n3 mESC             5.22       1   \n4 Muscle           7.10      10.1 \n\n\nSorted by median Cell_Len:\n\ndata %&gt;% group_by(Cell_Type) %&gt;% \n         summarise(median_Rg = median(Cell_Rg), median_len = median(Cell_Len)) %&gt;% \n         arrange(median_len)\n\n# A tibble: 4 × 3\n  Cell_Type   median_Rg median_len\n  &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 mESC             5.22       1   \n2 Fibroblasts      5.04       6.91\n3 Neuron           4.86       7.88\n4 Muscle           7.10      10.1 \n\n\nHmm, mESC seems to have changed the most. Having extreme values skew the mean is more representative of real data you will come across.\nThis big difference between mean and median is suspicious for mESC. So we need to see how the Rg of mESC is distributed. Histogram bins the data to give us a discretized distribution:\n\n#get mESC Rg:\nmesc_rg &lt;- data[data$Cell_Type==\"mESC\",]$Cell_Rg\n#plot histogram\nhist(mesc_rg,breaks=50,freq=FALSE,xlim=c(0,20))\n\n\n\n\nCan we “smooth” the histogram?\n\nhist(mesc_rg,breaks=50,freq=FALSE,xlim=c(0,20))\nlines(density(mesc_rg),xlim=c(0,20),lwd=3,col=\"blue\")\n\n\n\n\nSo obviously, there is an outlier population that is skewing the mean, but the median is not as affected by the outliers.\nThis distribution is also telling us that the trend in mean or median doesn’t account for the “spread” of the data. To compare how spread out the data is across the four cell types, we can use boxplots:\n\nlibrary(ggplot2)\nggplot(data) + \ngeom_boxplot(aes(x=Cell_Type,y=Cell_Rg),notch = T) +\ntheme_bw()\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\nSo the variability and two sided distribution is hidden by boxplots, violin plots help with that\n\nggplot(data) + \ngeom_violin(aes(x=Cell_Type,y=Cell_Rg)) +  \ngeom_boxplot(aes(x=Cell_Type,y=Cell_Rg),width=0.05) + \ntheme_bw()\n\n\n\n\nWith these exploratory analyses, we can hypothesize that muscle has higher Rg than fibroblasts and neurons. mESC has a complex distribution that needs further experiments, so we won’t analyze that cell type further."
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#how-significant-are-differences-between-muscle-cells-and-fibroblasts",
    "href": "content/bootcamp/stats/class-01.html#how-significant-are-differences-between-muscle-cells-and-fibroblasts",
    "title": "Stats Class 01",
    "section": "How significant are differences between muscle cells and fibroblasts?",
    "text": "How significant are differences between muscle cells and fibroblasts?\nWe want to ultimately answer the question: Does a given cell type have a different size than others?\nUsing statistical concepts, we can answer this question in many ways:\n\nWe can make an assumption of the underlying probability distribution - this gives us powerful tools to ask how different the size of two cell types are.\nWe could use resampling (class 2)\nWe could use non-parametric statistical tests (class 2)\n\nLet us explore underlying distribution of Rg of muscle cells\n\n#Get muscle Rg as a vector:\nmuscle_rg &lt;- data[data$Cell_Type==\"Muscle\",]$Cell_Rg\n#generate histogram\nmuscle_rg_hist &lt;- hist(muscle_rg,breaks=30,freq=F,xlim=c(0,12))\n\n\n\n\nA probability density function (pdf) is associated with continuous random variable.\n\nA pdf is greater than or equal to zero at all values of x\nA pdf integrates to 1\n\nA normal distribution is an example of a pdf that occurs commonly in the world around us.\nWe can use the normal distribution as an example to understand how to use pdfs.\nLet us assume that the muscle cell Rg is normally distributed with an average of 7 micron and a standard deviation of 1 micron. The ideal distribution would look like this:\n\n#Use seq to create a range of numbers\ncell_size_list &lt;- seq(0,12,0.2)\n#Use dnorm to calculate probability at each point of the vector above\ndist_cell_size_muscle &lt;- dnorm(cell_size_list,mean=7,sd=1)\nplot(cell_size_list,dist_cell_size_muscle,type='l')\n\n\n\n\nLet us say you fit a normal distribution to your muscle cell Rg data, this is how it might look:\n\nplot(muscle_rg_hist$mids,muscle_rg_hist$density,xlim=c(0,12),ylim=c(0,0.5))\nlines(cell_size_list,dist_cell_size_muscle,col='red',xlim=c(0,12),ylim=c(0,0.5))\n\n\n\n\n\nGoodness of fit\nHow do we see how good a fit this is? We will not go through rigorous measures of goodness of fit in this class, but we will do a quick visual check on how good a fit is using Q-Q plot.\nQ-Q plot compares the quantiles of two distributions. In our case, we have the observed distribution and the theoretical distribution.\nTo generate quantiles of our theoretical distribution, we will use qnorm:\n\ntheoretical_muscle &lt;- qnorm(ppoints(100),mean=7,sd=1)\n\nHere is how the theoretical and observed numbers are distributed. We are going to plot the “quintile” on y axis and the Rg on x axis:\n\nplot(y=ppoints(100),x=theoretical_muscle,xlim=c(4,10), type='l')\nlines(y=ppoints(100),x=sort(muscle_rg),xlim=c(4,10), type='l',lwd=2,col=\"red\")\n\n\n\n\nA Q-Q plot compares each point on black and red lines at each quintile:\n\nqqplot(muscle_rg,theoretical_muscle)\nabline(a = 0, b = 1, col = \"blue\")\n\n\n\n\nIf the points fall close to the diagonal, the two distributions are similar.\nPoints either falling below or above the line at the edges indicate that the tails of the distribution don’t agree.\n\n\nImportant note:\nThe two distributions being compared should be scaled to be similar. Here I generated the values of the normal distribution at the same position as that of the observed distribution.\n\n\nWe will next repeat this analysis for fibroblasts:\nSimilarly for fibbroblasts, a normal distribution could explain the underlying distribution of Rg.\nFirst the histogram of observed values:\n\n#Get fibroblast Rg as a vector:\nfib_rg &lt;- data[data$Cell_Type==\"Fibroblasts\",]$Cell_Rg\n#generate histogram\nfib_rg_hist &lt;- hist(fib_rg,breaks=30,freq=F,xlim=c(0,12))\n\n\n\n\nThen the theoretical distribution:\n\ndist_cell_size_fib &lt;- dnorm(cell_size_list,mean=5,sd=1)\n#plots:\nplot(fib_rg_hist$mids,fib_rg_hist$density,xlim=c(0,12),ylim=c(0,0.5))\nlines(cell_size_list,dist_cell_size_fib,col='red',xlim=c(0,12),ylim=c(0,0.5))\n\n\n\n\nAnd finally the Q-Q plot:\n\ntheoretical_fib &lt;- qnorm(ppoints(100),mean=5,sd=1)\nqqplot(fib_rg,theoretical_fib)\nabline(a = 0, b = 1, col = \"blue\")\n\n\n\n\nThis Q-Q plot looks good as well.\n\nNext class, we will start by asking how we can use these theoretical distributions to figure out if Rg of muscle and fibroblasts are different."
  }
]