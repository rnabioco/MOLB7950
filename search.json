[
  {
    "objectID": "content/course-info/final-projects.html",
    "href": "content/course-info/final-projects.html",
    "title": "MOLB 7950 — Final Projects",
    "section": "",
    "text": "Final projects can involve groups of 1-3 people.\nProjects are choose your own adventure:\n\nYou can start with a template. Each of these have a set of questions, that you can build off of:\n\ndna-block/ explores an MNase-seq data set in S. cerevisiae. Sub-nucleosomal fragments provide a DNA-based signal to understand chromatin transactions that lead to transcription.\nrna-block/published-data/ is the material Kent presented, examining the role of A-to-I editing using RNA-seq.\nrna-block/rna-localization/: is an exploration of neuronal RNA localization by RNA-seq.\n\nYou could find a data set on NCBI GEO of interest (e.g., relevant to your thesis work), and work it up with salmon, DEseq, and exploratory analysis.\nYou can start with your own sequencing data (bulk/single-cell RNA seq, DNA sequencing)."
  },
  {
    "objectID": "content/course-info/final-projects.html#overview",
    "href": "content/course-info/final-projects.html#overview",
    "title": "MOLB 7950 — Final Projects",
    "section": "",
    "text": "Final projects can involve groups of 1-3 people.\nProjects are choose your own adventure:\n\nYou can start with a template. Each of these have a set of questions, that you can build off of:\n\ndna-block/ explores an MNase-seq data set in S. cerevisiae. Sub-nucleosomal fragments provide a DNA-based signal to understand chromatin transactions that lead to transcription.\nrna-block/published-data/ is the material Kent presented, examining the role of A-to-I editing using RNA-seq.\nrna-block/rna-localization/: is an exploration of neuronal RNA localization by RNA-seq.\n\nYou could find a data set on NCBI GEO of interest (e.g., relevant to your thesis work), and work it up with salmon, DEseq, and exploratory analysis.\nYou can start with your own sequencing data (bulk/single-cell RNA seq, DNA sequencing)."
  },
  {
    "objectID": "content/course-info/final-projects.html#deliverables",
    "href": "content/course-info/final-projects.html#deliverables",
    "title": "MOLB 7950 — Final Projects",
    "section": "Deliverables",
    "text": "Deliverables\n\nAn Rmarkdown document with code, plots, interpretations, and next steps.\nIf you work in a group, list the members of the group at the top of the document, and make it clear which parts are your work by adding your initials to code chunks.\nShort presentations (8-10 minutes) by the groups the week of Nov 28. Presentations should include 1-2 slides of background, a hypothesis for the approach, code output (table or graph) that addresses the hypothesis, and one or more tests of the statistical significance of the observation."
  },
  {
    "objectID": "content/course-info/final-projects.html#grading-and-rubric",
    "href": "content/course-info/final-projects.html#grading-and-rubric",
    "title": "MOLB 7950 — Final Projects",
    "section": "Grading and rubric",
    "text": "Grading and rubric\nThe final project will be worth 20% of your grade and we will use the grading scheme outlined in the grading rubric.\nEach individual in a group will be evaluated separately, so contributions must be clearly marked in the document."
  },
  {
    "objectID": "content/course-info/support.html",
    "href": "content/course-info/support.html",
    "title": "MOLB 7950 — Getting help",
    "section": "",
    "text": "Course discussion will be through the Slack MOLB7950 organization.\nGuidelines for using Slack:\n\nUse dedicated channels for discussion in #class, questions about your #problem-sets, and your #final-project\nYou can ask for help by tagging the TAs in the #class channel. If you post @ta help, someone will start a thread where you can ask a question.\nIf needed, we can talk face-to-face via the /zoom integration.\n\n\n\n\n\nOur TAs will be available Tuesday & Thursday afternoons from 1-4pm."
  },
  {
    "objectID": "content/course-info/support.html#how-to-get-help",
    "href": "content/course-info/support.html#how-to-get-help",
    "title": "MOLB 7950 — Getting help",
    "section": "",
    "text": "Course discussion will be through the Slack MOLB7950 organization.\nGuidelines for using Slack:\n\nUse dedicated channels for discussion in #class, questions about your #problem-sets, and your #final-project\nYou can ask for help by tagging the TAs in the #class channel. If you post @ta help, someone will start a thread where you can ask a question.\nIf needed, we can talk face-to-face via the /zoom integration.\n\n\n\n\n\nOur TAs will be available Tuesday & Thursday afternoons from 1-4pm."
  },
  {
    "objectID": "content/course-info/team.html",
    "href": "content/course-info/team.html",
    "title": "MOLB 7950 — Teaching Team",
    "section": "",
    "text": "Instructor\nE-mail\nGitHub\nSchedule a meeting\n\n\n\n\nJay Hesselberth\n\n\n\n\n\nNeel Mukherjee\n\n\n\n\n\nSrinivas Ramachandran\n\n\n\n\n\nMatt Taliaferro\n\n\n\n\n\nKent Riemondy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructor\nE-mail\nGitHub\n\n\n\n\nEvan Morrison\n\n\n\n\nGrant Lo"
  },
  {
    "objectID": "content/course-info/team.html#teaching-team-and-office-hours",
    "href": "content/course-info/team.html#teaching-team-and-office-hours",
    "title": "MOLB 7950 — Teaching Team",
    "section": "",
    "text": "Instructor\nE-mail\nGitHub\nSchedule a meeting\n\n\n\n\nJay Hesselberth\n\n\n\n\n\nNeel Mukherjee\n\n\n\n\n\nSrinivas Ramachandran\n\n\n\n\n\nMatt Taliaferro\n\n\n\n\n\nKent Riemondy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructor\nE-mail\nGitHub\n\n\n\n\nEvan Morrison\n\n\n\n\nGrant Lo"
  },
  {
    "objectID": "content/course-info/problem-sets.html",
    "href": "content/course-info/problem-sets.html",
    "title": "MOLB 7950 — Problem Sets",
    "section": "",
    "text": "We reinforce concepts with problem sets assigned at the end of each class. During the main blocks, problem sets on Mon and Wed should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors / TAs\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\n\n\n\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run"
  },
  {
    "objectID": "content/course-info/problem-sets.html#problem-set-overview",
    "href": "content/course-info/problem-sets.html#problem-set-overview",
    "title": "MOLB 7950 — Problem Sets",
    "section": "",
    "text": "We reinforce concepts with problem sets assigned at the end of each class. During the main blocks, problem sets on Mon and Wed should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors / TAs\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\n\n\n\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run"
  },
  {
    "objectID": "content/course-info/links.html",
    "href": "content/course-info/links.html",
    "title": "MOLB 7950 — Course links",
    "section": "",
    "text": "In previous iterations of this course, we taught command-line (bash, grep, awk, etc) and Python programming. These skills are useful, but for consistency we opted to focus on R programming and RStudio as an analysis environment.\nAMC also offers shorter workshops on specific analysis strategies that you might find helpful."
  },
  {
    "objectID": "content/course-info/links.html#recommended-reading",
    "href": "content/course-info/links.html#recommended-reading",
    "title": "MOLB 7950 — Course links",
    "section": "Recommended reading",
    "text": "Recommended reading\nWe borrow from the following books and recommend referring them after the class has completed. Many of them contain exercises for learning specific approaches (e.g., RNA-seq).\n\nData processing and visualization\n\nR for Data Science\nR Packages is a thorough treatment of developing your own R packages.\nFundamentals of Data Visualization contains several nice graphical display concepts, with examples of what and what not to do in displaying data visually.\n\n\n\nStatistics\n\nPractical Statistics for Data Scientists covers several fundamental concepts with code for both R and Python.\nModern Statistics for Modern Biology is written by two leading figures in computational biology and contains several examples using Bioconductor.\nStatistics for Biologists is a collection of articles on statistical topic."
  },
  {
    "objectID": "content/course-info/links.html#acknowldgements-attribution",
    "href": "content/course-info/links.html#acknowldgements-attribution",
    "title": "MOLB 7950 — Course links",
    "section": "Acknowldgements & Attribution",
    "text": "Acknowldgements & Attribution\nWe have borrowed from several (open licensed) resources for course content, including:\n\nStats 545 at UBC, particularly their grading rubrics\nCourses from Mine Çetinkaya-Rundel, mostly inspiration for quarto formats"
  },
  {
    "objectID": "content/course-info/syllabus.html",
    "href": "content/course-info/syllabus.html",
    "title": "MOLB 7950 Syllabus",
    "section": "",
    "text": "MOLB 7950 is a hands-on tutorial of skills and theory needed to process, analyze, and visualize output from large biological data sets. We emphasize command-line tools and the R statistical computing environment.\n🗓️ Class will run from Aug 28 - Nov 1\n📍 Classes will be held in-person in AHSB 2201\n🕘 Class time is 9:00-10:30am\nMOLB 7950 is a three credit hour course.\nThe course is divided into blocks:\n\n\nTHe Bootcamp block covers R programming and introduces important statistical concepts and approaches. We will also cover data types you will encounter during biological data analysis and approaches for their analysis.\nDuring the bootcamp block, we will meet everyday for 90 minutes to cover fundamental concepts you will need throughout the course.\n\n\n\nAfter Bootcamp, will cover experimental approaches used to analyze DNA and RNA. Each block spans ~4 weeks, with each week focused on a particular type of experiment (see below). Each block covers statistical concepts needed for rigorous analysis and analysis approaches to process raw data to results (tables and figures) using reproducible coding techniques.\nIn most weeks we will discuss and analyze data from a publication. You are responsible for reading the week’s material before class begins on Monday.\n\n\n\nThe DNA block covers genome sequencing for identifying mutations, and two approaches for analyzing chromatin state (ChIP-seq and MNase-seq).\nThe RNA block covers RNA-seq, alternative splicing, differential gene expression, and RNA:protein interactions."
  },
  {
    "objectID": "content/course-info/syllabus.html#course-overview",
    "href": "content/course-info/syllabus.html#course-overview",
    "title": "MOLB 7950 Syllabus",
    "section": "",
    "text": "MOLB 7950 is a hands-on tutorial of skills and theory needed to process, analyze, and visualize output from large biological data sets. We emphasize command-line tools and the R statistical computing environment.\n🗓️ Class will run from Aug 28 - Nov 1\n📍 Classes will be held in-person in AHSB 2201\n🕘 Class time is 9:00-10:30am\nMOLB 7950 is a three credit hour course.\nThe course is divided into blocks:\n\n\nTHe Bootcamp block covers R programming and introduces important statistical concepts and approaches. We will also cover data types you will encounter during biological data analysis and approaches for their analysis.\nDuring the bootcamp block, we will meet everyday for 90 minutes to cover fundamental concepts you will need throughout the course.\n\n\n\nAfter Bootcamp, will cover experimental approaches used to analyze DNA and RNA. Each block spans ~4 weeks, with each week focused on a particular type of experiment (see below). Each block covers statistical concepts needed for rigorous analysis and analysis approaches to process raw data to results (tables and figures) using reproducible coding techniques.\nIn most weeks we will discuss and analyze data from a publication. You are responsible for reading the week’s material before class begins on Monday.\n\n\n\nThe DNA block covers genome sequencing for identifying mutations, and two approaches for analyzing chromatin state (ChIP-seq and MNase-seq).\nThe RNA block covers RNA-seq, alternative splicing, differential gene expression, and RNA:protein interactions."
  },
  {
    "objectID": "content/course-info/syllabus.html#schedule",
    "href": "content/course-info/syllabus.html#schedule",
    "title": "MOLB 7950 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nClasses begin on August 28 and end on November 1. Dates are from the Fall 2023 Academic Calendar.\nDuring the Bootcamp block, classes will be held every day, Mon-Fri from 9:00-10:30am.\nDuring the DNA & RNA blocks, we will have in-class exercises and discussion on Mon-Wed-Fri 9:00-10:30am."
  },
  {
    "objectID": "content/course-info/syllabus.html#location",
    "href": "content/course-info/syllabus.html#location",
    "title": "MOLB 7950 Syllabus",
    "section": "Location",
    "text": "Location\nClasses will be held in-person in AHSB 2201. All classes will be recorded and made available through Canvas."
  },
  {
    "objectID": "content/course-info/syllabus.html#policies",
    "href": "content/course-info/syllabus.html#policies",
    "title": "MOLB 7950 Syllabus",
    "section": "Policies",
    "text": "Policies\n\nAttendance\nClass attendance is a firm expectation; frequent absences or tardiness are considered cause for a grade reduction.\nif you are sick, please let us know (e-mail Jay and Neel) and stay home.\nAnticipated absences outside of sickness should be reported to the instructors of a given block as soon as possible to make plans for possible accommodation.\nWe will record all lectures on Panopto and they will be available online through Canvas.\n\n\nLate and missed work\nWe have a late work policy for homework assignments:\n\nIf a problem set set is late but within 24 hours of due date/time, the grade will be reduced by 50%\nIf a problem set is returned any later, no credit will be given.\nAll regrade requests must be discussed with the professor within one week of receiving your grade. There will be no grade changes after the final project.\n\n\n\nDiversity & Inclusiveness\nOur view is that students from all diverse backgrounds and perspectives will be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class iss a resource, strength, and benefit.\n\n\nDisability Policy\nStudents with disabilities who need accommodations are encouraged to contact the Office of Disability, Access & Inclusion as soon as possible to ensure that accommodations are implemented in a timely fashion.\n\n\nHonor code\nAcademic dishonesty will not be tolerated and is grounds for dismissal from the class with a failing grade (“F”). For other information, please consult the Graduate Student Handbook."
  },
  {
    "objectID": "content/course-info/syllabus.html#course-components",
    "href": "content/course-info/syllabus.html#course-components",
    "title": "MOLB 7950 Syllabus",
    "section": "Course components",
    "text": "Course components\n\nPre-requisites\nYou will need to complete three pre-requisites before the relevant section of the bootcamp starts:\n\nIntroduction to R (before week 2 of bootcamp)\nIntroduction to the tidyverse (before week 2 of bootcamp)\n\nThese should be listed under the Assignments in your DataCamp workspace.\n\n\nProblem Sets\n\nProblem sets will be assigned at the end of each class.\nYou can use external resources but must explicitly cite where you have obtained code (both code you used directly and “paraphrased” code / code used as inspiration). Any reused code that is not explicitly cited will be treated as plagiarism.\nYou can discuss the content of assignments with others in this class. If you do so, you must acknowledge your collaborator(s) at the top of your assignment, for example: “Collaborators: Hillary and Bernie”. Failure to acknowledge collaborators will result in a grade of 0. You may not copy code and/or answers directly from another student. If you copy other work, both parties will receive a grade of 0.\nThe problem set with the lowest score for each student will be dropped.\nRather than copying someone’s work, ask for help. You are not alone in this course!\n\n\n\nProfessionalism\n\nPlease refrain from texting or using your computer for anything other than coursework during class."
  },
  {
    "objectID": "content/course-info/syllabus.html#assignments-and-grading",
    "href": "content/course-info/syllabus.html#assignments-and-grading",
    "title": "MOLB 7950 Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThe course measures learning through daily problem sets, a final project, and your participation.\n\n\n\nType\n% of grade\n\n\n\n\nProblem Sets\n60\n\n\nFinal Project\n20\n\n\nParticpation\n20\n\n\n\nGrades will be assigned as follows:\n\n\n\nPercent total points\nGrade\n\n\n\n\n&gt;= 95\nA\n\n\n&gt;= 90\nA-\n\n\n&gt;= 85\nB+\n\n\n&gt;= 80\nB\n\n\n\n\nProblem sets\nWe reinforce concepts with problem sets assigned at the end of class that should take ~60 minutes to complete. Problems sets assigned on Friday will be more substantial, requiring ~1-2 hours to complete. Together the problem sets constitute 60% of your grade.\n\n\n\n\n\n\n\n\n\n\nAssigned\nDue\nGrades By\nWho grades\nTime to complete (approx)\n\n\n\n\nMon @ 12pm\nTues @ 5pm\nWed @ 5pm\nInstructors / TAs\n60 min\n\n\nTue @ 12pm\nWed @ 5pm\nThurs @ 5pm\nInstructors / TAs\n60 min\n\n\nWed @ 12pm\nThurs @ 5pm\nFri @ 5pm\nInstructors\n60 min\n\n\nThurs @ 12pm\nFri @ 5pm\nTues @ 5pm\nInstructors\n60 min\n\n\nFri @ 12pm\nMon @ 5pm\nWed @ 5pm\nInstructors / TAs\n1-2 hr\n\n\n\n\n\nFinal projects\nFinal projects can be completed in groups of 1-3 people. Projects will involve analysis of existing public data sets and end with a short presentation the last week of class. The final project constitutes 20% of your grade.\n\n\nGrading Rubrics\n\nProblem Set Rubric\nProblem sets are worth 60% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds Improvement\n\n\n\n\nCoding style\nStudent has gone beyond what was expected and required, coding manual is followed, code is well commented\nCoding style lacks refinement and has some errors, but code is readable and has some comments\nMany errors in coding style, little attention paid to making the code human readable\n\n\nCoding strategy\nComplicated problem broken down into sub-problems that are individually much simpler. Code is efficient, correct, and minimal. Code uses appropriate data structure (list, data frame, vector/matrix/array). Code checks for common errors\nCode is correct, but could be edited down to leaner code. Some “hacking” instead of using suitable data structure. Some checks for errors.\nCode tackles complicated problem in one big chunk. Code is repetitive and could easily be functionalized. No anticipation of errors.\n\n\nPresentation: graphs\nGraph(s) carefully tuned for desired purpose. One graph illustrates one point\nGraph(s) well chosen, but with a few minor problems: inappropriate aspect ratios, poor labels.\nGraph(s) poorly chosen to support questions.\n\n\nPresentation: tables\nTable(s) carefully constructed to make it easy to perform important comparisons. Careful styling highlights important features.\nTable(s) generally appropriate but possibly some minor formatting deficiencies.\nTable(s) with too many, or inconsistent, decimal places. Table(s) not appropriate for questions and findings. Major display problems.\n\n\nAchievement, mastery, cleverness, creativity\nStudent has gone beyond what was expected and required, e.g., extraordinary effort, additional tools not addressed by this course, unusually sophisticated application of tools from course.\nTools and techniques from the course are applied very competently and, perhaps,somewhat creatively. Chosen task was acceptable, but fairly conservative in ambition.\nStudent does not display the expected level of mastery of the tools and techniques in this course. Chosen task was too limited in scope.\n\n\nEase of access for instructor, compliance with course conventions for submitted work\nAccess as easy as possible, code runs!\nSatisfactory\nNot an earnest effort to reduce friction and comply with conventions and/or code does not run\n\n\n\n\n\nParticipation rubric\nParticipation is worth 20% of your grade. Values in parentheses represent point values for each level from 20 points total. This rubric will be assessed at the end of the semester.\n\n\n\n\n\n\n\n\n\nCriteria\nExpert\nCompetent\nNeeds improvement\n\n\n\n\nAttendance (physically present for class, or coordinating with instructor when absent)\nAttends class regularly (5)\nAttends most classes (4)\nAttends some classes (0-3)\n\n\nPreparation (activities required for in-class participation, like surveys and software installation)\nCompletes requested activities prior to class (5)\nCompletes most requested activities prior to class, sometimes needs to finish during class (4)\nRarely completes requested activities prior to class, often takes class time to complete (0-3)\n\n\nEngagement (in-class activities like coding exercises and discussion)\nActively engages in class activities (10)\nSometimes engages in class activities (8)\nDoesn’t engage in class activities (0-7)"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html",
    "href": "content/block-dna/variant-calling/class-01.html",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "VCF format\nFramework for variation discovery\n\n\n\n\n\n\nLearn to think about allele sampling as a binomial process\nUnderstand the implications of the binomial process for variant detection\nLearn to think about sequencing coverage as a Poisson distribution\nUnderstand the implications of the Poisson distribution on experimental design and cost.\n\n\n\n\nAllele sampling can be modeled as a coin flipping exercise.\nWith fair coin, the \\(P(heads)\\) = 0.5 and \\(P(tails)\\) = 0.5.\nWith fair genome sequencing of heterozygous C/T position, P(C) = 0.5 and P(T) = 0.5\nThe probability of k successes in n trials is given by the probability mass function:\n\\[ Pr(X=k) = {{n}\\choose{k}} p^k(1-p)^{n-k} \\]\nUsing this, we can ask: what is the probability of seeing \\(k = 1\\) tails in \\(n = 3\\) flips of a fair coin with \\(P(tail)\\) = 0.5?\n\nchoose(3, 1) * 0.5**1 * (1 - 0.5)**(3 - 1)\n\n[1] 0.375\n\n# or . . .\ndbinom(x = 1, size = 3, prob = 0.5)\n\n[1] 0.375\n\n\n\n\n\nLet’s say we sequence my genome to 30x coverage, i.e. each site is covered by a mean of 30 reads.\nEach read is a random sample from millions of double-stranded DNA fragments. After read alignment, is every site going to be covered by 30 reads?\nNo! Some sites have 17 reads aligned, some have 35. And this randomness is the problem we’re trying to address.\nWe model the alleles at each site as a binomial random variable that can hold one of two values. Think heads and tails of a coin, or reference and alternative alleles at a site.\nWe can use rbinom() to look at the distribution of outcomes.\n\nn: number of people tossing coins, or sites in the genome\nsize: number of times you looked at the coin, or looked at a site with a sequencing read, i.e. the sequencing coverage\nprob: probability of success (i.e., probability of tails with a fair coin)\n\n\nn &lt;- 30\nsize &lt;- 5\nprob &lt;- 0.5\n\nbarplot(table(rbinom(n, size, prob)))\n\n\n\n\n\n\nReady your coin!\nWe’re going to play a game.\nImagine you are all different sites in the genome. Your true identity (your genotype) is hidden. We are going to discover your true identity by flipping a coin.\nFor each set of coin flips, record the number of alternative alleles (tails) you see.\nFor those of you that are HETEROZYGOUS, we will learn your identity ONLY after 50% of your flips are alternative alleles (3 at toss6, 6 at toss12, 16 at toss32)\nEnter the number of times you see tails after flipping 6, 12, and 32 times on this spreadsheet:\nhttps://docs.google.com/spreadsheets/d/149q8wg6ctodnjZxnDj1SWUIw_Bjgvf9T0iVAvud0dVI/edit?usp=sharing\n\n\n\nsample(c(\"REF\", \"ALT\"), 1)\n\n[1] \"ALT\"\n\n\n\n\n\n\n\n\nplot_tosses &lt;- function(x) {\n  # returns a plot\n  ggplot(fct_count(as.factor(x)), aes(f, n)) +\n    geom_col() +\n    labs(\n      x = \"Number of tails (alternative alleles)\",\n      y = \"count\"\n    ) +\n    theme_minimal_grid()\n}\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 6 tosses?\n\n# theory\nn &lt;- 30\nsize &lt;- 5\nprob &lt;- 0.5\n\n# plot of counts\nx &lt;- rbinom(n, size, prob)\nplot_tosses(x)\n\n\n\n# real data\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 12 tosses?\n\n# theory\n# theory\nn &lt;- 30\nsize &lt;- 15\nprob &lt;- 0.5\n\n# plot of counts\nx &lt;- rbinom(n, size, prob)\nplot_tosses(x)\n\n\n\n# real data\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 32 tosses?\n\n# theory\n\n# real data\n\n\n\nThe binomial makes two assumptions:\n\nThe probability of success is the same in each trial (i.e. coin flip).\nThe trials are independent and do not affect each others’ outcome.\n\nWhen might these assumptions be violated by real genome sequencing data?\nExamine the heterozygous alignment slide\n\n\n\n\nIn the chunks above, each time we run rbinom() we get a different random sample. We can make this sample reproducible by setting a seed just before each random call:\n\n# not reproducible\nx &lt;- rbinom(10, 30, 0.5)\ny &lt;- rbinom(10, 30, 0.5)\n\nall(x == y)\n\n[1] FALSE\n\n# reproducible\nset.seed(1)\nx &lt;- rbinom(10, 30, 0.5)\nset.seed(1)\ny &lt;- rbinom(10, 30, 0.5)\n\nall(x == y)\n\n[1] TRUE\n\n\nWhat happens if you change the seed value above? I.e. Compare the values produced after set.seed(1) and set.seed(42).\nThe setting of seeds is a valuable tool if you want to generate a completely reproducible pipeline for others to run.\nFor example, the process of generating two-dimensional UMAP (or t-SNE) projection is a random process. You can recover the same projection each time by setting a seed at the appropriate step.\n\n\n\n\nWhat is the probability of seeing 10 or more tails in 20 flips of a fair coin? [0.5880985]\n\n\nsum(dbinom(10:20, 20, 0.5))\n\n[1] 0.5880985\n\n\n\nWhat is the probability of seeing 10 or fewer alternative alleles at 17-fold coverage, assuming equal allele probabilities? [0.833847]\n\n\nsum(dbinom(0:10, 17, 0.5))\n\n[1] 0.833847\n\n\n\nGenerate a random sample of 100 numbers uniformly distributed between 0 and 1. Now generate two more identical samples. Use all() to confirm whether the samples are identical .\n\n\nrunif(100)\n\n  [1] 0.20597457 0.17655675 0.68702285 0.38410372 0.76984142 0.49769924\n  [7] 0.71761851 0.99190609 0.38003518 0.77744522 0.93470523 0.21214252\n [13] 0.65167377 0.12555510 0.26722067 0.38611409 0.01339033 0.38238796\n [19] 0.86969085 0.34034900 0.48208012 0.59956583 0.49354131 0.18621760\n [25] 0.82737332 0.66846674 0.79423986 0.10794363 0.72371095 0.41127443\n [31] 0.82094629 0.64706019 0.78293276 0.55303631 0.52971958 0.78935623\n [37] 0.02333120 0.47723007 0.73231374 0.69273156 0.47761962 0.86120948\n [43] 0.43809711 0.24479728 0.07067905 0.09946616 0.31627171 0.51863426\n [49] 0.66200508 0.40683019 0.91287592 0.29360337 0.45906573 0.33239467\n [55] 0.65087047 0.25801678 0.47854525 0.76631067 0.08424691 0.87532133\n [61] 0.33907294 0.83944035 0.34668349 0.33377493 0.47635125 0.89219834\n [67] 0.86433947 0.38998954 0.77732070 0.96061800 0.43465948 0.71251468\n [73] 0.39999437 0.32535215 0.75708715 0.20269226 0.71112122 0.12169192\n [79] 0.24548851 0.14330438 0.23962942 0.05893438 0.64228826 0.87626921\n [85] 0.77891468 0.79730883 0.45527445 0.41008408 0.81087024 0.60493329\n [91] 0.65472393 0.35319727 0.27026015 0.99268406 0.63349326 0.21320814\n [97] 0.12937235 0.47811803 0.92407447 0.59876097\n\nset.seed(1)\nx &lt;- runif(100)\n\nset.seed(1)\ny &lt;- runif(100)\n\nall(x == y)\n\n[1] TRUE\n\n\n\nThought experiment: You create a new organism with an expanded genetic code containing a third base-pair (i.e, A:T, G:C, X:Y) that follows Chargaff’s rule. Does this expanded code change how you might approach sequencing its genome? What if the ploidy of this organism was larger, say 45N (like Tetrahymena)?\n\n\n\n\nFrom binomial theory, we learned we need a certain level of coverage to generate reliable variant calls, due to the randomness of sampling alleles at a site using sequencing.\nWe also learned that reads are not distributed randomly in a genome. Coverage is itself a distribution, and we typically discuss it’s mean. I.e. 30-fold coverage means on average each base is sequenced 30 times.\nHow do we estimate the numbers of times a base is expected to be sequenced given a certain level of coverage?\nThis theory comes from Lander and Waterman, who made two assumptions about sequencing:\n\nReads will be distributed randomly across the genome.\nThe ability to detect overlaps (alignments) doesn’t vary between reads\n\nThey concluded that read coverage is modeled by the Poisson distribution.\nThe Poisson probability function is:\n\\[ P(Y = y) = \\frac{{ e^{ - C } C ^ y }} {{ y!}} \\] where:\n\ny is the number of times a base is read. It is the exact number of times a base is sequenced.\nC is the mean coverage (lambda in the traditional Poisson sense). It’s the mean number of aligned reads covering a site.\n\nThe formula above gives the probability of a base being sequenced a certain number of times.\nFor example, what is the probability of a base being sequenced 3 times or less at at mean coverage of 10?\n\ny &lt;- 0:3\nC &lt;- 10\n\nsum(dpois(y, C))\n\n[1] 0.01033605\n\n# or ...\nppois(3, C)\n\n[1] 0.01033605\n\n\n\n\nWhat if we’re trying to characterize heterozygous alleles in a human genome at differing levels of coverage?\n\nHow many variants are in an average human genome? [Answer 4285714]\n\n\n# size (bp) of the haploid human genome\nG &lt;- 3e9\n# chance of seeing a common variant in a random person. comes from e.g.\n# https://www.nature.com/articles/nature15393\np_var &lt;- 1 / 700\n\n# calculate number of variants\nn_var &lt;- G * p_var\nn_var\n\n[1] 4285714\n\n\n\nHow many of these variants have zero coverage (i.e., no reads covering those sites) after sequencing to mean coverage of 5, 15, 30-fold? [Answer 28876.92, 1.61, 4.01041e-07].\n\n\nppois(0, 5) * n_var\n\n[1] 28876.92\n\nppois(0, 15) * n_var\n\n[1] 1.31101\n\nppois(0, 30) * n_var\n\n[1] 4.01041e-07\n\n\n\n\n\nThis theory boils down to a general equation, the Lander/Waterman equation, for computing the coverage you have given a certain level of sequencing:\n\\[ C = LN / G \\]\nwhere:\n\nC is coverage. I.e., the mean number of times each base is covered by a read.\nG is the haploid genome length. E.g., 3e9 for the human genome.\nL is the read length. On Illumina, typically 300 bp.\nN is the number of reads.\n\nLet’s use this equation to calculate the coverage for a few different genomes, given fixed read number and length.\n\nG_yeast &lt;- 12e6\nG_human &lt;- 3e9\n\nL &lt;- 300\nN &lt;- 20e6\n\nC_yeast &lt;- (L * N) / G_yeast\nC_human &lt;- (L * N) / G_human\n\nObviously those parameters provide a powerful approach for yeast genetics, but much less so for human genetics.\n\n\n\nOn an Illumina Novaseq 6000, each 300 bp read costs about 3^{-5} USD.\nHow much would it cost to sequence a single human at 30X coverage? Does that number surprise you?\n\n# how many reads do we need?\n# note that each read is 300 bp (150 bp from each end of a fragment)\nn &lt;- (30 * 3e9) / 300\n\n# how much does each read cost?\ncost &lt;- 3e4 / 10e9\n\nn * cost\n\n\n\n\n\nError rate of sequencing can have a big impact on variant interpretation.\nIllumina error rates are relatively low, ONT rates are relatively high. We’ll look at this by pulling quality scores from a 1,000 reads from the Illumina and ONT platforms.\nFirst, read data from the FASTQ files. This is not rectangular data, so we’ll use Python.\n\nimport gzip\nfrom collections import Counter\n\ndef qual_counts(fq_gz):\n  quals = Counter()\n  nl = 0\n  with gzip.open(fq_gz, 'rb') as fq:\n    for line in fq:\n      if nl % 4 == 3:\n        quals.update(line.strip())\n      nl += 1\n  return quals      \n\nillumina_quals = qual_counts('data/illumina.fastq.gz')\nont_quals = qual_counts('data/ont.fastq.gz')\n\nfh = open('quals.tsv', 'w')\nfor qual, count in illumina_quals.items():\n  print(qual, count, 'illumina', file=fh, sep='\\t')\nfor qual, count in ont_quals.items():\n  print(qual, count, 'ont', file=fh, sep='\\t')\nfh.close()\n\nThe counts are in TSV format now, back to R (phew).\n\ntab &lt;- read_tsv(\"quals.tsv\", col_names = c(\"qual\", \"count\", \"type\"))\n\nRows: 59 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (1): type\ndbl (2): qual, count\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(tab, aes(qual, count, fill = type, alpha = 0.2)) +\n  geom_col() +\n  labs(\n    title = \"Quality score comparison Illumina and ONT reads\",\n    subtitle = \"Thoughts?\"\n  ) +\n  theme_minimal_grid() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nWhy don’t we just sequence everybody to, like, 1000-fold coverage, and not worry about binomial sampling theory?\nDetermine the number of reads needed to sequence the human genome to 1000X coverage given the data below.\n\n\n# standard Illumina read length\nL &lt;- 300\n# size (bp) of haploid human genome\nG &lt;- 3e9\n\n# Use the Lander-Waterman equation to solve for read number:\n\nNow figure out how much this experiment would cost.\n\n# How much would this cost?\nusd_per_bp &lt;- 30e4 / 10e9\n\nHow might you dramatically reduce the cost of this experiment, but retain much of the useful information acquired from the genome? I.e., how would you “zoom in” to informative portions of the genome?\n\nThought experiment: The organism you’re studying happens to be tetraploid (like salmon). You want to use genome sequencing to identify variants in an individual. How does this change the way you think about allele sampling and coverage?\nThought experiment: How does this theory impact your thinking in designing an experiment where coverage variation along the genome is the signal (e.g., chromatin immunoprecipitation + sequencing).\nGiven what you learned about read coverage sampling, what statistical models do you think “peak calling” algorithms use?\n\n\n\n\nThe slides and content around the theory of variant sampling were borrowed from Aaron Quinlan’s Applied Computational Genomics course."
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#reading",
    "href": "content/block-dna/variant-calling/class-01.html#reading",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "VCF format\nFramework for variation discovery"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#learning-objectives",
    "href": "content/block-dna/variant-calling/class-01.html#learning-objectives",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "Learn to think about allele sampling as a binomial process\nUnderstand the implications of the binomial process for variant detection\nLearn to think about sequencing coverage as a Poisson distribution\nUnderstand the implications of the Poisson distribution on experimental design and cost."
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#allele-sampling-and-the-binomial-distribution",
    "href": "content/block-dna/variant-calling/class-01.html#allele-sampling-and-the-binomial-distribution",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "Allele sampling can be modeled as a coin flipping exercise.\nWith fair coin, the \\(P(heads)\\) = 0.5 and \\(P(tails)\\) = 0.5.\nWith fair genome sequencing of heterozygous C/T position, P(C) = 0.5 and P(T) = 0.5\nThe probability of k successes in n trials is given by the probability mass function:\n\\[ Pr(X=k) = {{n}\\choose{k}} p^k(1-p)^{n-k} \\]\nUsing this, we can ask: what is the probability of seeing \\(k = 1\\) tails in \\(n = 3\\) flips of a fair coin with \\(P(tail)\\) = 0.5?\n\nchoose(3, 1) * 0.5**1 * (1 - 0.5)**(3 - 1)\n\n[1] 0.375\n\n# or . . .\ndbinom(x = 1, size = 3, prob = 0.5)\n\n[1] 0.375"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#simulating-sequencing-outcomes",
    "href": "content/block-dna/variant-calling/class-01.html#simulating-sequencing-outcomes",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "Let’s say we sequence my genome to 30x coverage, i.e. each site is covered by a mean of 30 reads.\nEach read is a random sample from millions of double-stranded DNA fragments. After read alignment, is every site going to be covered by 30 reads?\nNo! Some sites have 17 reads aligned, some have 35. And this randomness is the problem we’re trying to address.\nWe model the alleles at each site as a binomial random variable that can hold one of two values. Think heads and tails of a coin, or reference and alternative alleles at a site.\nWe can use rbinom() to look at the distribution of outcomes.\n\nn: number of people tossing coins, or sites in the genome\nsize: number of times you looked at the coin, or looked at a site with a sequencing read, i.e. the sequencing coverage\nprob: probability of success (i.e., probability of tails with a fair coin)\n\n\nn &lt;- 30\nsize &lt;- 5\nprob &lt;- 0.5\n\nbarplot(table(rbinom(n, size, prob)))\n\n\n\n\n\n\nReady your coin!\nWe’re going to play a game.\nImagine you are all different sites in the genome. Your true identity (your genotype) is hidden. We are going to discover your true identity by flipping a coin.\nFor each set of coin flips, record the number of alternative alleles (tails) you see.\nFor those of you that are HETEROZYGOUS, we will learn your identity ONLY after 50% of your flips are alternative alleles (3 at toss6, 6 at toss12, 16 at toss32)\nEnter the number of times you see tails after flipping 6, 12, and 32 times on this spreadsheet:\nhttps://docs.google.com/spreadsheets/d/149q8wg6ctodnjZxnDj1SWUIw_Bjgvf9T0iVAvud0dVI/edit?usp=sharing\n\n\n\nsample(c(\"REF\", \"ALT\"), 1)\n\n[1] \"ALT\""
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#comparing-theory-with-real-data",
    "href": "content/block-dna/variant-calling/class-01.html#comparing-theory-with-real-data",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "plot_tosses &lt;- function(x) {\n  # returns a plot\n  ggplot(fct_count(as.factor(x)), aes(f, n)) +\n    geom_col() +\n    labs(\n      x = \"Number of tails (alternative alleles)\",\n      y = \"count\"\n    ) +\n    theme_minimal_grid()\n}\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 6 tosses?\n\n# theory\nn &lt;- 30\nsize &lt;- 5\nprob &lt;- 0.5\n\n# plot of counts\nx &lt;- rbinom(n, size, prob)\nplot_tosses(x)\n\n\n\n# real data\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 12 tosses?\n\n# theory\n# theory\nn &lt;- 30\nsize &lt;- 15\nprob &lt;- 0.5\n\n# plot of counts\nx &lt;- rbinom(n, size, prob)\nplot_tosses(x)\n\n\n\n# real data\n\nWhat is the distribution of tails (alternate alleles) we expect to see after 32 tosses?\n\n# theory\n\n# real data\n\n\n\nThe binomial makes two assumptions:\n\nThe probability of success is the same in each trial (i.e. coin flip).\nThe trials are independent and do not affect each others’ outcome.\n\nWhen might these assumptions be violated by real genome sequencing data?\nExamine the heterozygous alignment slide"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#reproducibility-with-seeds-short-aside",
    "href": "content/block-dna/variant-calling/class-01.html#reproducibility-with-seeds-short-aside",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "In the chunks above, each time we run rbinom() we get a different random sample. We can make this sample reproducible by setting a seed just before each random call:\n\n# not reproducible\nx &lt;- rbinom(10, 30, 0.5)\ny &lt;- rbinom(10, 30, 0.5)\n\nall(x == y)\n\n[1] FALSE\n\n# reproducible\nset.seed(1)\nx &lt;- rbinom(10, 30, 0.5)\nset.seed(1)\ny &lt;- rbinom(10, 30, 0.5)\n\nall(x == y)\n\n[1] TRUE\n\n\nWhat happens if you change the seed value above? I.e. Compare the values produced after set.seed(1) and set.seed(42).\nThe setting of seeds is a valuable tool if you want to generate a completely reproducible pipeline for others to run.\nFor example, the process of generating two-dimensional UMAP (or t-SNE) projection is a random process. You can recover the same projection each time by setting a seed at the appropriate step."
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#excercises",
    "href": "content/block-dna/variant-calling/class-01.html#excercises",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "What is the probability of seeing 10 or more tails in 20 flips of a fair coin? [0.5880985]\n\n\nsum(dbinom(10:20, 20, 0.5))\n\n[1] 0.5880985\n\n\n\nWhat is the probability of seeing 10 or fewer alternative alleles at 17-fold coverage, assuming equal allele probabilities? [0.833847]\n\n\nsum(dbinom(0:10, 17, 0.5))\n\n[1] 0.833847\n\n\n\nGenerate a random sample of 100 numbers uniformly distributed between 0 and 1. Now generate two more identical samples. Use all() to confirm whether the samples are identical .\n\n\nrunif(100)\n\n  [1] 0.20597457 0.17655675 0.68702285 0.38410372 0.76984142 0.49769924\n  [7] 0.71761851 0.99190609 0.38003518 0.77744522 0.93470523 0.21214252\n [13] 0.65167377 0.12555510 0.26722067 0.38611409 0.01339033 0.38238796\n [19] 0.86969085 0.34034900 0.48208012 0.59956583 0.49354131 0.18621760\n [25] 0.82737332 0.66846674 0.79423986 0.10794363 0.72371095 0.41127443\n [31] 0.82094629 0.64706019 0.78293276 0.55303631 0.52971958 0.78935623\n [37] 0.02333120 0.47723007 0.73231374 0.69273156 0.47761962 0.86120948\n [43] 0.43809711 0.24479728 0.07067905 0.09946616 0.31627171 0.51863426\n [49] 0.66200508 0.40683019 0.91287592 0.29360337 0.45906573 0.33239467\n [55] 0.65087047 0.25801678 0.47854525 0.76631067 0.08424691 0.87532133\n [61] 0.33907294 0.83944035 0.34668349 0.33377493 0.47635125 0.89219834\n [67] 0.86433947 0.38998954 0.77732070 0.96061800 0.43465948 0.71251468\n [73] 0.39999437 0.32535215 0.75708715 0.20269226 0.71112122 0.12169192\n [79] 0.24548851 0.14330438 0.23962942 0.05893438 0.64228826 0.87626921\n [85] 0.77891468 0.79730883 0.45527445 0.41008408 0.81087024 0.60493329\n [91] 0.65472393 0.35319727 0.27026015 0.99268406 0.63349326 0.21320814\n [97] 0.12937235 0.47811803 0.92407447 0.59876097\n\nset.seed(1)\nx &lt;- runif(100)\n\nset.seed(1)\ny &lt;- runif(100)\n\nall(x == y)\n\n[1] TRUE\n\n\n\nThought experiment: You create a new organism with an expanded genetic code containing a third base-pair (i.e, A:T, G:C, X:Y) that follows Chargaff’s rule. Does this expanded code change how you might approach sequencing its genome? What if the ploidy of this organism was larger, say 45N (like Tetrahymena)?"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#sequencing-coverage",
    "href": "content/block-dna/variant-calling/class-01.html#sequencing-coverage",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "From binomial theory, we learned we need a certain level of coverage to generate reliable variant calls, due to the randomness of sampling alleles at a site using sequencing.\nWe also learned that reads are not distributed randomly in a genome. Coverage is itself a distribution, and we typically discuss it’s mean. I.e. 30-fold coverage means on average each base is sequenced 30 times.\nHow do we estimate the numbers of times a base is expected to be sequenced given a certain level of coverage?\nThis theory comes from Lander and Waterman, who made two assumptions about sequencing:\n\nReads will be distributed randomly across the genome.\nThe ability to detect overlaps (alignments) doesn’t vary between reads\n\nThey concluded that read coverage is modeled by the Poisson distribution.\nThe Poisson probability function is:\n\\[ P(Y = y) = \\frac{{ e^{ - C } C ^ y }} {{ y!}} \\] where:\n\ny is the number of times a base is read. It is the exact number of times a base is sequenced.\nC is the mean coverage (lambda in the traditional Poisson sense). It’s the mean number of aligned reads covering a site.\n\nThe formula above gives the probability of a base being sequenced a certain number of times.\nFor example, what is the probability of a base being sequenced 3 times or less at at mean coverage of 10?\n\ny &lt;- 0:3\nC &lt;- 10\n\nsum(dpois(y, C))\n\n[1] 0.01033605\n\n# or ...\nppois(3, C)\n\n[1] 0.01033605\n\n\n\n\nWhat if we’re trying to characterize heterozygous alleles in a human genome at differing levels of coverage?\n\nHow many variants are in an average human genome? [Answer 4285714]\n\n\n# size (bp) of the haploid human genome\nG &lt;- 3e9\n# chance of seeing a common variant in a random person. comes from e.g.\n# https://www.nature.com/articles/nature15393\np_var &lt;- 1 / 700\n\n# calculate number of variants\nn_var &lt;- G * p_var\nn_var\n\n[1] 4285714\n\n\n\nHow many of these variants have zero coverage (i.e., no reads covering those sites) after sequencing to mean coverage of 5, 15, 30-fold? [Answer 28876.92, 1.61, 4.01041e-07].\n\n\nppois(0, 5) * n_var\n\n[1] 28876.92\n\nppois(0, 15) * n_var\n\n[1] 1.31101\n\nppois(0, 30) * n_var\n\n[1] 4.01041e-07\n\n\n\n\n\nThis theory boils down to a general equation, the Lander/Waterman equation, for computing the coverage you have given a certain level of sequencing:\n\\[ C = LN / G \\]\nwhere:\n\nC is coverage. I.e., the mean number of times each base is covered by a read.\nG is the haploid genome length. E.g., 3e9 for the human genome.\nL is the read length. On Illumina, typically 300 bp.\nN is the number of reads.\n\nLet’s use this equation to calculate the coverage for a few different genomes, given fixed read number and length.\n\nG_yeast &lt;- 12e6\nG_human &lt;- 3e9\n\nL &lt;- 300\nN &lt;- 20e6\n\nC_yeast &lt;- (L * N) / G_yeast\nC_human &lt;- (L * N) / G_human\n\nObviously those parameters provide a powerful approach for yeast genetics, but much less so for human genetics.\n\n\n\nOn an Illumina Novaseq 6000, each 300 bp read costs about 3^{-5} USD.\nHow much would it cost to sequence a single human at 30X coverage? Does that number surprise you?\n\n# how many reads do we need?\n# note that each read is 300 bp (150 bp from each end of a fragment)\nn &lt;- (30 * 3e9) / 300\n\n# how much does each read cost?\ncost &lt;- 3e4 / 10e9\n\nn * cost"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#sequencing-error-rate",
    "href": "content/block-dna/variant-calling/class-01.html#sequencing-error-rate",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "Error rate of sequencing can have a big impact on variant interpretation.\nIllumina error rates are relatively low, ONT rates are relatively high. We’ll look at this by pulling quality scores from a 1,000 reads from the Illumina and ONT platforms.\nFirst, read data from the FASTQ files. This is not rectangular data, so we’ll use Python.\n\nimport gzip\nfrom collections import Counter\n\ndef qual_counts(fq_gz):\n  quals = Counter()\n  nl = 0\n  with gzip.open(fq_gz, 'rb') as fq:\n    for line in fq:\n      if nl % 4 == 3:\n        quals.update(line.strip())\n      nl += 1\n  return quals      \n\nillumina_quals = qual_counts('data/illumina.fastq.gz')\nont_quals = qual_counts('data/ont.fastq.gz')\n\nfh = open('quals.tsv', 'w')\nfor qual, count in illumina_quals.items():\n  print(qual, count, 'illumina', file=fh, sep='\\t')\nfor qual, count in ont_quals.items():\n  print(qual, count, 'ont', file=fh, sep='\\t')\nfh.close()\n\nThe counts are in TSV format now, back to R (phew).\n\ntab &lt;- read_tsv(\"quals.tsv\", col_names = c(\"qual\", \"count\", \"type\"))\n\nRows: 59 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (1): type\ndbl (2): qual, count\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(tab, aes(qual, count, fill = type, alpha = 0.2)) +\n  geom_col() +\n  labs(\n    title = \"Quality score comparison Illumina and ONT reads\",\n    subtitle = \"Thoughts?\"\n  ) +\n  theme_minimal_grid() +\n  scale_fill_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#excercises-2",
    "href": "content/block-dna/variant-calling/class-01.html#excercises-2",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "Why don’t we just sequence everybody to, like, 1000-fold coverage, and not worry about binomial sampling theory?\nDetermine the number of reads needed to sequence the human genome to 1000X coverage given the data below.\n\n\n# standard Illumina read length\nL &lt;- 300\n# size (bp) of haploid human genome\nG &lt;- 3e9\n\n# Use the Lander-Waterman equation to solve for read number:\n\nNow figure out how much this experiment would cost.\n\n# How much would this cost?\nusd_per_bp &lt;- 30e4 / 10e9\n\nHow might you dramatically reduce the cost of this experiment, but retain much of the useful information acquired from the genome? I.e., how would you “zoom in” to informative portions of the genome?\n\nThought experiment: The organism you’re studying happens to be tetraploid (like salmon). You want to use genome sequencing to identify variants in an individual. How does this change the way you think about allele sampling and coverage?\nThought experiment: How does this theory impact your thinking in designing an experiment where coverage variation along the genome is the signal (e.g., chromatin immunoprecipitation + sequencing).\nGiven what you learned about read coverage sampling, what statistical models do you think “peak calling” algorithms use?"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-01.html#resources",
    "href": "content/block-dna/variant-calling/class-01.html#resources",
    "title": "Genome sequencing 1 - theory",
    "section": "",
    "text": "The slides and content around the theory of variant sampling were borrowed from Aaron Quinlan’s Applied Computational Genomics course."
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html",
    "href": "content/block-dna/variant-calling/class-02.html",
    "title": "Genome sequencing 2 - practice",
    "section": "",
    "text": "From binomial theory, we learned we need a certain level of coverage to generate reliable variant calls, due to the randomness of sampling alleles at a site using sequencing.\nWe also learned that reads are not distributed randomly in a genome. Coverage is itself a distribution, and we typically discuss it’s mean. I.e. 30-fold coverage means on average each base is sequenced 30 times.\nHow do we estimate the numbers of times a base is expected to be sequenced given a certain level of coverage?\nThis theory comes from Lander and Waterman, who made two assumptions about sequencing:\n\nReads will be distributed randomly across the genome.\nThe ability to detect overlaps (alignments) doesn’t vary between reads\n\nThey concluded that read coverage is modeled by the Poisson distribution.\nThe Poisson probability function is:\n\\[ P(Y = y) = \\frac{{ e^{ - C } C ^ y }} {{ y!}} \\]\nwhere:\n\ny is the number of times a base is read. It is the exact number of times a base is sequenced.\nC is the mean coverage (lambda in the traditional Poisson sense). It’s the mean number of aligned reads covering a site.\n\nThe formula above gives the probability of a base being sequenced a certain number of times.\nFor example, what is the probability of a base being sequenced 3 times or less at at mean coverage of 10?\n\nsum(dpois(x = 0:3, lambda = 10))\n\n[1] 0.01033605\n\n# or ...\nppois(3, lambda = 10)\n\n[1] 0.01033605\n\n\n\n\nWhat if we’re trying to characterize heterozygous alleles in a human genome at differing levels of coverage?\n\nHow many variants are in an average human genome? [Answer 4285714]\n\n\n# size (bp) of the haploid human genome\nG &lt;- 3e9\n# chance of seeing a common variant in a random person. comes from e.g.\n# https://www.nature.com/articles/nature15393\np_var &lt;- 1 / 700\n\n# calculate number of variants\nn_var &lt;- G * p_var\nn_var\n\n[1] 4285714\n\n\n\nHow many of these variants have zero coverage (i.e., no reads covering those sites) after sequencing to mean coverage of 5, 15, 30-fold? [Answer 28876.92, 1.61, 4.01041e-07].\n\n\nppois(0, 5) * n_var\n\n[1] 28876.92\n\nppois(0, 15) * n_var\n\n[1] 1.31101\n\nppois(0, 30) * n_var\n\n[1] 4.01041e-07"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#sequencing-coverage",
    "href": "content/block-dna/variant-calling/class-02.html#sequencing-coverage",
    "title": "Genome sequencing 2 - practice",
    "section": "",
    "text": "From binomial theory, we learned we need a certain level of coverage to generate reliable variant calls, due to the randomness of sampling alleles at a site using sequencing.\nWe also learned that reads are not distributed randomly in a genome. Coverage is itself a distribution, and we typically discuss it’s mean. I.e. 30-fold coverage means on average each base is sequenced 30 times.\nHow do we estimate the numbers of times a base is expected to be sequenced given a certain level of coverage?\nThis theory comes from Lander and Waterman, who made two assumptions about sequencing:\n\nReads will be distributed randomly across the genome.\nThe ability to detect overlaps (alignments) doesn’t vary between reads\n\nThey concluded that read coverage is modeled by the Poisson distribution.\nThe Poisson probability function is:\n\\[ P(Y = y) = \\frac{{ e^{ - C } C ^ y }} {{ y!}} \\]\nwhere:\n\ny is the number of times a base is read. It is the exact number of times a base is sequenced.\nC is the mean coverage (lambda in the traditional Poisson sense). It’s the mean number of aligned reads covering a site.\n\nThe formula above gives the probability of a base being sequenced a certain number of times.\nFor example, what is the probability of a base being sequenced 3 times or less at at mean coverage of 10?\n\nsum(dpois(x = 0:3, lambda = 10))\n\n[1] 0.01033605\n\n# or ...\nppois(3, lambda = 10)\n\n[1] 0.01033605\n\n\n\n\nWhat if we’re trying to characterize heterozygous alleles in a human genome at differing levels of coverage?\n\nHow many variants are in an average human genome? [Answer 4285714]\n\n\n# size (bp) of the haploid human genome\nG &lt;- 3e9\n# chance of seeing a common variant in a random person. comes from e.g.\n# https://www.nature.com/articles/nature15393\np_var &lt;- 1 / 700\n\n# calculate number of variants\nn_var &lt;- G * p_var\nn_var\n\n[1] 4285714\n\n\n\nHow many of these variants have zero coverage (i.e., no reads covering those sites) after sequencing to mean coverage of 5, 15, 30-fold? [Answer 28876.92, 1.61, 4.01041e-07].\n\n\nppois(0, 5) * n_var\n\n[1] 28876.92\n\nppois(0, 15) * n_var\n\n[1] 1.31101\n\nppois(0, 30) * n_var\n\n[1] 4.01041e-07"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#learning-objectives",
    "href": "content/block-dna/variant-calling/class-02.html#learning-objectives",
    "title": "Genome sequencing 2 - practice",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the format and content of VCF files\nLearn how to parse and analyze VCF files to summarize and identify candidate variants\nUnderstand the value of variant annotation for variant interpretation."
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#the-experiment",
    "href": "content/block-dna/variant-calling/class-02.html#the-experiment",
    "title": "Genome sequencing 2 - practice",
    "section": "The experiment",
    "text": "The experiment\nYeast alpha and beta tubulin genes encode heterodimeric protein components that assemble into long filaments important for cell division and motility.\n\nThe parent strain has two point mutations at the TUB1 and TUB3 loci.\nSuppressor mutants of this strain were selected for growth in the presence of a microtubule destabilizing drug.\nSuppressor isolates were each backcrossed 2X to the parent strain, confirming that suppression segregates 2:2.\nAfter the backcross, a suppressor F2 isolate was backcrossed to our wild type strain to test whether suppression was linked to the TUB1 or TUB3 mutants; we found suppression was not linked.\n\nStrains 4365-4368 are a tetrad from this F2 x wt cross with genotypes:\n\n4366_S2 (tub sup)\n4365_S1 (tub SUP)\n4367_S3 (TUB sup)\n4368_S4 (TUB SUP)\n\nGoal: identify and characterize genetic variants that segregate in the expected 2:2 pattern in these strains\n\nThe awesome power of yeast genetics\nYeast are not humans (duh). They can be made stably haploid, so the genotype-phenotype link is easier to study.\nThis is valuable because you can directly isolate a recessive mutation in a genetic background. Moreover you can do a cross and identify mutations that segregate with the phenotype among the progeny. The advantage here is that you can identify common variants (relative to the reference genome) that are not associated with phenotype and eliminate them as candidates.\n“Backcrossing” refers to the process of crossing an offspring to its wild-type parent. Repeated backcrossing and mutant isolation allows fine mapping of monogenic traits via increased numbers of meiotic crossovers.\n\n\nHuman trio sequencing\nIdentifying variants in a yeast cross is conceptually similar to human trio sequencing, wherein you sequence MOM, DAD, and KID to identify variants in KID that plausibly explain KID’s phenotype. The main difference between the yeast and human experiments is that humans are diploid, so you’re often looking for either:\n\nHeterzygous alleles of an uncommon variant in MOM and DAD; and a homozygous alternative allele in KID. (recessive case)\nGain-of-function, de novo alleles in KID that are not in MOM and DAD. (dominant case)\n\nYou can increase the power of the approach in humans by expanding to people in a pedigree where the phenotype has been annotated.\n\n\nGenome sequencing\n\nPurify genomic DNA and shear to ~500 bp fragments\nLigate adaptor oligonucleotides, PCR amplify to incorporate sample-specific barcodes\nSequence this library, collecting ~10 million, 2 x 150 bp reads from the ends of each fragment\n\n\n\nData analysis\nWe’re not going to do this because it takes more computational resources than RStudio Cloud can provide. For reference, these two steps take &lt; 60 minutes (for the yeast genome)\n\nAlign to the reference Saccharomyces cerevisiae reference genome with bwa. [FASTQ &gt; BAM].\nbwa mem -x ref.fa reads.fq | samtools sort &gt; alignment.bam\nCall variants in the BAM file with respect to the reference using freebayes [BAM &gt; VCF]\nfreebayes -f ref.fa alignment.bam &gt; combined.vcf"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#vcf-inspection",
    "href": "content/block-dna/variant-calling/class-02.html#vcf-inspection",
    "title": "Genome sequencing 2 - practice",
    "section": "VCF inspection",
    "text": "VCF inspection\n\nVariant identification\nThere two files in data/:\n\ncombined.vcf.gz\ncombined.annotated.vcf.gz\n\nFor the first steps, we’ll start with the first file, combined.vcf.gz.\nWe’ll use the VariantAnnotation library from Bioconductor to inspect the file.\nLet’s inspect the VCF file to review its structure.\n\nlibrary(VariantAnnotation)\nvcf &lt;- readVcf(\"data/combined.vcf.gz\")\nvcf\n\nclass: CollapsedVCF \ndim: 16708 4 \nrowRanges(vcf):\n  GRanges with 5 metadata columns: paramRangeID, REF, ALT, QUAL, FILTER\ninfo(vcf):\n  DataFrame with 43 columns: NS, DP, DPB, AC, AN, AF, RO, AO, PRO, PAO, QR, ...\ninfo(header(vcf)):\n           Number Type    Description                                          \n   NS      1      Integer Number of samples with data                          \n   DP      1      Integer Total read depth at the locus                        \n   DPB     1      Float   Total read depth per bp at the locus; bases in rea...\n   AC      A      Integer Total number of alternate alleles in called genotypes\n   AN      1      Integer Total number of alleles in called genotypes          \n   AF      A      Float   Estimated allele frequency in the range (0,1]        \n   RO      1      Integer Count of full observations of the reference haplot...\n   AO      A      Integer Count of full observations of this alternate haplo...\n   PRO     1      Float   Reference allele observation count, with partial o...\n   PAO     A      Float   Alternate allele observations, with partial observ...\n   QR      1      Integer Reference allele quality sum in phred                \n   QA      A      Integer Alternate allele quality sum in phred                \n   PQR     1      Float   Reference allele quality sum in phred for partial ...\n   PQA     A      Float   Alternate allele quality sum in phred for partial ...\n   SRF     1      Integer Number of reference observations on the forward st...\n   SRR     1      Integer Number of reference observations on the reverse st...\n   SAF     A      Integer Number of alternate observations on the forward st...\n   SAR     A      Integer Number of alternate observations on the reverse st...\n   SRP     1      Float   Strand balance probability for the reference allel...\n   SAP     A      Float   Strand balance probability for the alternate allel...\n   AB      A      Float   Allele balance at heterozygous sites: a number bet...\n   ABP     A      Float   Allele balance probability at heterozygous sites: ...\n   RUN     A      Integer Run length: the number of consecutive repeats of t...\n   RPP     A      Float   Read Placement Probability: Phred-scaled upper-bou...\n   RPPR    1      Float   Read Placement Probability for reference observati...\n   RPL     A      Float   Reads Placed Left: number of reads supporting the ...\n   RPR     A      Float   Reads Placed Right: number of reads supporting the...\n   EPP     A      Float   End Placement Probability: Phred-scaled upper-boun...\n   EPPR    1      Float   End Placement Probability for reference observatio...\n   DPRA    A      Float   Alternate allele depth ratio.  Ratio between depth...\n   ODDS    1      Float   The log odds ratio of the best genotype combinatio...\n   GTI     1      Integer Number of genotyping iterations required to reach ...\n   TYPE    A      String  The type of allele, either snp, mnp, ins, del, or ...\n   CIGAR   A      String  The extended CIGAR representation of each alternat...\n   NUMALT  1      Integer Number of unique non-reference alleles in called g...\n   MEANALT A      Float   Mean number of unique non-reference allele observa...\n   LEN     A      Integer allele length                                        \n   MQM     A      Float   Mean mapping quality of observed alternate alleles   \n   MQMR    1      Float   Mean mapping quality of observed reference alleles   \n   PAIRED  A      Float   Proportion of observed alternate alleles which are...\n   PAIREDR 1      Float   Proportion of observed reference alleles which are...\n   MIN_DP  1      Integer Minimum depth in gVCF output block.                  \n   END     1      Integer Last position (inclusive) in gVCF output record.     \ngeno(vcf):\n  List of length 10: GT, GQ, GL, DP, AD, RO, QR, AO, QA, MIN_DP\ngeno(header(vcf)):\n          Number Type    Description                                           \n   GT     1      String  Genotype                                              \n   GQ     1      Float   Genotype Quality, the Phred-scaled marginal (or unc...\n   GL     G      Float   Genotype Likelihood, log10-scaled likelihoods of th...\n   DP     1      Integer Read Depth                                            \n   AD     R      Integer Number of observation for each allele                 \n   RO     1      Integer Reference allele observation count                    \n   QR     1      Integer Sum of quality of the reference observations          \n   AO     A      Integer Alternate allele observation count                    \n   QA     A      Integer Sum of quality of the alternate observations          \n   MIN_DP 1      Integer Minimum depth in gVCF output block.                   \n\n# What are the first few lines?\n# What are the column names (first line after the comment section)\n# What is in the INFO field?\n# What is in the FORMAT field?\n\nHow many samples are in the VCF file?\n\n# use `length` to get the number of samples\nsamples(header(vcf))\n\n[1] \"4366_S2\" \"4365_S1\" \"4367_S3\" \"4368_S4\"\n\n\n\nExcercises\nHow many variants are in the VCF file?\n\nHow many sites involve complex REF or ALT alleles (i.e., not SNPs)?"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#identifying-variants-associated-with-phenotypes",
    "href": "content/block-dna/variant-calling/class-02.html#identifying-variants-associated-with-phenotypes",
    "title": "Genome sequencing 2 - practice",
    "section": "Identifying variants associated with phenotypes",
    "text": "Identifying variants associated with phenotypes\nIn yeast genetics, phenotypic status is represented by capitalization. After selecting for a phenotype and doing genetic mapping, organisms that are wild-type for a trait are noted with upper case letters, while mutant organisms are lower case.\nFor example, two of the strains below have wild-type tubulin function (TUB) and two others have mutant tubulin function (tub; these are temperature-sensitive alleles of TUB1 and TUB3).\n\n4366_S2 (tub sup)\n4365_S1 (tub SUP)\n4367_S3 (TUB sup)\n4368_S4 (TUB SUP)\n\n(yes, I know these appear numerically out of order; they’re in order with respect to genotype)\nWe’ll inspect the gt_types associated with each variant to ask for variant types that segregate in the expected pattern.\n\ngt_types is an array of 0,1,2,3 == HOM_REF, HET, UNKNOWN, HOM_ALT`\n\nSo [0, 0, 0, 0] means all four samples are HOM_REF (homozygous reference). So [0, 1, 0, 1] means samples 1 and 3 are HOM_REF and 2 and 4 are HET (heterozygous).\nFor variants that explain the tub phenotype, we want to filter for [1, 1, 0, 0] or [3, 3, 0, 0]\nFor variants that explain the sup phenotype, we want to filter for [1, 0, 1, 0] or [3, 3, 0, 0]\nN.B.: The genome of these cells is haploid. But I forgot to adjust the --ploidy parameter in freebayes (default is 2, diploid). So some of the sites have enough coverage to be called heterozygous, when really they should only be “homozygous” alternate.\nLet’s filter for variants that track with the tub phenotype.\nThat’s an excitingly small number! How else might we winnow these down? Hint: let’s filter for high confidence calls.\nWelp. That didn’t help much, and we don’t have many more ways with this file to winnow down the number further with the existing data"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#variant-annotation",
    "href": "content/block-dna/variant-calling/class-02.html#variant-annotation",
    "title": "Genome sequencing 2 - practice",
    "section": "Variant Annotation",
    "text": "Variant Annotation\nVARIANT ANNOTATION TO THE RESCUE.\nRemember from my lecture that one key step of variant identification is variant annotation.\nThis involves taking an initial VCF file and annotating each variant and annotating it with data from external sources.\nQuestion: What are some things you might want to know about each variant?\nThe variant annotation step is done using the snpEff software, which takes the unannotated VCF file and a set of external annotations and creates a new, annotated VCF.\nThat new file is in data/combined.annotated.vcf.gz.\nLet’s inspect the annotated VCF. How many variants are in the annotated VCF file? How many are in the unannotated file?\nLet’s find some variants that impact coding regions specifically.\nSuch annotations include whether it’s coding or non-coding, what impact it might have on an open reading frame, and even information about the predicted impact of specific mutations (LOW, MODERATE, HIGH impacts).\nHow does the software decide between LOW, MODERATE, and HIGH impacts? This is, in part, pre-calculated in the form of the BLOSUM matrix, which quantifies the log-likelihood of substitutions between resides in a multiple sequence alignments.\nQuestion: What drives the low likelihood of substitution between e.g., tryptophan (W) and aspartate (D)? Perhaps looking at amino acid side chain chemistries might help.\nYES! Now that is a small number of variants. What are they?\nBut that’s sort of ugly. Can we clean it up? Let’s inspect the contents of the ANN field.\nTry to print:\n\nthe chromosome\nthe position\nfields 2, 3, 4, 12, and 13 of the first ANN field (these are joined by commas)"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#variant-interpretation",
    "href": "content/block-dna/variant-calling/class-02.html#variant-interpretation",
    "title": "Genome sequencing 2 - practice",
    "section": "Variant interpretation",
    "text": "Variant interpretation\nLet’s try to interpret the impact of this variant.\n\nWhat’s the impact of the variant on the open reading frame?\nWhere does this mutation lie with respect to annotated protein domains? [Use yeastgenome.org]\nWhat’s the function of this gene? Has it been linked to tubulin function? [Use pubmed.com]"
  },
  {
    "objectID": "content/block-dna/variant-calling/class-02.html#a-thought-experiment-to-compare-the-utility-of-illumina-and-ont",
    "href": "content/block-dna/variant-calling/class-02.html#a-thought-experiment-to-compare-the-utility-of-illumina-and-ont",
    "title": "Genome sequencing 2 - practice",
    "section": "A thought experiment to compare the utility of Illumina and ONT",
    "text": "A thought experiment to compare the utility of Illumina and ONT\nImagine you have a locus structured like this:\n----A--B----Alu---&lt;&lt;&lt;&lt;----Alu----C--D------\nwhere A, B, C, D are genes, the &lt;&lt;&lt;&lt; is a directional promoter, and the Alu elements are representative repetitive elements.\nRecombination between the Alu elements could sometimes give you this:\n----A--B----Alu---&gt;&gt;&gt;&gt;----Alu----C--D------"
  },
  {
    "objectID": "content/bootcamp/r/exercises/exercises-01.html",
    "href": "content/bootcamp/r/exercises/exercises-01.html",
    "title": "Exercises-01",
    "section": "",
    "text": "Contact Info\nSuja Jagannathan sujatha.jagannathan@cuanschutz.edu\n\n\nOffice Hours\nUse https://calendly.com/molb7950 to schedule a time with a TA.\n\n\n\nLearning Objectives for the R Bootcamp\n\nFollow best coding practices (class 1)\nKnow the fundamentals of R programming (class 1)\nBecome familiar with “tidyverse” suite of packages\n\ntidyr: “Tidy” a messy dataset (class 2)\ndplyr: Transform data to derive new information (class 3)\nggplot2: Visualize and communicate results (class 4)\n\nPractice reproducible analysis using Rmarkdown (Rigor & Reproducibility) (classes 1-5)\n\n\n\nToday’s class outline - class 1\n\nCoding best practices\nReview R basics\n\nR vs Rstudio (Exercises #1-2)\nFunctions & Arguments (Exercises #3-4)\nData types (Exercise #5)\nData structures (Exercises #6-7)\nR Packages (Exercise #8)\n\nReview Rmarkdown (Exercise #9)\nRstudio cheatsheets (Exercise #10)\n\n\n\n\nCoding best practices\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.” — Hadley Wickham\n\n\n\nFile Names\n\nFile names should be meaningful and end in .R, .Rmd, etc.\nAvoid using special characters in file names - stick with numbers, letters, -, and _.\nNever include spaces in file names!\n\n    ###### Good  \n    fit_models.R  \n    utility_functions.Rmd  \n\n    ###### Bad  \n    fit models.R  \n    foo.r  \n    stuff.r  \n\nIf files should be run in a particular order, prefix them with numbers.\nIf it seems likely you’ll have more than 10 files, left pad with zero.\nIt looks nice (constant width) and sorts nicely.\n\n    00_download.R  \n    01_explore.R  \n    ...  \n    09_model.R  \n    10_visualize.R  \n\nAvoid capitalizing when not necessary.\nIf you want to include dates in your file name, use the ISO 8601 standard: YYYY-MM-DD\nUse delimiters intentionally! (helps you to recover metadata easily from file names)\nFor example, “_” to delimit fields; “-” to delimit words\n\n2019-02-15_class1_data-wrangling.Rmd\n\nAvoid hard coding file names and instead use relative paths.\n~ represents the current working directory.\nUse getwd() to figure out what your working directory is.\n\n###### Good\n\"~/class1/code/test.R\"\n    \n###### Bad\n\"/Users/sjaganna/Desktop/CU-onedrive/08-teaching/molb7910/class1/data.csv\"\n\n\n\nOrganisation\n\nTry to give a file a concise name that evokes its contents\nOne way to organize your files is by grouping them into data, code, plots, etc.\nFor example, in this class we often use the following structure:\n\n      exercises\n         - exercises-01.Rmd\n         - data\n         - img\n         - setup\n         ...\n\n\n\nInternal structure of code\nUse commented lines of - and = to break up your code chunk into easily readable segments. Or better yet, make each “action” it’s own chunk and give it a name.\n# Load data ---------------------------\n\n# Plot data ---------------------------\n\n\n\nQuestions?\n\n\n\n\nR Basics - Overview\n\nR, Rstudio (Exercise #1)\nR as a calculator (Exercise #2)\nFunctions and arguments (Exercises #3-4)\nData types: numeric, character, logical (& more) (Exercise #5)\nData structures: vector, list, matrix, data frame, tibbles (Exercises #6-7)\nPackage system, Rstudio, and Rmarkdown (Exercises #8-9)\n\n\n\nR vs Rstudio - Exercise 1\nWhat is R? What is Rstudio?\n\nR is a programming language used for statistical computing\nRStudio is an integrated development environment (IDE) for R. It includes a console, terminal, syntax-highlighting editor that supports direct code execution, tools for plotting, history, workspace management, and much more.\nYou can use R without RStudio, but not the other way around.\n\nLet’s do the following to explore Rstudio:\n\nLook at Rstudio panels one at a time\nEnvironment, History, Console, Terminal, Files, Plots, Packages, Help, etc.\n\n\n\nR as a calculator - Exercise 2\n\nR can function like an advanced calculator\ntry simple math\n\n\n2 + 3 * 5 # Note the order of operations.\n3 - 7 # value of 3-7\n3 / 2 # Division\n5^2 # 5 raised to the second power\n# This is a comment line\n\n\nassign a numeric value to an object\n\n\nnum &lt;- 5^2 # we just created an \"object\" num\n\n\nprint the object to check\n\n\nnum\n\n\ndo a computation on the object\n\n\nnum + 100\n\nNote: Objects can be over-written. So be careful if you reuse names.\n\n\n\nFunctions and arguments - Exercise 3\n\nFunctions are fundamental building blocks of R\nMost functions take one or more arguments and transform an input object in a specific way.\n\n\nlog\n?log\nlog(4)\nlog(4, base = 2)\n\n\n\n\nWriting a simple function - Exercise 4\n\naddtwo &lt;- function(x) {\n  num &lt;- x + 2\n  return(num)\n}\n\naddtwo(4)\n\n\nf &lt;- function(x, y) {\n  z &lt;- 3 * x + 4 * y\n  return(z)\n}\n\nf(2, 3)\n\n\n\n\nQuestions?\n\n\n\n\nData types\n\nThere are many data types in R.\nFor this class, the most commonly used ones are numeric, character, and logical.\nAll these data types can be used to create vectors natively.\n\n\n\nData types - Exercise 5\n\ntypeof(4) # numeric data time\n\n[1] \"double\"\n\ntypeof(\"suja\") # character data type\n\n[1] \"character\"\n\ntypeof(TRUE) # logical data type\n\n[1] \"logical\"\n\ntypeof(as.character(TRUE)) # coercing one data type to another\n\n[1] \"character\"\n\n\n\n\n\nData structures\n\nR has multiple data structures.\nMost of the time you will deal with tabular data sets, you will manipulate them, take sub-sections of them.\nIt is essential to know what are the common data structures in R and how they can be used.\nR deals with named data structures, this means you can give names to data structures and manipulate or operate on them using those names.\n\n\n\n\n\n\nSource: Devopedia\n\n\nTibbles\n\nA tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.\nTibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist).\nThis forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\ntibble() does much less than data.frame():\n\nit never changes the type of the inputs\nit never changes the names of variables\nit never creates row.names()\n\n\nSource: tibbles chapter in R for data science.\n\n\n\nVectors - Exercise 6\n\nVectors are one of the core R data structures.\nIt is basically a list of elements of the same type (numeric,character or logical).\nLater you will see that every column of a table will be represented as a vector.\nR handles vectors easily and intuitively.\nThe operations on vectors will propagate to all the elements of the vectors.\n\nCreate the following vectors\n\nx &lt;- c(1, 3, 2, 10, 5) # create a vector named x with 5 components\n# `c` is for combine\n# you could use '=' but I don't recommend it.\ny &lt;- 1:5 # create a vector of consecutive integers y\ny + 2 # scalar addition\n2 * y # scalar multiplication\ny^2 # raise each component to the second power\n2^y # raise 2 to the first through fifth power\ny # y itself has not been unchanged\ny &lt;- y * 2 # here, y is changed\n\n\n\n\nData frames - Exercise 7\n\nA data frame is more general than a matrix, in that different columns can have different modes (numeric, character, factor, etc.).\nA data frame can be constructed by data.frame() function.\nFor example, we illustrate how to construct a data frame from genomic intervals or coordinates.\n\nCreate a dataframe mydata\n\nchr &lt;- c(\"chr1\", \"chr1\", \"chr2\", \"chr2\")\nstrand &lt;- c(\"-\", \"-\", \"+\", \"+\")\nstart &lt;- c(200, 4000, 100, 400)\nend &lt;- c(250, 410, 200, 450)\n\nmydata.df &lt;- data.frame(chr, strand, start, end) # creating dataframe\nmydata.df\n\nmydata.tbl &lt;- tibble(chr, strand, start, end) # creating a tibble\nmydata.tbl\n\n\n\n\nR packages - Exercise 8\n\nAn R package is a collection of code, data, documentation, and tests that is easily sharable\nA package often has a collection of custom functions that enable you to carry out a workflow. eg. DESeq for RNA-seq analysis\nThe most popular places to get R packages from are CRAN, Bioconductor, and Github.\nOnce a package is installed, one still has to “load” them into the environment using a library(&lt;package&gt;) call.\n\nLet’s do the following to explore R packages * Look at the “Environment” panel in Rstudio * Explore Global Environment * Explore the contents of a package\n\n\n\nRmarkdown Exercise - Exercise 9\n\nRmarkdown is a fully reproducible authoring framework to create, collaborate, and communicate your work.\nRmarkdown supports a number of output formats including pdfs, word documents, slide shows, html, etc.\nAn Rmarkdown document is a plain text file with the extension .Rmd and contains the following basic components:\n\nAn (optional) YAML header surrounded by —s.\nChunks of R code surrounded by ```.\nText mixed with simple text formatting like # heading and italics.\n\n\nLet’s do the following to explore Rmarkdown documents * Create a new .Rmd document * knit the document to see the output\n\n\n\nHomework instructions\n\nToday’s homework is:\n\nTo go over everything we covered today and make sure you understand it. (Use office hours if you have questions) - Expected time spent: 30 min - 1 hour\nGo over Rstudio and Rmarkdown cheatsheets (Finding cheatsheets: Exercise 10) - Expected time spent: 30 min on each cheatsheet\n\n\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * Introduction to R by Altuna Akalin: http://compgenomr.github.io/book/introduction-to-r.html * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html\n\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nInstalling R: https://cran.r-project.org/\nInstalling RStudio: https://rstudio.com/products/rstudio/download/"
  },
  {
    "objectID": "content/bootcamp/r/exercises/exercises-02.html",
    "href": "content/bootcamp/r/exercises/exercises-02.html",
    "title": "Exercises-02",
    "section": "",
    "text": "Suja Jagannathan sujatha.jagannathan@cuanschutz.edu"
  },
  {
    "objectID": "content/bootcamp/r/exercises/exercises-02.html#regular-expressions",
    "href": "content/bootcamp/r/exercises/exercises-02.html#regular-expressions",
    "title": "Exercises-02",
    "section": "Regular expressions",
    "text": "Regular expressions\n\n\n\n\n\n Source: Rstudio cheatsheets\nUseful website: Regexr\nNote: stringr is an entire package focused on working with character strings. I highly recommend checking it out!\n\n\nProblem Set and Grading Rubric\n\nToday’s problem set assignment will allow you to practice the tidyr tools we learned in class today.\nThere is a total of 5 exercises, each with 4 points for a total of 20 points.\nLink to grading rubric.\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * Data Science with R by Garrett Grolemund: https://garrettgman.github.io/tidying/ * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nData Science with R by Garrett Grolemund https://garrettgman.github.io/tidying/"
  },
  {
    "objectID": "content/bootcamp/r/exercises/exercises-03.html",
    "href": "content/bootcamp/r/exercises/exercises-03.html",
    "title": "Exercises-03",
    "section": "",
    "text": "Contact Info\nSuja Jagannathan sujatha.jagannathan@cuanschutz.edu\n\n\nOffice Hours\nUse https://calendly.com/molb7950 to schedule a time with a TA.\n\n\n\nLearning Objectives for the R Bootcamp\n\nFollow best coding practices (class 1)\nKnow the fundamentals of R programming (class 1)\nBecome familiar with “tidyverse” suite of packages\n\ntidyr: “Tidy” a messy dataset (class 2)\ndplyr: Transform data to derive new information (class 3)\nggplot2: Visualize and communicate results (class 4)\n\nPractice reproducible analysis using Rmarkdown (Rigor & Reproducibility) (classes 1-5)\n\n\n\nToday’s class outline - class 3\n\nIntroduce dplyr & today’s datasets (Exercise 1)\nReview basic functions of dplyr\n\ncore dplyr verbs:\n\narrange (Exercise 2)\nfilter (Exercise 3)\nselect (Exercise 4)\nmutate (Exercise 5)\nsummarise (Exercise 6)\n\nmodify scope of verbs using: group_by (Exercise 7)\npipe: %&gt;% (Exercise 5)\nCombining tables with dplyr: join functions, binding columns/rows, etc. (Exercise 8)\n… and many others! rename, count, add_row, add_column, distinct, sample_n, sample_frac, slice, pull (Exercise 9)\n\nHomework instructions\n\n\n\n\ndplyr\n\ndplyr is a tidyverse package which provides a set of tools for efficiently manipulating datasets in R.\nImplemented in C++ and extremely fast even with large datasets.\nFollows the tidyverse grammar and philosophy; human-readable and intuitive\nDifferent dplyr verbs can be strung together using pipes %&gt;%\n\n\n\nToday’s datasets\n\nIn this class, we will use one of the datasets that come with the dplyr package to explore all the functions provided by dplyr.\n\ndplyr::starwars data frame contains data about 87 characters from Starwars\ndplyr::band_members, dplyr::band_instruments, dplyr::band_instruments2 describe band members and instruments of the Beatles and Rolling Stones\n\n\n\nGetting familiar with the data - Exercise 1\n\n# dplyr::starwars\nhead(starwars)\nstarwars\nglimpse(starwars)\nView(starwars)\n\n# dplyr::band_members\nband_members\nband_instruments\nband_instruments2\n\n\n\n\ndplyr package\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\narrange() changes the ordering of the rows.\nfilter() picks cases based on their values.\nselect() picks variables based on their names.\nmutate() adds new variables that are functions of existing variables\nsummarise() reduces multiple values down to a single summary.\n\n\nThese all combine naturally with group_by() which allows you to perform any operation &lt;80&gt;&lt;9c&gt;by group&lt;80&gt;&lt;9d&gt;.\nPipes %&gt;% allows different functions to be used together to create a workflow. x %&gt;% f(y) turns into f(x, y)\nSource: https://dplyr.tidyverse.org/\n\n\n\narrange - Syntax\n\narrange() to order rows by values of a column or columns (low to high). desc() orders high to low.\n\narrange(data = ..., &lt;colname&gt;)\n\n\narrange - Exercise 2\n\narrange(starwars, height) # default is to arrange in ascending order\narrange(starwars, desc(height)) # arrange in descending order\narrange(starwars, height, mass) # arrange by multiple columns\n\n\n\n\nfilter - Syntax\n\nfilter() chooses rows/cases where conditions are true.\n\nfilter(data = ..., &lt;condition&gt;)\n\n\nfilter - Exercise 3\n\nfilter(starwars, skin_color == \"light\")\nfilter(starwars, height &lt; 150)\nfilter(starwars, mass &gt; mean(mass, na.rm = TRUE))\n\nfilter(starwars, is.na(hair_color))\nView(filter(starwars, is.na(hair_color))) # if you want to look at the filtered data in a table without creating a new table\n\n\nMost frequently used comparison operators are: &gt;, &lt;, &gt;=, &lt;=, == (equal), != (not equal), is.na(), !is.na(), and %in% (contained in a list).\n\n\nfilter(starwars, skin_color %in% c(\"light\", \"fair\", \"pale\")) # using %in%\n\n# can also store as a named vector and use %in% with the vector\ncolor &lt;- c(\"light\", \"fair\", \"pale\")\nfilter(starwars, skin_color %in% color) # using %in%\n\n\nConditions can be combined using & (and), | (or).\n\n\nfilter(starwars, skin_color == \"light\" | eye_color == \"brown\") # using or\nfilter(starwars, skin_color == \"light\" & eye_color == \"brown\") # using and\n\n\n\n\nselect - Syntax\n\nselect extracts one or more columns from a table\nselect_if() to extract all columns of a particular type\n\nselect(data = ..., &lt;colname&gt;)  \nselect_if(data = ..., &lt;condition&gt;)\n\n\nselect - Exercise 4\n\nselect(starwars, hair_color)\nselect(starwars, -hair_color) # can also use not `!hair_color`\nselect(starwars, hair_color, skin_color, eye_color)\nselect(starwars, hair_color:eye_color) # select using x:y columns\nselect(starwars, !(hair_color:eye_color)) # reverse selection using the not `!` operator\nselect(starwars, ends_with(\"color\")) # select by specific conditions are met by column names: starts_with, ends_with, contains...\n\nselect_if(starwars, is.numeric) # select_if to return all columns with numeric values\n\n\n\n\nmutate - Syntax\n\nmutate() to compute new columns\n\nMutate has a LOT of variants. \n\n\n\n\n\n Source: Rstudio cheatsheets \nmutate(data = ..., &lt;newcolname&gt; =  funs(&lt;oldcolname&gt;))\nmutate(data = ..., &lt;colname&gt;, funs(x))\n\n# using pipes! %&gt;%\ndata %&gt;% mutate(&lt;colname&gt;, funs(x)) # this is useful when you need to use multiple functions to act sequentially on a dataframe. Input becomes obvious by leading with it. \n\n\nmutate (& pipe %&gt;%)- Exercise 5\n\n# create a new column to display height in meters\nstarwars %&gt;% mutate(height_m = height / 100) # using pipe to feed data into the function\n\n# using the pipe to feed data into multiple functions sequentially\nstarwars %&gt;%\n  mutate(height_m = height / 100) %&gt;% # this columns is always appended to the end of the table, by default\n  select(name, height_m, height, everything()) # using select to rearrange columns\n\n# mutate allows you to refer to columns that you&lt;e2&gt;&lt;80&gt;&lt;99&gt;ve just created\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(name, BMI, everything())\n\n# output needs to be saved into a new dataframe since dplyr does not \"change\" the original dataframe\nstarwars_bmi &lt;- starwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(name, BMI, everything())\n\n# using if_else clauses with mutate\nstarwars_height &lt;- starwars %&gt;%\n  mutate(tall_short = if_else(\n    condition = height &gt; 160,\n    true = \"tall\",\n    false = \"short\"\n  )) %&gt;%\n  select(name, tall_short, everything())\n\nWhy use pipes? So you don’t have to create too many intermediate files! However, note that intermediate files can be useful in the context of troubleshooting a pipeline.\n\n\nrowwise operations (if time permits)\n\ndplyr (& tidyverse in general) is very easy to perform vectorized operations column-wise, and not so easy with row-wise operations.\nthe function rowwise() offers a solution to this\nLet’s look at an example:\n\n\n# let's input data (same one used in yesterday's problem set)\ndata &lt;- read_csv(file = \"data/data_transcript_exp_subset.csv\")\n\n# calculate mean for each time point using mutate in a rowwise fashion!\ndata_mean &lt;- data %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    mean_0h = mean(c(rna_0h_rep1, rna_0h_rep2, rna_0h_rep3)),\n    mean_14h = mean(c(rna_14h_rep1, rna_14h_rep2, rna_14h_rep3))\n  )\n\nMore information here: https://dplyr.tidyverse.org/articles/rowwise.html\n\n\n\n\nQuestions?\n\n\n\n\nsummarise - Syntax\n\nsummarise() reduces multiple values down to a single summary.\n\n\n\n\n\n\n\n\nsummarise(data = ..., &lt;newcolname&gt; =  funs(&lt;oldcolname&gt;))\nsummarise_at(data = ..., vars(&lt;cols), funs())\n\n\n\nsummarise - Exercise 6\n\nstarwars %&gt;% summarise(mean_height = mean(height, na.rm = TRUE))\nstarwars %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE)\nstarwars %&gt;% summarise_at(vars(height:mass), mean, na.rm = TRUE)\n\n\nsummarise() on its own is not terribly useful. But it becomes powerful when paired with group_by().\n\n\n\n\ngroup_by - Syntax\n\ngroup_by reate a grouped copy of a table.\nThis changes the unit of analysis from the complete dataset to individual groups.\nThen, when you use the dplyr verbs on a grouped data frame they&lt;80&gt;&lt;99&gt;ll be automatically applied &lt;80&gt;&lt;9c&gt;by group&lt;80&gt;&lt;9d&gt;.\n\ngroup_by(data = ..., &lt;colname&gt;)\n\n\ngroup_by + summarize - Exercise 7\n\nstarwars %&gt;%\n  group_by(species)\n\nstarwars %&gt;%\n  group_by(species) %&gt;% # group by species\n  summarise(\n    height = mean(height, na.rm = TRUE)\n  )\n\nstarwars %&gt;%\n  group_by(species, gender) %&gt;% # group by multiple variables\n  summarise(\n    height = mean(height, na.rm = TRUE),\n    mass = mean(mass, na.rm = TRUE) # calculate multiple summaries\n  )\n\n\n\n\n\nQuestions?\n\n\n\n\nCombining tables\n\nCombine variables (i.e. columns)\n\nbind_cols()\nleft_join()\nright_join()\ninner_join()\nfull_join()\n\nCombine cases (i.e. rows)\n\nbind_rows()\nintersect()\nsetdiff()\nunion()\n\n\n\n\n\n\n\n Source: Rstudio cheatsheets\n\n\n\nCombine variables and cases - pasting tables together - Syntax\n\nbind_cols() to paste tables beside each other\nbind_rows() to paste tables below each other as they are.\n\nbind_cols(data1, data2) # should have same number of rows\nbind_rows(data1, data2) # should have same number of columns\n\n\nCombine variables - joining tables by a variable - Syntax\n\nleft_join() to join matching values from the right dataset to the left dataset\nright_join() to join matching values from the left dataset to the right dataset\ninner_join() to only join matching values present in both datasets\nfull_join() to join all values in both datasets\n\nleft_join(data1, data2)\nleft_join(data1, data2, by = ...) # if multiple columns match\nleft_join(data1, data2, by = c(\"col1\" = \"col2\")) # if the columns names that match are different\n\n\nCombine variables - joining tables by a variable - Exercise 8\n\nband_members %&gt;% left_join(band_instruments)\nband_members %&gt;% right_join(band_instruments)\nband_members %&gt;% inner_join(band_instruments)\nband_members %&gt;% full_join(band_instruments)\nband_members %&gt;% full_join(band_instruments2) # this won't work\nband_members %&gt;% full_join(band_instruments2, by = c(\"name\" = \"artist\"))\n\n\nJoining tables is something you will do a LOT in genomic analyses.\n\nThings to watch out for are to:\n\nexplicitly say which variable to join by\nmaking sure there aren’t subtle difference between what you think of as “common” variables\neg. Paul vs. Paul M; ENST79286869869 vs ENST79286869869.12\n\n\n\n\n\nOther dplyr verbs\nThere are many other dplyr verbs: rename, count, add_row, add_column, distinct, sample_n, sample_frac, slice, pull\nCheck out the dplyr cheatsheet to learn more!\n\n\nrename - Syntax\n\nrename() renames the variables in a table. Keeps all columns. select() can also do this, but with some caveats.\n\nrename(data = ..., &lt;colname&gt;)\n\n\nrename - Exercise 9\n\nband_instruments2\n\nrename(band_instruments2, name = artist)\n\nselect(band_instruments2, name = artist) # select drops the rest of the columns\nselect(band_instruments2, name = artist, everything()) # this fixes it, but one extra thing to remember\n\n\n\n\nProblem Set and Grading Rubric\n\nToday’s problem set assignment will allow you to practice the tidyr tools we learned in class today.\nThere is a total of 4 exercises, each with 5 points for a total of 20 points.\nGrading rubric listed at the beginning of the problem set.\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * dplyr vignette: https://dplyr.tidyverse.org/index.html * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nData Science with R by Garrett Grolemund https://garrettgman.github.io/tidying/"
  },
  {
    "objectID": "content/bootcamp/r/exercises/exercises-04.html",
    "href": "content/bootcamp/r/exercises/exercises-04.html",
    "title": "Exercises-04",
    "section": "",
    "text": "Contact Info\nSuja Jagannathan sujatha.jagannathan@cuanschutz.edu\n\n\nOffice Hours\nUse https://calendly.com/molb7950 to schedule a time with a TA.\n\n\n\nLearning Objectives for the R Bootcamp\n\nFollow best coding practices (class 1)\nKnow the fundamentals of R programming (class 1)\nBecome familiar with “tidyverse” suite of packages\n\ntidyr: “Tidy” a messy dataset (class 2)\ndplyr: Transform data to derive new information (class 3)\nggplot2: Visualize and communicate results (class 4)\n\nPractice reproducible analysis using Rmarkdown (Rigor & Reproducibility) (classes 1-5)\n\n\n\nToday’s class outline - class 4\n\nIntroduce ggplot2 & today’s datasets (Exercise 1)\nUnderstand the basics of ggplot2 (Exercise 2, 3)\nCreate more complex plots\n\nGeom functions (Exercise 4-8)\nGeom_point properties (Exercise 9)\nPosition adjustments (Exercise 10)\nCoordinate and Scale Functions (Exercise 11)\nZooming into a plot (Exercise 12)\nFaceting (Exercise 13)\nThemes (Exercise 14)\nLabels & Legends (Exercise 15)\n\nAdditional points\n\nAdding lines to plots (Exercise 16)\nMaking multi-panel figures (Exercise 17)\nSaving a plot (Exercise 18)\n\nHomework instructions\n\n\n\n\nggplot2\nggplot2 is based on the “grammar of graphics”, the idea that you can build every graph from the same components: a data set, a coordinate system, and geoms &lt;80&gt;&lt;94&gt; visual marks that represent data points.\n\n\nToday’s datasets\n\nIn this class, we will use one of the datasets that come with the ggplot2 package.\n\nggplot2::diamonds data frame contains data about 87 characters from Starwars\n\n\n\nGetting familiar with the data - Exercise 1\n\nhelp(\"diamonds\")\nhead(diamonds)\nglimpse(diamonds)\nView(diamonds)\n\n\n\n\nThe basic syntax of ggplot()\nggplot(): build plots piece by piece\nThe concept of ggplot divides a plot into three different fundamental parts:\nplot = data + coordinate-system + geometry.\n data: a data frame  coordinate-system: specify x and y variables  geometry: specify type of plots - histogram, boxplot, line, density, dotplot, bar, etc.\n\n\n\n\n\n\n\naesthetics can map variables in the data to visual properties of the geom (aesthetics) like size, color, and x and y locations to make the plot more information rich.\n\n\n\n\n\n\n\n\nMaking a plot step-by-step (Exercise 2)\n\n# initialize with data\nggplot(data = diamonds) # specify which dataframe to use - no plot yet\n\n#  specify the coordinate system\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) # data to map onto x and y axes\n\n# specify geometry\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point() # `geom` is 'geom_point()'\n######### Note: the position of `+` is important ##########\n\n# map aesthetics to other variables\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point()\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point(alpha = 0.2) # adjusting transparency of points\n\n\n\nLooking under the hood of ggplot (Exercise 3)\n\n# initialize with data\np1 &lt;- ggplot(data = diamonds) # specify which dataframe to use - no plot yet\n\n#  specify the coordinate system\np2 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) # data to map onto x and y axes\n\n# specify geometry\np3 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point() # `geom` is 'geom_boxplot()'\n\n# map aesthetics to other variables\np4 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point() # `geom` is 'geom_boxplot()'\n\np5 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point(alpha = 0.2) # `geom` is 'geom_boxplot()'\n\ntypeof(p1)\nnames(p1)\nhead(p1$data)\nsummary(p1)\nsummary(p2)\nsummary(p3)\nsummary(p4)\nsummary(p5)\n\n\n\nggplot is overkill for simple plots, but powerfully simple in making complex plots\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat)\n) +\n  geom_histogram() # histogram\n\n## why can't I just do this?\nhist(diamonds$carat)\n\nYou can. But the advantage of ggplot is that it is equally “simple” to make basic and complex plots. The underlying grammar lets you exquisitly customize the appearance of your plot and make publishable figures.\n\n\n\nQuestions?\n\n\n\nCreate more complex plots\n\n\n\n\n\n\n\n\nGeom function\n\nUse a geom function to represent data points, use the geom&lt;80&gt;&lt;99&gt;s aesthetic properties to represent variables.\nEach function returns a layer.\nThere are a LOT of geom functions in ggplot that are specific to plots with 1, 2, or 3 variables (i.e. if you really need to plot 3 variables - better to use aesthetics instead)\n\n\n\nGeom functions for one variable - Exercise 4\n\n\n\n\n\n\n\n# bar plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut)\n) +\n  geom_bar() # bar\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, fill = clarity)\n) +\n  geom_bar()\n\n# density plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat)\n) +\n  geom_density() # density\n\n# color the density plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat)\n) +\n  geom_density(fill = \"tomato1\")\n\n# plot subsets by mapping `fill` to `cut`\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, fill = cut)\n) +\n  geom_density(alpha = 0.8)\n\n# use ggridges to plot subsets in a staggered fashion!\nggplot(\n  data = diamonds,\n  mapping = aes(carat, y = cut, fill = cut)\n) +\n  geom_density_ridges() # function from the ggridges package, NOT ggplot2\n\n\n\n\nGeom functions for two variables\nWith two variables, depending on the nature of the data, you can have different kinds of geoms: - discrete x, continuous y - continuous x, continuous y - continuous bivariate - & others (check out the cheatsheet!)\n\n\ndiscrete x, continuous y - Exercise 5\n\n\n\n\n\n\n\n# column plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price)\n) +\n  geom_col()\n\noptions(scipen = 10000) # disables scientific notation - only have to type once for the whole Rmd\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price)\n) +\n  geom_col()\n\n# box plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price)\n) +\n  geom_boxplot()\n\n# box plot with fill color by cut\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price, fill = cut)\n) +\n  geom_boxplot()\n\n# violin plot with fill color by cut\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price, fill = cut)\n) +\n  geom_violin()\n\n\n\ncontinuous x, continuous y - Exercise 6\n\n\n\n\n\n\n\n# subset diamonds to see points better\ndiamonds_subset &lt;- diamonds %&gt;% sample_n(size = 1000)\n\n# scatter plot\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point()\n\n# geom_smooth\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_smooth()\n\n# combining geoms - 1\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point() +\n  geom_smooth()\n\n# combining geoms - 2\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point() +\n  geom_rug()\n\n\n\ncontinuous bivariate - Exercise 7\n\n\n\n\n\n\n\n# scatter plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point()\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_hex()\n\n\n\n\nGeom functions for three variables - Exercise 8\n\n\n\n\n\n\nOne example with geom_tile()\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = carat, fill = price)\n) +\n  geom_tile(colour = \"white\") +\n  scale_fill_gradientn(colors = c(\"red\", \"white\", \"blue\"))\n\n\n\n\nQuestions?\n\n\n\nshape, size, fill, color, and transparency - Exercise 9\nR has 25 built in shapes that are identified by numbers. There are some seeming duplicates: for example, 0, 15, and 22 are all squares. The difference comes from the interaction of the colour and fill aesthetics. The hollow shapes (0&lt;80&gt;&lt;93&gt;14) have a border determined by colour; the solid shapes (15&lt;80&gt;&lt;93&gt;18) are filled with colour; the filled shapes (21&lt;80&gt;&lt;93&gt;24) have a border of colour and are filled with fill.\n\n\n\n\n\n\n\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point(aes(fill = cut, size = depth),\n    alpha = 0.8,\n    shape = 24,\n    color = \"white\"\n  )\n\nNote that aesthetics can also be defined within geoms\n\n\nPosition adjustments - Exercise 10\nPosition adjustments determine how to arrange geoms that would otherwise occupy the same space.\n\n\n\n\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, fill = clarity)\n) +\n  geom_bar()\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, fill = clarity)\n) +\n  geom_bar(position = \"dodge\")\n\nJitter can be used to avoid over-plotting along with geom_point.\n\n\nCoordinate and Scale Functions - Exercise 11\nWe won’t go into these functions too much today, but here is a brief overview:\n\nThe coordinate system determines how the x and y aesthetics combine to position elements in the plot. The default coordinate system is Cartesian ( coord_cartesian() ), which can be tweaked with coord_map() , coord_fixed() , coord_flip() , and coord_trans() , or completely replaced with coord_polar()\nScales control the details of how data values are translated to visual properties. There are 20+ scale functions. We will look at one; the ggplot2 cheatsheet is your friend for the rest.\n\n\n# coord transform\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  coord_trans(x = \"log10\")\n\n# coord_flip\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut)\n) +\n  geom_bar() # bar\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut)\n) +\n  geom_bar() +\n  coord_flip()\n\n# scales\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  scale_x_log10()\n\nBrief aside: ggplot can also handle on-the-fly data transformations like below.\n\n# log transformed carat and USD converted to CAD\nggplot(diamonds, aes(log10(carat), price * 1.32)) +\n  geom_point()\n\n\n\nZooming into a plot - Exercise 12\nOne might often want to change the limits of x or y axes to zoom in. There are multiple ways to do this.\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  coord_cartesian(xlim = c(0, 2), ylim = c(0, 5000))\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  xlim(0, 2) +\n  ylim(0, 5000)\n\n\n\n\nQuestions?\n\n\n\nFaceting to plot subsets of data into separate panels - Exercise 13\nFacets divide a plot into subplots based on the values of one or more discrete variables.\n\n\n\n\n\n\n\n# density plot for data subsets\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, fill = cut)\n) +\n  geom_density(alpha = 0.8)\n\n# density plot with facets\nggplot(\n  data = diamonds,\n  mapping = aes(x = log(price), fill = cut)\n) +\n  geom_density(color = \"black\") +\n  facet_wrap(~cut, nrow = 1)\n\n# scatter plot with facets\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = .05) +\n  facet_wrap(~cut, nrow = 1)\n\n\n\n\nThemes - Exercise 14\nThemes can significantly affect the appearance of your plot. Thanksfully, there are a lot to choose from. \n\n\n\n\n\n\n# default theme\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8)\n\n# theme black & white\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_bw()\n\n# theme few\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_few()\n\n# theme wsj\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_wsj()\n\n# theme economist\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_economist()\n\nYou can also customize pre-existing themes\n\nmytheme &lt;- theme_minimal(base_size = 15) +\n  theme(\n    aspect.ratio = 1,\n    panel.background =\n      element_rect(\n        colour = \"black\",\n        size = 1\n      )\n  )\n\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  mytheme # notice the lack of parantheses, because this is an object, not a function\n\n\n\nLabels & Legends - Exercise 15\n\n\n\n\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = log(price), fill = cut)\n) +\n  geom_boxplot() +\n  labs(\n    y = \"Price (log scale)\", x = \"Cut\", color = \"Cut\",\n    title = \"Distribution of diamond prices by cut\",\n    subtitle = \"Data come from a random sample of 1000 diamonds\",\n    caption = \"Source: diamonds dataset from ggplot2\"\n  ) +\n  annotate(geom = \"text\", x = 1, y = 5, label = \"Random text\") +\n  theme_bw()\n\n\n\n\nQuestions?\n\n\n\nAdditional points\n\n\nHow to add a line to a plot? (Exercise 16)\n\np &lt;- ggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_few()\n\np + geom_line()\np + geom_hline(aes(yintercept = 5000))\np + geom_vline(aes(xintercept = 2))\np + geom_smooth(method = lm)\np + geom_abline(aes(intercept = 0.5, slope = 5000))\n\n\n\nHow to combine multiple plots into a figure? (Exercise 17)\n\nplot1 &lt;- p\nplot2 &lt;- p + geom_hline(aes(yintercept = 5000))\nplot3 &lt;- p + geom_vline(aes(xintercept = 2))\nplot4 &lt;- p + geom_abline(aes(intercept = 0.5, slope = 5000))\n\nall_plots &lt;- plot_grid(plot1, plot2, plot3, plot4, labels = c(\"A\", \"B\", \"C\", \"D\"), nrow = 2)\nall_plots\n\n# we have 4 legends, which is too many - can they be removed?\n# Yes, but it is not exactly straightforward\nlegend &lt;- get_legend(plot1 + theme(legend.position = \"bottom\"))\nplot1 &lt;- p + theme(legend.position = \"none\")\nplot2 &lt;- p + geom_hline(aes(yintercept = 5000)) + theme(legend.position = \"none\")\nplot3 &lt;- p + geom_vline(aes(xintercept = 2)) + theme(legend.position = \"none\")\nplot4 &lt;- p + geom_abline(aes(intercept = 0.5, slope = 5000)) + theme(legend.position = \"none\")\n\nall_plots &lt;- plot_grid(plot1, plot2, plot3, plot4, labels = c(\"A\", \"B\", \"C\", \"D\"), nrow = 2)\nplot_final &lt;- plot_grid(all_plots, legend, ncol = 1, rel_heights = c(1, .1))\nplot_final\n\nMore information on using plot_grid (from package cowplot) is here\n\n\n\nSaving plots (Exercise 18)\n\nggsave(\"img/plot_final.png\", width = 5, height = 5)\n# Saves last plot as 5&lt;e2&gt;&lt;80&gt;&lt;99&gt; x 5&lt;e2&gt;&lt;80&gt;&lt;99&gt; file named \"plot_final.png\" in working directory. Matches file type to file extension\n\n\n\n\nThe Final Problem Set and Grading Rubric\n\nThe final problem set assignment will be posted by 4pm today (i.e. not at noon)\nIt is due Monday, August 31, by noon\nYou can work on the assignment in class tomorrow - which will be used for review and answering questions\nGrading rubric will be listed at the beginning of the problem set.\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html * TheRBootcamp: https://therbootcamp.github.io/BaselRBootcamp_2018April/_sessions/D3S2_PlottingI/PlottingI_practical_answers.html\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nData Science with R by Garrett Grolemund https://garrettgman.github.io/tidying/\nggplot2 reference: https://ggplot2.tidyverse.org/reference/\nggthemes: https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/"
  },
  {
    "objectID": "content/bootcamp/r/exercises/exercises-05.html",
    "href": "content/bootcamp/r/exercises/exercises-05.html",
    "title": "Exercises-05",
    "section": "",
    "text": "Exercises\n\nExercise 1\n\n\nExercise 2\n\n\nExercise 3\n\n\nExercise 4\n\n\nExercise 5"
  },
  {
    "objectID": "content/bootcamp/r/class-01.html",
    "href": "content/bootcamp/r/class-01.html",
    "title": "class-01",
    "section": "",
    "text": "Learning Objectives for the R Bootcamp\n\nFollow best coding practices (class 1)\nKnow the fundamentals of R programming (class 1)\nBecome familiar with “tidyverse” suite of packages\n\ntidyr: “Tidy” a messy dataset (class 2)\ndplyr: Transform data to derive new information (class 3)\nggplot2: Visualize and communicate results (class 4)\n\nPractice reproducible analysis using Rmarkdown (Rigor & Reproducibility) (classes 1-5)\n\n\n\nToday’s class outline - class 1\n\nCoding best practices\nReview R basics\n\nR vs Rstudio (Exercises #1-2)\nFunctions & Arguments (Exercises #3-4)\nData types (Exercise #5)\nData structures (Exercises #6-7)\nR Packages (Exercise #8)\n\nReview Rmarkdown (Exercise #9)\nRstudio cheatsheets (Exercise #10)\n\n\n\nCoding best practices\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.” — Hadley Wickham\n\n\n\nFile Names\n\nFile names should be meaningful and end in .R, .Rmd, etc.\nAvoid using special characters in file names - stick with numbers, letters, -, and _.\nNever include spaces in file names!\n\n###### Good  \nfit_models.R  \nutility_functions.Rmd\n\n###### Bad  \nfit models.R  \ntmp.r  \nstuff.r  \n\nIf files should be run in a particular order, prefix them with numbers.\nIf it seems likely you’ll have more than 10 files, left pad with zero.\nIt looks nice (constant width) and sorts nicely.\n\n    00_download.R  \n    01_explore.R  \n    ...  \n    09_model.R  \n    10_visualize.R  \n\nAvoid capitalizing when not necessary.\nIf you want to include dates in your file name, use the ISO 8601 standard: YYYY-MM-DD\nUse delimiters intentionally! (helps you to recover metadata easily from file names)\nFor example, “_” to delimit fields; “-” to delimit words\n\n2019-02-15_class1_data-wrangling.Rmd\n\nAvoid hard coding file names and instead use relative paths.\n~ represents the current working directory.\nUse getwd() to figure out what your working directory is.\n\n###### Good\n\"~/class1/code/test.R\"\n    \n###### Bad\n\"/Users/sjaganna/Desktop/CU-onedrive/08-teaching/molb7910/class1/data.csv\"\n\n\nOrganisation\n\nTry to give a file a concise name that evokes its contents\nOne way to organize your files is by grouping them into data, code, plots, etc.\nFor example, in this class we often use the following structure:\n\n      exercises\n         - exercises-01.Rmd\n         - data\n         - img\n         - setup\n         ...\n\n\nInternal structure of code\nUse commented lines of - and = to break up your code chunk into easily readable segments. Or better yet, make each “action” it’s own chunk and give it a name.\n# Load data ---------------------------\n\n# Plot data ---------------------------\n\n\nR Basics - Overview\n\nR, Rstudio (Exercise #1)\nR as a calculator (Exercise #2)\nFunctions and arguments (Exercises #3-4)\nData types: numeric, character, logical (& more) (Exercise #5)\nData structures: vector, list, matrix, data frame, tibbles (Exercises #6-7)\nPackage system, Rstudio, and Rmarkdown (Exercises #8-9)\n\n\n\nR vs Rstudio - Exercise 1\nWhat is R? What is Rstudio?\n\nR is a programming language used for statistical computing\nRStudio is an integrated development environment (IDE) for R. It includes a console, terminal, syntax-highlighting editor that supports direct code execution, tools for plotting, history, workspace management, and much more.\nYou can use R without RStudio, but not the other way around.\n\nLet’s do the following to explore Rstudio:\n\nLook at Rstudio panels one at a time\nEnvironment, History, Console, Terminal, Files, Plots, Packages, Help, etc.\n\n\n\nR as a calculator - Exercise 2\n\nR can function like an advanced calculator\ntry simple math\n\n\n2 + 3 * 5 # Note the order of operations.\n\n[1] 17\n\n3 - 7 # value of 3-7\n\n[1] -4\n\n3 / 2 # Division\n\n[1] 1.5\n\n5^2 # 5 raised to the second power\n\n[1] 25\n\n# This is a comment line\n\n\nassign a numeric value to an object\n\n\nnum &lt;- 5^2 # we just created an \"object\" num\n\n\nprint the object to check\n\n\nnum\n\n[1] 25\n\n\n\ndo a computation on the object\n\n\nnum + 100\n\n[1] 125\n\n\nNote: Objects can be over-written. So be careful if you reuse names.\n\n\nFunctions and arguments - Exercise 3\n\nFunctions are fundamental building blocks of R\nMost functions take one or more arguments and transform an input object in a specific way.\nTab completion is your friend!\n\n\nlog\n\nfunction (x, base = exp(1))  .Primitive(\"log\")\n\n?log\nlog(4)\n\n[1] 1.386294\n\nlog(4, base = 2)\n\n[1] 2\n\n\n\n\nWriting a simple function - Exercise 4\n\naddtwo &lt;- function(x) {\n  num &lt;- x + 2\n  return(num)\n}\n\naddtwo(4)\n\n[1] 6\n\n\n\nf &lt;- function(x, y) {\n  z &lt;- 3 * x + 4 * y\n  return(z)\n}\n\nf(2, 3)\n\n[1] 18\n\n\n\n\nData types\n\nThere are many data types in R.\nFor this class, the most commonly used ones are numeric, character, and logical.\nAll these data types can be used to create vectors natively.\n\n\n\nData types - Exercise 5\n\ntypeof(4) # numeric data time\n\n[1] \"double\"\n\ntypeof(\"suja\") # character data type\n\n[1] \"character\"\n\ntypeof(TRUE) # logical data type\n\n[1] \"logical\"\n\ntypeof(as.character(TRUE)) # coercing one data type to another\n\n[1] \"character\"\n\n\n\n\nData structures\n\nR has multiple data structures.\nMost of the time you will deal with tabular data sets, you will manipulate them, take sub-sections of them.\nIt is essential to know what are the common data structures in R and how they can be used.\nR deals with named data structures, this means you can give names to data structures and manipulate or operate on them using those names.\n\n\n\n\n\n\nSource: Devopedia\n\n\nTibbles\n\nA tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.\nTibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist).\nThis forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\ntibble() does much less than data.frame():\n\nit never changes the type of the inputs\nit never changes the names of variables\nit never creates row.names()\n\n\nSource: tibbles chapter in R for data science.\n\n\nVectors - Exercise 6\n\nVectors are one of the core R data structures.\nIt is basically a list of elements of the same type (numeric,character or logical).\nLater you will see that every column of a table will be represented as a vector.\nR handles vectors easily and intuitively.\nThe operations on vectors will propagate to all the elements of the vectors.\n\nCreate the following vectors\n\nx &lt;- c(1, 3, 2, 10, 5) # create a vector named x with 5 components\n# `c` is for combine\n# you could use '=' but I don't recommend it.\ny &lt;- 1:5 # create a vector of consecutive integers y\ny + 2 # scalar addition\n\n[1] 3 4 5 6 7\n\n2 * y # scalar multiplication\n\n[1]  2  4  6  8 10\n\ny^2 # raise each component to the second power\n\n[1]  1  4  9 16 25\n\n2^y # raise 2 to the first through fifth power\n\n[1]  2  4  8 16 32\n\ny # y itself has not been unchanged\n\n[1] 1 2 3 4 5\n\ny &lt;- y * 2 # here, y is changed\n\n\n\nData frames - Exercise 7\n\nA data frame is more general than a matrix, in that different columns can have different modes (numeric, character, factor, etc.).\nA data frame can be constructed by data.frame() function.\nFor example, we illustrate how to construct a data frame from genomic intervals or coordinates.\n\nCreate a dataframe mydata\n\nchr &lt;- c(\"chr1\", \"chr1\", \"chr2\", \"chr2\")\nstrand &lt;- c(\"-\", \"-\", \"+\", \"+\")\nstart &lt;- c(200, 4000, 100, 400)\nend &lt;- c(250, 410, 200, 450)\n\nmydata.df &lt;- data.frame(chr, strand, start, end) # creating dataframe\nmydata.df\n\n   chr strand start end\n1 chr1      -   200 250\n2 chr1      -  4000 410\n3 chr2      +   100 200\n4 chr2      +   400 450\n\nmydata.tbl &lt;- tibble(chr, strand, start, end) # creating a tibble\nmydata.tbl\n\n# A tibble: 4 x 4\n  chr   strand start   end\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 chr1  -        200   250\n2 chr1  -       4000   410\n3 chr2  +        100   200\n4 chr2  +        400   450\n\n\n\n\nR packages - Exercise 8\n\nAn R package is a collection of code, data, documentation, and tests that is easily sharable\nA package often has a collection of custom functions that enable you to carry out a workflow. eg. DESeq for RNA-seq analysis\nThe most popular places to get R packages from are CRAN, Bioconductor, and Github.\nOnce a package is installed, one still has to “load” them into the environment using a library(&lt;package&gt;) call.\n\nLet’s do the following to explore R packages * Look at the “Environment” panel in Rstudio * Explore Global Environment * Explore the contents of a package\n\n\nRmarkdown Exercise - Exercise 9\n\nRmarkdown is a fully reproducible authoring framework to create, collaborate, and communicate your work.\nRmarkdown supports a number of output formats including pdfs, word documents, slide shows, html, etc.\nAn Rmarkdown document is a plain text file with the extension .Rmd and contains the following basic components:\n\nAn (optional) YAML header surrounded by —s.\nChunks of R code surrounded by ```.\nText mixed with simple text formatting like # heading and italics.\n\n\nLet’s do the following to explore Rmarkdown documents * Create a new .Rmd document * knit the document to see the output\n\n\nHomework instructions\n\nToday’s homework is:\n\nTo go over everything we covered today and make sure you understand it. (Use office hours if you have questions) - Expected time spent: 30 min - 1 hour\nGo over Rstudio and Rmarkdown cheatsheets (Finding cheatsheets: Exercise 10) - - Expected time spent: 30 min on each cheatsheet\n\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * Introduction to R by Altuna Akalin: http://compgenomr.github.io/book/introduction-to-r.html * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nInstalling R: https://cran.r-project.org/\nInstalling RStudio: https://rstudio.com/products/rstudio/download/"
  },
  {
    "objectID": "content/bootcamp/r/class-03.html",
    "href": "content/bootcamp/r/class-03.html",
    "title": "Exercises-03",
    "section": "",
    "text": "Suja Jagannathan sujatha.jagannathan@cuanschutz.edu"
  },
  {
    "objectID": "content/bootcamp/r/class-03.html#dplyr",
    "href": "content/bootcamp/r/class-03.html#dplyr",
    "title": "Exercises-03",
    "section": "dplyr",
    "text": "dplyr\n\ndplyr is a tidyverse package which provides a set of tools for efficiently manipulating datasets in R.\nImplemented in C++ and extremely fast even with large datasets.\nFollows the tidyverse grammar and philosophy; human-readable and intuitive\nDifferent dplyr verbs can be strung together using pipes %&gt;%\n\n\nToday’s datasets\n\nIn this class, we will use one of the datasets that come with the dplyr package to explore all the functions provided by dplyr.\n\ndplyr::starwars data frame contains data about 87 characters from Starwars\ndplyr::band_members, dplyr::band_instruments, dplyr::band_instruments2 describe band members and instruments of the Beatles and Rolling Stones\n\n\n\nGetting familiar with the data - Exercise 1\n\n# dplyr::starwars\nhead(starwars)\n\n# A tibble: 6 x 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky~    172    77 blond      fair       blue            19   male  mascu~\n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu~\n3 R2-D2         96    32 &lt;NA&gt;       white, bl~ red             33   none  mascu~\n4 Darth Va~    202   136 none       white      yellow          41.9 male  mascu~\n5 Leia Org~    150    49 brown      light      brown           19   fema~ femin~\n6 Owen Lars    178   120 brown, gr~ light      blue            52   male  mascu~\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars\n\n# A tibble: 87 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk~    172    77 blond      fair       blue            19   male  mascu~\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu~\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl~ red             33   none  mascu~\n 4 Darth V~    202   136 none       white      yellow          41.9 male  mascu~\n 5 Leia Or~    150    49 brown      light      brown           19   fema~ femin~\n 6 Owen La~    178   120 brown, gr~ light      blue            52   male  mascu~\n 7 Beru Wh~    165    75 brown      light      blue            47   fema~ femin~\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu~\n 9 Biggs D~    183    84 black      light      brown           24   male  mascu~\n10 Obi-Wan~    182    77 auburn, w~ fair       blue-gray       57   male  mascu~\n# i 77 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or~\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2~\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.~\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N~\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"~\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",~\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, ~\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",~\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini~\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T~\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma~\n$ films      &lt;list&gt; &lt;\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return~\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp~\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",~\n\n# View(starwars)\n\n# dplyr::band_members\nband_members\n\n# A tibble: 3 x 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\nband_instruments\n\n# A tibble: 3 x 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\nband_instruments2\n\n# A tibble: 3 x 2\n  artist plays \n  &lt;chr&gt;  &lt;chr&gt; \n1 John   guitar\n2 Paul   bass  \n3 Keith  guitar\n\n\n\n\ndplyr package\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\narrange() changes the ordering of the rows.\nfilter() picks cases based on their values.\nselect() picks variables based on their names.\nmutate() adds new variables that are functions of existing variables\nsummarise() reduces multiple values down to a single summary.\n\n\nThese all combine naturally with group_by() which allows you to perform any operation &lt;80&gt;&lt;9c&gt;by group&lt;80&gt;&lt;9d&gt;.\nPipes %&gt;% allows different functions to be used together to create a workflow. x %&gt;% f(y) turns into f(x, y)\nSource: https://dplyr.tidyverse.org/\n\n\narrange - Syntax\n\narrange() to order rows by values of a column or columns (low to high). desc() orders high to low.\n\narrange(data = ..., &lt;colname&gt;)\n\n\narrange - Exercise 2\n\narrange(starwars, height) # default is to arrange in ascending order\n\n# A tibble: 87 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Yoda         66    17 white      green      brown            896 male  mascu~\n 2 Ratts T~     79    15 none       grey, blue unknown           NA male  mascu~\n 3 Wicket ~     88    20 brown      brown      brown              8 male  mascu~\n 4 Dud Bolt     94    45 none       blue, grey yellow            NA male  mascu~\n 5 R2-D2        96    32 &lt;NA&gt;       white, bl~ red               33 none  mascu~\n 6 R4-P17       96    NA none       silver, r~ red, blue         NA none  femin~\n 7 R5-D4        97    32 &lt;NA&gt;       white, red red               NA none  mascu~\n 8 Sebulba     112    40 none       grey, red  orange            NA male  mascu~\n 9 Gasgano     122    NA none       white, bl~ black             NA male  mascu~\n10 Watto       137    NA black      blue, grey yellow            NA male  mascu~\n# i 77 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\narrange(starwars, desc(height)) # arrange in descending order\n\n# A tibble: 87 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Yarael ~    264    NA none       white      yellow          NA   male  mascu~\n 2 Tarfful     234   136 brown      brown      blue            NA   male  mascu~\n 3 Lama Su     229    88 none       grey       black           NA   male  mascu~\n 4 Chewbac~    228   112 brown      unknown    blue           200   male  mascu~\n 5 Roos Ta~    224    82 none       grey       orange          NA   male  mascu~\n 6 Grievous    216   159 none       brown, wh~ green, y~       NA   male  mascu~\n 7 Taun We     213    NA none       grey       black           NA   fema~ femin~\n 8 Rugor N~    206    NA none       green      orange          NA   male  mascu~\n 9 Tion Me~    206    80 none       grey       black           NA   male  mascu~\n10 Darth V~    202   136 none       white      yellow          41.9 male  mascu~\n# i 77 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\narrange(starwars, height, mass) # arrange by multiple columns\n\n# A tibble: 87 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Yoda         66    17 white      green      brown            896 male  mascu~\n 2 Ratts T~     79    15 none       grey, blue unknown           NA male  mascu~\n 3 Wicket ~     88    20 brown      brown      brown              8 male  mascu~\n 4 Dud Bolt     94    45 none       blue, grey yellow            NA male  mascu~\n 5 R2-D2        96    32 &lt;NA&gt;       white, bl~ red               33 none  mascu~\n 6 R4-P17       96    NA none       silver, r~ red, blue         NA none  femin~\n 7 R5-D4        97    32 &lt;NA&gt;       white, red red               NA none  mascu~\n 8 Sebulba     112    40 none       grey, red  orange            NA male  mascu~\n 9 Gasgano     122    NA none       white, bl~ black             NA male  mascu~\n10 Watto       137    NA black      blue, grey yellow            NA male  mascu~\n# i 77 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\n\nfilter - Syntax\n\nfilter() chooses rows/cases where conditions are true.\n\nfilter(data = ..., &lt;condition&gt;)\n\n\nfilter - Exercise 3\n\nfilter(starwars, skin_color == \"light\")\n\n# A tibble: 11 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 \"Leia O~    150    49 brown      light      brown             19 fema~ femin~\n 2 \"Owen L~    178   120 brown, gr~ light      blue              52 male  mascu~\n 3 \"Beru W~    165    75 brown      light      blue              47 fema~ femin~\n 4 \"Biggs ~    183    84 black      light      brown             24 male  mascu~\n 5 \"Lobot\"     175    79 none       light      blue              37 male  mascu~\n 6 \"Cord\\u~    157    NA brown      light      brown             NA fema~ femin~\n 7 \"Dorm\\u~    165    NA brown      light      brown             NA fema~ femin~\n 8 \"Raymus~    188    79 brown      light      brown             NA male  mascu~\n 9 \"Rey\"        NA    NA brown      light      hazel             NA fema~ femin~\n10 \"Poe Da~     NA    NA brown      light      brown             NA male  mascu~\n11 \"Padm\\u~    165    45 brown      light      brown             46 fema~ femin~\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nfilter(starwars, height &lt; 150)\n\n# A tibble: 10 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 R2-D2        96    32 &lt;NA&gt;       white, bl~ red               33 none  mascu~\n 2 R5-D4        97    32 &lt;NA&gt;       white, red red               NA none  mascu~\n 3 Yoda         66    17 white      green      brown            896 male  mascu~\n 4 Wicket ~     88    20 brown      brown      brown              8 male  mascu~\n 5 Watto       137    NA black      blue, grey yellow            NA male  mascu~\n 6 Sebulba     112    40 none       grey, red  orange            NA male  mascu~\n 7 Dud Bolt     94    45 none       blue, grey yellow            NA male  mascu~\n 8 Gasgano     122    NA none       white, bl~ black             NA male  mascu~\n 9 Ratts T~     79    15 none       grey, blue unknown           NA male  mascu~\n10 R4-P17       96    NA none       silver, r~ red, blue         NA none  femin~\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nfilter(starwars, mass &gt; mean(mass, na.rm = TRUE))\n\n# A tibble: 10 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Darth V~    202   136 none       white      yellow          41.9 male  mascu~\n 2 Owen La~    178   120 brown, gr~ light      blue            52   male  mascu~\n 3 Chewbac~    228   112 brown      unknown    blue           200   male  mascu~\n 4 Jabba D~    175  1358 &lt;NA&gt;       green-tan~ orange         600   herm~ mascu~\n 5 Jek Ton~    180   110 brown      fair       blue            NA   male  mascu~\n 6 IG-88       200   140 none       metal      red             15   none  mascu~\n 7 Bossk       190   113 none       green      red             53   male  mascu~\n 8 Dexter ~    198   102 none       brown      yellow          NA   male  mascu~\n 9 Grievous    216   159 none       brown, wh~ green, y~       NA   male  mascu~\n10 Tarfful     234   136 brown      brown      blue            NA   male  mascu~\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nfilter(starwars, is.na(hair_color))\n\n# A tibble: 5 x 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 C-3PO        167    75 &lt;NA&gt;       gold       yellow           112 none  mascu~\n2 R2-D2         96    32 &lt;NA&gt;       white, bl~ red               33 none  mascu~\n3 R5-D4         97    32 &lt;NA&gt;       white, red red               NA none  mascu~\n4 Greedo       173    74 &lt;NA&gt;       green      black             44 male  mascu~\n5 Jabba De~    175  1358 &lt;NA&gt;       green-tan~ orange           600 herm~ mascu~\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# View(filter(starwars, is.na(hair_color))) # if you want to look at the filtered data in a table without creating a new table\n\n\nMost frequently used comparison operators are: &gt;, &lt;, &gt;=, &lt;=, == (equal), != (not equal), is.na(), !is.na(), and %in% (contained in a list).\n\n\nfilter(starwars, skin_color %in% c(\"light\", \"fair\", \"pale\")) # using %in%\n\n# A tibble: 33 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk~    172    77 blond      fair       blue            19   male  mascu~\n 2 Leia Or~    150    49 brown      light      brown           19   fema~ femin~\n 3 Owen La~    178   120 brown, gr~ light      blue            52   male  mascu~\n 4 Beru Wh~    165    75 brown      light      blue            47   fema~ femin~\n 5 Biggs D~    183    84 black      light      brown           24   male  mascu~\n 6 Obi-Wan~    182    77 auburn, w~ fair       blue-gray       57   male  mascu~\n 7 Anakin ~    188    84 blond      fair       blue            41.9 male  mascu~\n 8 Wilhuff~    180    NA auburn, g~ fair       blue            64   male  mascu~\n 9 Han Solo    180    80 brown      fair       brown           29   male  mascu~\n10 Wedge A~    170    77 brown      fair       hazel           21   male  mascu~\n# i 23 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# can also store as a named vector and use %in% with the vector\ncolor &lt;- c(\"light\", \"fair\", \"pale\")\nfilter(starwars, skin_color %in% color) # using %in%\n\n# A tibble: 33 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk~    172    77 blond      fair       blue            19   male  mascu~\n 2 Leia Or~    150    49 brown      light      brown           19   fema~ femin~\n 3 Owen La~    178   120 brown, gr~ light      blue            52   male  mascu~\n 4 Beru Wh~    165    75 brown      light      blue            47   fema~ femin~\n 5 Biggs D~    183    84 black      light      brown           24   male  mascu~\n 6 Obi-Wan~    182    77 auburn, w~ fair       blue-gray       57   male  mascu~\n 7 Anakin ~    188    84 blond      fair       blue            41.9 male  mascu~\n 8 Wilhuff~    180    NA auburn, g~ fair       blue            64   male  mascu~\n 9 Han Solo    180    80 brown      fair       brown           29   male  mascu~\n10 Wedge A~    170    77 brown      fair       hazel           21   male  mascu~\n# i 23 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\nConditions can be combined using & (and), | (or).\n\n\nfilter(starwars, skin_color == \"light\" | eye_color == \"brown\") # using or\n\n# A tibble: 25 x 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Leia Or~    150  49   brown      light      brown           19   fema~ femin~\n 2 Owen La~    178 120   brown, gr~ light      blue            52   male  mascu~\n 3 Beru Wh~    165  75   brown      light      blue            47   fema~ femin~\n 4 Biggs D~    183  84   black      light      brown           24   male  mascu~\n 5 Han Solo    180  80   brown      fair       brown           29   male  mascu~\n 6 Yoda         66  17   white      green      brown          896   male  mascu~\n 7 Boba Fe~    183  78.2 black      fair       brown           31.5 male  mascu~\n 8 Lando C~    177  79   black      dark       brown           31   male  mascu~\n 9 Lobot       175  79   none       light      blue            37   male  mascu~\n10 Arvel C~     NA  NA   brown      fair       brown           NA   male  mascu~\n# i 15 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nfilter(starwars, skin_color == \"light\" & eye_color == \"brown\") # using and\n\n# A tibble: 7 x 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 \"Leia Or~    150    49 brown      light      brown             19 fema~ femin~\n2 \"Biggs D~    183    84 black      light      brown             24 male  mascu~\n3 \"Cord\\u0~    157    NA brown      light      brown             NA fema~ femin~\n4 \"Dorm\\u0~    165    NA brown      light      brown             NA fema~ femin~\n5 \"Raymus ~    188    79 brown      light      brown             NA male  mascu~\n6 \"Poe Dam~     NA    NA brown      light      brown             NA male  mascu~\n7 \"Padm\\u0~    165    45 brown      light      brown             46 fema~ femin~\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\n\nselect - Syntax\n\nselect extracts one or more columns from a table\nselect_if() to extract all columns of a particular type\n\nselect(data = ..., &lt;colname&gt;)  \nselect_if(data = ..., &lt;condition&gt;)\n\n\nselect - Exercise 4\n\nselect(starwars, hair_color)\n\n# A tibble: 87 x 1\n   hair_color   \n   &lt;chr&gt;        \n 1 blond        \n 2 &lt;NA&gt;         \n 3 &lt;NA&gt;         \n 4 none         \n 5 brown        \n 6 brown, grey  \n 7 brown        \n 8 &lt;NA&gt;         \n 9 black        \n10 auburn, white\n# i 77 more rows\n\nselect(starwars, -hair_color) # can also use not `!hair_color`\n\n# A tibble: 87 x 13\n   name      height  mass skin_color eye_color birth_year sex   gender homeworld\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 Luke Sky~    172    77 fair       blue            19   male  mascu~ Tatooine \n 2 C-3PO        167    75 gold       yellow         112   none  mascu~ Tatooine \n 3 R2-D2         96    32 white, bl~ red             33   none  mascu~ Naboo    \n 4 Darth Va~    202   136 white      yellow          41.9 male  mascu~ Tatooine \n 5 Leia Org~    150    49 light      brown           19   fema~ femin~ Alderaan \n 6 Owen Lars    178   120 light      blue            52   male  mascu~ Tatooine \n 7 Beru Whi~    165    75 light      blue            47   fema~ femin~ Tatooine \n 8 R5-D4         97    32 white, red red             NA   none  mascu~ Tatooine \n 9 Biggs Da~    183    84 light      brown           24   male  mascu~ Tatooine \n10 Obi-Wan ~    182    77 fair       blue-gray       57   male  mascu~ Stewjon  \n# i 77 more rows\n# i 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n#   starships &lt;list&gt;\n\nselect(starwars, hair_color, skin_color, eye_color)\n\n# A tibble: 87 x 3\n   hair_color    skin_color  eye_color\n   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 blond         fair        blue     \n 2 &lt;NA&gt;          gold        yellow   \n 3 &lt;NA&gt;          white, blue red      \n 4 none          white       yellow   \n 5 brown         light       brown    \n 6 brown, grey   light       blue     \n 7 brown         light       blue     \n 8 &lt;NA&gt;          white, red  red      \n 9 black         light       brown    \n10 auburn, white fair        blue-gray\n# i 77 more rows\n\nselect(starwars, hair_color:eye_color) # select using x:y columns\n\n# A tibble: 87 x 3\n   hair_color    skin_color  eye_color\n   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 blond         fair        blue     \n 2 &lt;NA&gt;          gold        yellow   \n 3 &lt;NA&gt;          white, blue red      \n 4 none          white       yellow   \n 5 brown         light       brown    \n 6 brown, grey   light       blue     \n 7 brown         light       blue     \n 8 &lt;NA&gt;          white, red  red      \n 9 black         light       brown    \n10 auburn, white fair        blue-gray\n# i 77 more rows\n\nselect(starwars, !(hair_color:eye_color)) # reverse selection using the not `!` operator\n\n# A tibble: 87 x 11\n   name    height  mass birth_year sex   gender homeworld species films vehicles\n   &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   &lt;lis&gt; &lt;list&gt;  \n 1 Luke S~    172    77       19   male  mascu~ Tatooine  Human   &lt;chr&gt; &lt;chr&gt;   \n 2 C-3PO      167    75      112   none  mascu~ Tatooine  Droid   &lt;chr&gt; &lt;chr&gt;   \n 3 R2-D2       96    32       33   none  mascu~ Naboo     Droid   &lt;chr&gt; &lt;chr&gt;   \n 4 Darth ~    202   136       41.9 male  mascu~ Tatooine  Human   &lt;chr&gt; &lt;chr&gt;   \n 5 Leia O~    150    49       19   fema~ femin~ Alderaan  Human   &lt;chr&gt; &lt;chr&gt;   \n 6 Owen L~    178   120       52   male  mascu~ Tatooine  Human   &lt;chr&gt; &lt;chr&gt;   \n 7 Beru W~    165    75       47   fema~ femin~ Tatooine  Human   &lt;chr&gt; &lt;chr&gt;   \n 8 R5-D4       97    32       NA   none  mascu~ Tatooine  Droid   &lt;chr&gt; &lt;chr&gt;   \n 9 Biggs ~    183    84       24   male  mascu~ Tatooine  Human   &lt;chr&gt; &lt;chr&gt;   \n10 Obi-Wa~    182    77       57   male  mascu~ Stewjon   Human   &lt;chr&gt; &lt;chr&gt;   \n# i 77 more rows\n# i 1 more variable: starships &lt;list&gt;\n\nselect(starwars, ends_with(\"color\")) # select by specific conditions are met by column names: starts_with, ends_with, contains...\n\n# A tibble: 87 x 3\n   hair_color    skin_color  eye_color\n   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 blond         fair        blue     \n 2 &lt;NA&gt;          gold        yellow   \n 3 &lt;NA&gt;          white, blue red      \n 4 none          white       yellow   \n 5 brown         light       brown    \n 6 brown, grey   light       blue     \n 7 brown         light       blue     \n 8 &lt;NA&gt;          white, red  red      \n 9 black         light       brown    \n10 auburn, white fair        blue-gray\n# i 77 more rows\n\nselect_if(starwars, is.numeric) # select_if to return all columns with numeric values\n\n# A tibble: 87 x 3\n   height  mass birth_year\n    &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1    172    77       19  \n 2    167    75      112  \n 3     96    32       33  \n 4    202   136       41.9\n 5    150    49       19  \n 6    178   120       52  \n 7    165    75       47  \n 8     97    32       NA  \n 9    183    84       24  \n10    182    77       57  \n# i 77 more rows\n\n\n\n\nmutate - Syntax\n\nmutate() to compute new columns\n\nMutate has a LOT of variants.\n\n\n\n\n\nSource: Rstudio cheatsheets\nmutate(data = ..., &lt;newcolname&gt; =  funs(&lt;oldcolname&gt;))\nmutate(data = ..., &lt;colname&gt;, funs(x))\n\n# using pipes! %&gt;%\ndata %&gt;% mutate(&lt;colname&gt;, funs(x)) # this is useful when you need to use multiple functions to act sequentially on a dataframe. Input becomes obvious by leading with it. \n\n\nmutate (& pipe %&gt;%)- Exercise 5\n\n# create a new column to display height in meters\nstarwars %&gt;% mutate(height_m = height / 100) # using pipe to feed data into the function\n\n# A tibble: 87 x 15\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk~    172    77 blond      fair       blue            19   male  mascu~\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu~\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl~ red             33   none  mascu~\n 4 Darth V~    202   136 none       white      yellow          41.9 male  mascu~\n 5 Leia Or~    150    49 brown      light      brown           19   fema~ femin~\n 6 Owen La~    178   120 brown, gr~ light      blue            52   male  mascu~\n 7 Beru Wh~    165    75 brown      light      blue            47   fema~ femin~\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu~\n 9 Biggs D~    183    84 black      light      brown           24   male  mascu~\n10 Obi-Wan~    182    77 auburn, w~ fair       blue-gray       57   male  mascu~\n# i 77 more rows\n# i 6 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, height_m &lt;dbl&gt;\n\n# using the pipe to feed data into multiple functions sequentially\nstarwars %&gt;%\n  mutate(height_m = height / 100) %&gt;% # this columns is always appended to the end of the table, by default\n  select(name, height_m, height, everything()) # using select to rearrange columns\n\n# A tibble: 87 x 15\n   name   height_m height  mass hair_color skin_color eye_color birth_year sex  \n   &lt;chr&gt;     &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n 1 Luke ~     1.72    172    77 blond      fair       blue            19   male \n 2 C-3PO      1.67    167    75 &lt;NA&gt;       gold       yellow         112   none \n 3 R2-D2      0.96     96    32 &lt;NA&gt;       white, bl~ red             33   none \n 4 Darth~     2.02    202   136 none       white      yellow          41.9 male \n 5 Leia ~     1.5     150    49 brown      light      brown           19   fema~\n 6 Owen ~     1.78    178   120 brown, gr~ light      blue            52   male \n 7 Beru ~     1.65    165    75 brown      light      blue            47   fema~\n 8 R5-D4      0.97     97    32 &lt;NA&gt;       white, red red             NA   none \n 9 Biggs~     1.83    183    84 black      light      brown           24   male \n10 Obi-W~     1.82    182    77 auburn, w~ fair       blue-gray       57   male \n# i 77 more rows\n# i 6 more variables: gender &lt;chr&gt;, homeworld &lt;chr&gt;, species &lt;chr&gt;,\n#   films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# mutate allows you to refer to columns that you&lt;e2&gt;&lt;80&gt;&lt;99&gt;ve just created\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(name, BMI, everything())\n\n# A tibble: 87 x 16\n   name        BMI height  mass hair_color skin_color eye_color birth_year sex  \n   &lt;chr&gt;     &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n 1 Luke Sky~  26.0    172    77 blond      fair       blue            19   male \n 2 C-3PO      26.9    167    75 &lt;NA&gt;       gold       yellow         112   none \n 3 R2-D2      34.7     96    32 &lt;NA&gt;       white, bl~ red             33   none \n 4 Darth Va~  33.3    202   136 none       white      yellow          41.9 male \n 5 Leia Org~  21.8    150    49 brown      light      brown           19   fema~\n 6 Owen Lars  37.9    178   120 brown, gr~ light      blue            52   male \n 7 Beru Whi~  27.5    165    75 brown      light      blue            47   fema~\n 8 R5-D4      34.0     97    32 &lt;NA&gt;       white, red red             NA   none \n 9 Biggs Da~  25.1    183    84 black      light      brown           24   male \n10 Obi-Wan ~  23.2    182    77 auburn, w~ fair       blue-gray       57   male \n# i 77 more rows\n# i 7 more variables: gender &lt;chr&gt;, homeworld &lt;chr&gt;, species &lt;chr&gt;,\n#   films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;, height_m &lt;dbl&gt;\n\n# output needs to be saved into a new dataframe since dplyr does not \"change\" the original dataframe\nstarwars_bmi &lt;- starwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(name, BMI, everything())\n\n# using if_else clauses with mutate\nstarwars_height &lt;- starwars %&gt;%\n  mutate(tall_short = if_else(\n    condition = height &gt; 160,\n    true = \"tall\",\n    false = \"short\"\n  )) %&gt;%\n  select(name, tall_short, everything())\n\nWhy use pipes? So you don’t have to create too many intermediate files! However, note that intermediate files can be useful in the context of troubleshooting a pipeline.\n\n\nrowwise operations (if time permits)\n\ndplyr (& tidyverse in general) is very easy to perform vectorized operations column-wise, and not so easy with row-wise operations.\nthe function rowwise() offers a solution to this\nLet’s look at an example:\n\n\n# let's input data (same one used in yesterday's problem set)\ndata &lt;- read_csv(file = \"data/data_transcript_exp_subset.csv\")\n\nRows: 100 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): ensembl_transcript_id\ndbl (6): rna_0h_rep1, rna_0h_rep2, rna_0h_rep3, rna_14h_rep1, rna_14h_rep2, ...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# calculate mean for each time point using mutate in a rowwise fashion!\ndata_mean &lt;- data %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    mean_0h = mean(c(rna_0h_rep1, rna_0h_rep2, rna_0h_rep3)),\n    mean_14h = mean(c(rna_14h_rep1, rna_14h_rep2, rna_14h_rep3))\n  )\n\nMore information here: https://dplyr.tidyverse.org/articles/rowwise.html\n\n\nsummarise - Syntax\n\nsummarise() reduces multiple values down to a single summary.\n\n\n\n\n\n\nsummarise(data = ..., &lt;newcolname&gt; =  funs(&lt;oldcolname&gt;))\nsummarise_at(data = ..., vars(&lt;cols), funs())\n\n\n\nsummarise - Exercise 6\n\nstarwars %&gt;% summarise(mean_height = mean(height, na.rm = TRUE))\n\n# A tibble: 1 x 1\n  mean_height\n        &lt;dbl&gt;\n1        174.\n\nstarwars %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE)\n\n# A tibble: 1 x 3\n  height  mass birth_year\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1   174.  97.3       87.6\n\nstarwars %&gt;% summarise_at(vars(height:mass), mean, na.rm = TRUE)\n\n# A tibble: 1 x 2\n  height  mass\n   &lt;dbl&gt; &lt;dbl&gt;\n1   174.  97.3\n\n\n\nsummarise() on its own is not terribly useful. But it becomes powerful when paired with group_by().\n\n\n\ngroup_by - Syntax\n\ngroup_by reate a grouped copy of a table.\nThis changes the unit of analysis from the complete dataset to individual groups.\nThen, when you use the dplyr verbs on a grouped data frame they&lt;80&gt;&lt;99&gt;ll be automatically applied &lt;80&gt;&lt;9c&gt;by group&lt;80&gt;&lt;9d&gt;.\n\ngroup_by(data = ..., &lt;colname&gt;)\n\n\ngroup_by + summarize - Exercise 7\n\nstarwars %&gt;%\n  group_by(species)\n\n# A tibble: 87 x 14\n# Groups:   species [38]\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk~    172    77 blond      fair       blue            19   male  mascu~\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu~\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl~ red             33   none  mascu~\n 4 Darth V~    202   136 none       white      yellow          41.9 male  mascu~\n 5 Leia Or~    150    49 brown      light      brown           19   fema~ femin~\n 6 Owen La~    178   120 brown, gr~ light      blue            52   male  mascu~\n 7 Beru Wh~    165    75 brown      light      blue            47   fema~ femin~\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu~\n 9 Biggs D~    183    84 black      light      brown           24   male  mascu~\n10 Obi-Wan~    182    77 auburn, w~ fair       blue-gray       57   male  mascu~\n# i 77 more rows\n# i 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars %&gt;%\n  group_by(species) %&gt;% # group by species\n  summarise(\n    height = mean(height, na.rm = TRUE)\n  )\n\n# A tibble: 38 x 2\n   species   height\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Aleena       79 \n 2 Besalisk    198 \n 3 Cerean      198 \n 4 Chagrian    196 \n 5 Clawdite    168 \n 6 Droid       131.\n 7 Dug         112 \n 8 Ewok         88 \n 9 Geonosian   183 \n10 Gungan      209.\n# i 28 more rows\n\nstarwars %&gt;%\n  group_by(species, gender) %&gt;% # group by multiple variables\n  summarise(\n    height = mean(height, na.rm = TRUE),\n    mass = mean(mass, na.rm = TRUE) # calculate multiple summaries\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 42 x 4\n# Groups:   species [38]\n   species   gender    height  mass\n   &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 Aleena    masculine     79  15  \n 2 Besalisk  masculine    198 102  \n 3 Cerean    masculine    198  82  \n 4 Chagrian  masculine    196 NaN  \n 5 Clawdite  feminine     168  55  \n 6 Droid     feminine      96 NaN  \n 7 Droid     masculine    140  69.8\n 8 Dug       masculine    112  40  \n 9 Ewok      masculine     88  20  \n10 Geonosian masculine    183  80  \n# i 32 more rows"
  },
  {
    "objectID": "content/bootcamp/r/class-03.html#combining-tables",
    "href": "content/bootcamp/r/class-03.html#combining-tables",
    "title": "Exercises-03",
    "section": "Combining tables",
    "text": "Combining tables\n\nCombine variables (i.e. columns)\n\nbind_cols()\nleft_join()\nright_join()\ninner_join()\nfull_join()\n\nCombine cases (i.e. rows)\n\nbind_rows()\nintersect()\nsetdiff()\nunion()\n\n\n\n\n\n\n\nSource: Rstudio cheatsheets\n\nCombine variables and cases - pasting tables together - Syntax\n\nbind_cols() to paste tables beside each other\nbind_rows() to paste tables below each other as they are.\n\nbind_cols(data1, data2) # should have same number of rows\nbind_rows(data1, data2) # should have same number of columns\n\n\nCombine variables - joining tables by a variable - Syntax\n\nleft_join() to join matching values from the right dataset to the left dataset\nright_join() to join matching values from the left dataset to the right dataset\ninner_join() to only join matching values present in both datasets\nfull_join() to join all values in both datasets\n\nleft_join(data1, data2)\nleft_join(data1, data2, by = ...) # if multiple columns match\nleft_join(data1, data2, by = c(\"col1\" = \"col2\")) # if the columns names that match are different\n\n\nCombine variables - joining tables by a variable - Exercise 8\n\nband_members %&gt;% left_join(band_instruments)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 3 x 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\nband_members %&gt;% right_join(band_instruments)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 3 x 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n3 Keith &lt;NA&gt;    guitar\n\nband_members %&gt;% inner_join(band_instruments)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 2 x 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\nband_members %&gt;% full_join(band_instruments)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 4 x 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n# band_members %&gt;% full_join(band_instruments2)\n# the above code won't work because there are no matching column names\nband_members %&gt;% full_join(band_instruments2, by = c(\"name\" = \"artist\"))\n\n# A tibble: 4 x 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\n\nJoining tables is something you will do a LOT in genomic analyses.\n\nThings to watch out for are to:\n\nexplicitly say which variable to join by\nmaking sure there aren’t subtle difference between what you think of as “common” variables\neg. Paul vs. Paul M; ENST79286869869 vs ENST79286869869.12\n\n\n\n\n\nOther dplyr verbs\nThere are many other dplyr verbs: rename, count, add_row, add_column, distinct, sample_n, sample_frac, slice, pull\nCheck out the dplyr cheatsheet to learn more!\n\n\nrename - Syntax\n\nrename() renames the variables in a table. Keeps all columns. select() can also do this, but with some caveats.\n\nrename(data = ..., &lt;colname&gt;)\n\n\nrename - Exercise 9\n\nband_instruments2\n\n# A tibble: 3 x 2\n  artist plays \n  &lt;chr&gt;  &lt;chr&gt; \n1 John   guitar\n2 Paul   bass  \n3 Keith  guitar\n\nrename(band_instruments2, name = artist)\n\n# A tibble: 3 x 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\nselect(band_instruments2, name = artist) # select drops the rest of the columns\n\n# A tibble: 3 x 1\n  name \n  &lt;chr&gt;\n1 John \n2 Paul \n3 Keith\n\nselect(band_instruments2, name = artist, everything()) # this fixes it, but one extra thing to remember\n\n# A tibble: 3 x 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\n\n\nProblem Set and Grading Rubric\n\nToday’s problem set assignment will allow you to practice the tidyr tools we learned in class today.\nThere is a total of 4 exercises, each with 5 points for a total of 20 points.\nGrading rubric listed at the beginning of the problem set.\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * dplyr vignette: https://dplyr.tidyverse.org/index.html * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nData Science with R by Garrett Grolemund https://garrettgman.github.io/tidying/"
  },
  {
    "objectID": "content/bootcamp/r/class-02.html",
    "href": "content/bootcamp/r/class-02.html",
    "title": "Exercises-02",
    "section": "",
    "text": "Suja Jagannathan sujatha.jagannathan@cuanschutz.edu"
  },
  {
    "objectID": "content/bootcamp/r/class-02.html#tidyverse",
    "href": "content/bootcamp/r/class-02.html#tidyverse",
    "title": "Exercises-02",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nTidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying design philosophy, grammar, and data structures.\n25 packages, total (as of today) - we will focus mainly on tidyr, dplyr, and ggplot2\n\n\n\n\n\n\n Source: R for Data Science by Hadley Wickham\n\nData import - readr - Exercise 1\n\n\n\n\n\n\n\n\n\n\nSource: Rstudio cheatsheets\n\nLet’s try importing a small dataset - Exercise # 1\n\n\ngetwd() # good to know which folder you are on since the path to file is relative\n\n[1] \"/Users/jayhesselberth/devel/rnabioco/molb-7950/content/bootcamp/r\"\n\n# same as `pwd` in bash\nmydata.tbl &lt;- read_csv(file = \"data/mydata.csv\") # read in the file\n\nNew names:\nRows: 4 Columns: 5\n-- Column specification\n-------------------------------------------------------- Delimiter: \",\" chr\n(2): chr, strand dbl (3): ...1, start, end\ni Use `spec()` to retrieve the full column specification for this data. i\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n* `` -&gt; `...1`\n\nmydata.tbl # look at the contents of the data object\n\n# A tibble: 4 x 5\n   ...1 chr   strand start   end\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 chr1  -        200   250\n2     2 chr1  -       4000   410\n3     3 chr2  +        100   200\n4     4 chr2  +        400   450\n\nd1 &lt;- mydata.tbl # rename file\nd1 # look at contents of new object\n\n# A tibble: 4 x 5\n   ...1 chr   strand start   end\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 chr1  -        200   250\n2     2 chr1  -       4000   410\n3     3 chr2  +        100   200\n4     4 chr2  +        400   450\n\nrm(mydata.tbl) # removing a dataset\n\nNote: All of these functions can also be used in an interactive manner via Environment &gt; Import Dataset &gt; From Text (readr)\n\n\nWhat is tidy data?\n\n“Tidy datasets are all alike but every messy dataset is messy in its own way.”\n— Hadley Wickham\n\n\n\n\n\n\nSource: Rstudio cheatsheets\n\n\nDatasets for today’s class - Exercise 2\n\nIn this class, we will use the datasets that come with the tidyr package to explore all the functions provided by tidyr.\nExplore the contents of tidyr package (Exercise #2)\ntable1, table2, table3, table4a, table4b, and table5 all display the number of TB cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000.\n\n\n\nGetting familiar with the data - Exercise 3\nR provides many functions to examine features of a data object\n\nView() - To open the table up in an excel-like interface - not recommended for large tables\nclass() - what kind of object is it (high-level)?\ntypeof() - what is the object&lt;80&gt;&lt;99&gt;s data type (low-level)?\nis_tibble() - use is.? to confirm data type\nstr() - what is the structure of the object?\nattributes() - does it have any metadata?\nLet’s explore table1\n\n\n# View(table1) # to look at the table in Viewer\ntable1 # to print the table to console\n\n# A tibble: 6 x 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nclass(table1)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ntypeof(table1)\n\n[1] \"list\"\n\nis_tibble(table1)\n\n[1] TRUE\n\nstr(table1)\n\ntibble [6 x 4] (S3: tbl_df/tbl/data.frame)\n $ country   : chr [1:6] \"Afghanistan\" \"Afghanistan\" \"Brazil\" \"Brazil\" ...\n $ year      : num [1:6] 1999 2000 1999 2000 1999 ...\n $ cases     : num [1:6] 745 2666 37737 80488 212258 ...\n $ population: num [1:6] 2.00e+07 2.06e+07 1.72e+08 1.75e+08 1.27e+09 ...\n\nattributes(table1)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3 4 5 6\n\n$names\n[1] \"country\"    \"year\"       \"cases\"      \"population\"\n\n\nNote: a tibble can be coerced into a data.frame using as.data.frame(tbl)\n\n\nGetting familiar with the data - Exercise 4\nSome of the useful Data Frame Functions are as follows:\n- head() - shows first 6 rows\n- tail() - shows last 6 rows\n- dim() - returns the dimensions of data frame (i.e. number of rows and number of columns)\n- nrow() - number of rows\n- ncol() - number of columns\n- names() or colnames() - both show the names attribute for a data frame\n- sapply(dataframe, class) - shows the class of each column in the data frame*\n- glimpse()\n*Iteration is not covered in this bootcamp. But there is a great primer on the package that does this, purr, on Rstudio Primers. https://rstudio.cloud/learn/primers/5\nMore functions to explore data - table2 this time.\n\nhead(table2)\n\n# A tibble: 6 x 4\n  country      year type           count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Afghanistan  1999 cases            745\n2 Afghanistan  1999 population  19987071\n3 Afghanistan  2000 cases           2666\n4 Afghanistan  2000 population  20595360\n5 Brazil       1999 cases          37737\n6 Brazil       1999 population 172006362\n\ntail(table2)\n\n# A tibble: 6 x 4\n  country  year type            count\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 Brazil   2000 cases           80488\n2 Brazil   2000 population  174504898\n3 China    1999 cases          212258\n4 China    1999 population 1272915272\n5 China    2000 cases          213766\n6 China    2000 population 1280428583\n\ntail(table2, n = 8) # specify number of lines to print\n\n# A tibble: 8 x 4\n  country  year type            count\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 Brazil   1999 cases           37737\n2 Brazil   1999 population  172006362\n3 Brazil   2000 cases           80488\n4 Brazil   2000 population  174504898\n5 China    1999 cases          212258\n6 China    1999 population 1272915272\n7 China    2000 cases          213766\n8 China    2000 population 1280428583\n\ndim(table2)\n\n[1] 12  4\n\nnrow(table2)\n\n[1] 12\n\nncol(table2)\n\n[1] 4\n\nnames(table2)\n\n[1] \"country\" \"year\"    \"type\"    \"count\"  \n\nsapply(table2, class)\n\n    country        year        type       count \n\"character\"   \"numeric\" \"character\"   \"numeric\" \n\nclass(table2) # observe the difference from the above command\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(table2)\n\nRows: 12\nColumns: 4\n$ country &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"B~\n$ year    &lt;dbl&gt; 1999, 1999, 2000, 2000, 1999, 1999, 2000, 2000, 1999, 1999, 20~\n$ type    &lt;chr&gt; \"cases\", \"population\", \"cases\", \"population\", \"cases\", \"popula~\n$ count   &lt;dbl&gt; 745, 19987071, 2666, 20595360, 37737, 172006362, 80488, 174504~\n\n\n\n\nGetting familiar with the data - summary, hist, & table - Exercise 5\nsummary: A generic function used to produce result summaries of the results of various model fitting functions.\n\nsummary(table1) # summary of the whole table\n\n   country               year          cases          population       \n Length:6           Min.   :1999   Min.   :   745   Min.   :1.999e+07  \n Class :character   1st Qu.:1999   1st Qu.: 11434   1st Qu.:5.845e+07  \n Mode  :character   Median :2000   Median : 59112   Median :1.733e+08  \n                    Mean   :2000   Mean   : 91277   Mean   :4.901e+08  \n                    3rd Qu.:2000   3rd Qu.:179316   3rd Qu.:9.983e+08  \n                    Max.   :2000   Max.   :213766   Max.   :1.280e+09  \n\nsummary(table1$cases) # summary of just one column of the table\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    745   11434   59112   91277  179316  213766 \n\n\nhist: Takes in a vector of values and plots a histogram.\n\nhist(table1$cases) # summary of just one column of the table\n\n\n\n\ntable: Uses the cross-classifying factors to build a contingency table of the counts at each combination of factor levels.\n\ntable(table2$year) # tally of one column\n\n\n1999 2000 \n   6    6 \n\ntable(table2$year, table2$type) # two columns at a time\n\n      \n       cases population\n  1999     3          3\n  2000     3          3"
  },
  {
    "objectID": "content/bootcamp/r/class-02.html#tidying-data",
    "href": "content/bootcamp/r/class-02.html#tidying-data",
    "title": "Exercises-02",
    "section": "Tidying data",
    "text": "Tidying data\nThe four verbs to keep in mind for reshaping data with tidyr are:\n- pivot_wider\n- pivot_longer\n- separate\n- unite\nThere are other verbs as well - as always, look at the tidyr cheatsheet!\n\npivot_wider - syntax\npivot_wider() “widens” data, increasing the number of columns and decreasing the number of rows.\n\n\n\n\n\npivot_wider(\n  data,\n  names_from = name,\n  values_from = value,\n  ...\n)\n\n\npivot_wider - Exercise 6\n\nlibrary(tidyr)\ntable1 # this is a tidy dataset\n\n# A tibble: 6 x 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2 # this one is NOT tidy\n\n# A tibble: 12 x 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\npivot_wider(table2,\n  names_from = type,\n  values_from = count\n)\n\n# A tibble: 6 x 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n# if you want to save the tidy table, you have to assign the output to a new object\ntable2_tidy &lt;- pivot_wider(\n  table2,\n  names_from = type,\n  values_from = count\n)\n\ntable2_tidy\n\n# A tibble: 6 x 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\npivot_longer - Syntax\npivot_longer() “lengthens” data, increasing the number of rows and decreasing the number of columns.\n\n\n\n\n\npivot_longer(\n  data,\n  cols,\n  names_to = \"name\",\n  values_to = \"value\",\n  ...\n)\n\n\npivot_longer - Exercise 7\n\ntable4a\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\npivot_longer(table4a, 2:3, names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 x 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\npivot_longer(table4a, -country, names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 x 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\ntable4_tidy &lt;- pivot_longer(table4a, -country, names_to = \"year\", values_to = \"cases\")\ntable4_tidy\n\n# A tibble: 6 x 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\n\nseparate - Syntax\nGiven either a regular expression or a vector of character positions, separate() turns a single character column into multiple columns.\n\n\n\n\n\nseparate(\n  data,\n  col,\n  into,\n  sep = \"[^[:alnum:]]+\", #any sequence of non-alphanumeric values\n  remove = TRUE, # default is to remove the original column\n  convert = FALSE, # default is to not convert\n  extra = \"warn\",\n  fill = \"warn\",\n  ...\n)\n\n\n\nseparate - Exercise 8\n\ntable3\n\n# A tibble: 6 x 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ntable3_tidy_1 &lt;- separate(table3,\n  rate,\n  into = c(\"cases\", \"population\"),\n  sep = \"/\"\n)\ntable3_tidy_1\n\n# A tibble: 6 x 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nseparate_rows - Syntax\nGiven either a regular expression or a vector of character positions, separate() turns a single character column into multiple rows.\n\n\n\n\n\nseparate_rows(\n  data, \n  ..., \n  sep = \"[^[:alnum:].]+\", \n  convert = FALSE)\n\n\nseparate_rows - Exercise 9\n\ntable3\n\n# A tibble: 6 x 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ntable3_tidy_2 &lt;- separate_rows(table3, rate, sep = \"/\")\ntable3_tidy_2\n\n# A tibble: 12 x 3\n   country      year rate      \n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     \n 1 Afghanistan  1999 745       \n 2 Afghanistan  1999 19987071  \n 3 Afghanistan  2000 2666      \n 4 Afghanistan  2000 20595360  \n 5 Brazil       1999 37737     \n 6 Brazil       1999 172006362 \n 7 Brazil       2000 80488     \n 8 Brazil       2000 174504898 \n 9 China        1999 212258    \n10 China        1999 1272915272\n11 China        2000 213766    \n12 China        2000 1280428583\n\n\nThis is not a great example because in creating two rows, the case and population numbers are now mixed up and we have lost information. So always think about and be intentional to avoid information loss.\n\n\nunite - Syntax\nunite() combines multiple columns into a single column.\n\n\n\n\n\nunite(data, \n      col, \n      ..., #select columns to unite\n      sep = \"[^[:alnum:]]+\", #any sequence of non-alphanumeric values\n      remove = TRUE, #default is set to TRUE\n      na.rm = FALSE) #default is set to FALSE\n\n\nunite - Exercise 10\n\ntable6 &lt;- read_csv(file = \"data/table6.csv\")\n\nNew names:\nRows: 3 Columns: 5\n-- Column specification\n-------------------------------------------------------- Delimiter: \",\" chr\n(1): country dbl (4): ...1, century, year, cases\ni Use `spec()` to retrieve the full column specification for this data. i\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n* `` -&gt; `...1`\n\ntable6_tidy &lt;- unite(table6, \"new\", century, year, sep = \"\")\ntable6_tidy\n\n# A tibble: 3 x 4\n   ...1 country    new    cases\n  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n1     1 Afganistan 1999     745\n2     2 Brazil     1999   37737\n3     3 China      1999  212258\n\n\n\n\nHandling missing values\n\n\n\n\n\nSource: Rstudio cheatsheets"
  },
  {
    "objectID": "content/bootcamp/r/class-02.html#regular-expressions",
    "href": "content/bootcamp/r/class-02.html#regular-expressions",
    "title": "Exercises-02",
    "section": "Regular expressions",
    "text": "Regular expressions\n\n\n\n\n\n Source: Rstudio cheatsheets\nUseful website: Regexr\nNote: stringr is an entire package focused on working with character strings. I highly recommend checking it out!\n\nProblem Set and Grading Rubric\n\nToday’s problem set assignment will allow you to practice the tidyr tools we learned in class today.\nThere is a total of 5 exercises, each with 4 points for a total of 20 points.\nLink to grading rubric.\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * Data Science with R by Garrett Grolemund: https://garrettgman.github.io/tidying/ * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nData Science with R by Garrett Grolemund https://garrettgman.github.io/tidying/"
  },
  {
    "objectID": "content/bootcamp/r/class-05.html",
    "href": "content/bootcamp/r/class-05.html",
    "title": "Exercises-05",
    "section": "",
    "text": "R bootcamp review\n\ntidyr - nothing requested\ndplyr\n\ngroup_by - examples below\n\n\n\n# import data\ndata_transcript_exp_tidy &lt;- read_csv(\"data/data_transcript_exp_tidy.csv\")\n\nRows: 600 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (4): ensembl_transcript_id, type, time, replicate\ndbl (1): count\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# save to have smaller name\ndata &lt;- data_transcript_exp_tidy\n# examples of grouping by differently - type these into the console to see the difference\ngroup_by(data, type)\n\n# A tibble: 600 x 5\n# Groups:   type [1]\n   ensembl_transcript_id      type  time  replicate count\n   &lt;chr&gt;                      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 ENST00000327044.6_51_2298  rna   0h    rep1        243\n 2 ENST00000327044.6_51_2298  rna   0h    rep2        322\n 3 ENST00000327044.6_51_2298  rna   0h    rep3        303\n 4 ENST00000327044.6_51_2298  rna   14h   rep1        177\n 5 ENST00000327044.6_51_2298  rna   14h   rep2        177\n 6 ENST00000327044.6_51_2298  rna   14h   rep3        239\n 7 ENST00000338591.7_360_2034 rna   0h    rep1         19\n 8 ENST00000338591.7_360_2034 rna   0h    rep2         17\n 9 ENST00000338591.7_360_2034 rna   0h    rep3         15\n10 ENST00000338591.7_360_2034 rna   14h   rep1          9\n# i 590 more rows\n\ngroup_by(data, replicate)\n\n# A tibble: 600 x 5\n# Groups:   replicate [3]\n   ensembl_transcript_id      type  time  replicate count\n   &lt;chr&gt;                      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 ENST00000327044.6_51_2298  rna   0h    rep1        243\n 2 ENST00000327044.6_51_2298  rna   0h    rep2        322\n 3 ENST00000327044.6_51_2298  rna   0h    rep3        303\n 4 ENST00000327044.6_51_2298  rna   14h   rep1        177\n 5 ENST00000327044.6_51_2298  rna   14h   rep2        177\n 6 ENST00000327044.6_51_2298  rna   14h   rep3        239\n 7 ENST00000338591.7_360_2034 rna   0h    rep1         19\n 8 ENST00000338591.7_360_2034 rna   0h    rep2         17\n 9 ENST00000338591.7_360_2034 rna   0h    rep3         15\n10 ENST00000338591.7_360_2034 rna   14h   rep1          9\n# i 590 more rows\n\ngroup_by(data, time)\n\n# A tibble: 600 x 5\n# Groups:   time [2]\n   ensembl_transcript_id      type  time  replicate count\n   &lt;chr&gt;                      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 ENST00000327044.6_51_2298  rna   0h    rep1        243\n 2 ENST00000327044.6_51_2298  rna   0h    rep2        322\n 3 ENST00000327044.6_51_2298  rna   0h    rep3        303\n 4 ENST00000327044.6_51_2298  rna   14h   rep1        177\n 5 ENST00000327044.6_51_2298  rna   14h   rep2        177\n 6 ENST00000327044.6_51_2298  rna   14h   rep3        239\n 7 ENST00000338591.7_360_2034 rna   0h    rep1         19\n 8 ENST00000338591.7_360_2034 rna   0h    rep2         17\n 9 ENST00000338591.7_360_2034 rna   0h    rep3         15\n10 ENST00000338591.7_360_2034 rna   14h   rep1          9\n# i 590 more rows\n\ngroup_by(data, ensembl_transcript_id)\n\n# A tibble: 600 x 5\n# Groups:   ensembl_transcript_id [100]\n   ensembl_transcript_id      type  time  replicate count\n   &lt;chr&gt;                      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 ENST00000327044.6_51_2298  rna   0h    rep1        243\n 2 ENST00000327044.6_51_2298  rna   0h    rep2        322\n 3 ENST00000327044.6_51_2298  rna   0h    rep3        303\n 4 ENST00000327044.6_51_2298  rna   14h   rep1        177\n 5 ENST00000327044.6_51_2298  rna   14h   rep2        177\n 6 ENST00000327044.6_51_2298  rna   14h   rep3        239\n 7 ENST00000338591.7_360_2034 rna   0h    rep1         19\n 8 ENST00000338591.7_360_2034 rna   0h    rep2         17\n 9 ENST00000338591.7_360_2034 rna   0h    rep3         15\n10 ENST00000338591.7_360_2034 rna   14h   rep1          9\n# i 590 more rows\n\n# example of how group_by affects the way summarise works\ndata %&gt;% summarise(count = mean(count, na.rm = TRUE))\n\n# A tibble: 1 x 1\n  count\n  &lt;dbl&gt;\n1  144.\n\ngroup_by(data, time) %&gt;% summarise(count = mean(count, na.rm = TRUE))\n\n# A tibble: 2 x 2\n  time  count\n  &lt;chr&gt; &lt;dbl&gt;\n1 0h     185.\n2 14h    103.\n\ngroup_by(data, ensembl_transcript_id) %&gt;% summarise(count = mean(count, na.rm = TRUE))\n\n# A tibble: 100 x 2\n   ensembl_transcript_id          count\n   &lt;chr&gt;                          &lt;dbl&gt;\n 1 ENST00000054650.8_159_876       8.39\n 2 ENST00000054666.10_116_416    121.  \n 3 ENST00000054668.5_220_418       3.75\n 4 ENST00000234590.8_121_1423   7993.  \n 5 ENST00000263741.11_1328_1496   29.4 \n 6 ENST00000263741.11_315_1338   141.  \n 7 ENST00000270708.11_75_1455     45.7 \n 8 ENST00000288774.7_29_1067      16.1 \n 9 ENST00000291386.3_370_895     134.  \n10 ENST00000307896.10_39_753       8.39\n# i 90 more rows\n\ngroup_by(data, ensembl_transcript_id, time) %&gt;% summarise(count = mean(count, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ensembl_transcript_id'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 200 x 3\n# Groups:   ensembl_transcript_id [100]\n   ensembl_transcript_id        time    count\n   &lt;chr&gt;                        &lt;chr&gt;   &lt;dbl&gt;\n 1 ENST00000054650.8_159_876    0h       11.3\n 2 ENST00000054650.8_159_876    14h       5.5\n 3 ENST00000054666.10_116_416   0h      149  \n 4 ENST00000054666.10_116_416   14h      93.7\n 5 ENST00000054668.5_220_418    0h        0  \n 6 ENST00000054668.5_220_418    14h       7.5\n 7 ENST00000234590.8_121_1423   0h    10522. \n 8 ENST00000234590.8_121_1423   14h    5465. \n 9 ENST00000263741.11_1328_1496 0h       32.5\n10 ENST00000263741.11_1328_1496 14h      26.3\n# i 190 more rows\n\n\n\nfunctions to use within mutate - google is your friend\nmanipulation of cols & rows - use cheatsheets\nggplot\n\naesthetic mapping: Ref\n\n\n\n# specifying colors of plots\ndiamonds_subset &lt;- diamonds %&gt;% sample_n(size = 1000)\n\n# scatter plot with color by cut - aesthetics specified in the main \"mapping\"\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point()\n\n\n\n# scatter plot with color by cut - aesthetics specified in the geom \"mapping\"\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point(aes(color = cut))\n\n\n\n# color specified in the main mapping will apply universally to all geoms\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n# color specified in the geom mapping only applies to that layer\nggplot(diamonds_subset, mapping = aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\npiping data into ggplot - example below\n\n\ndiamonds_subset %&gt;% ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nmore applied examples - plenty to come in the rest of the course\nspecifying colors of plots: tutorial\n\n\n# coloring by a single color - more information in the tutorial above\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point(color = \"red\") +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nother\n\nlog10 - ?log10\nHow to use Help pages: help()\nrequired vs. optional arguments: this is possible to distinguish if you have a well-documented function. If not, trial-an-error is how it goes.\nUse of commas with multiple vars: Depends on the function. Look at the exact syntax required for specific functions on the cheatsheet to know what to use: col1, col2, col3, vs col1:col3, etc\nMetacharacters “.” etc.: Refer to cheatsheet on Regex + stringr"
  },
  {
    "objectID": "content/bootcamp/r/class-04.html",
    "href": "content/bootcamp/r/class-04.html",
    "title": "Exercises-04",
    "section": "",
    "text": "Suja Jagannathan sujatha.jagannathan@cuanschutz.edu"
  },
  {
    "objectID": "content/bootcamp/r/class-04.html#ggplot2",
    "href": "content/bootcamp/r/class-04.html#ggplot2",
    "title": "Exercises-04",
    "section": "ggplot2",
    "text": "ggplot2\nggplot2 is based on the “grammar of graphics”, the idea that you can build every graph from the same components: a data set, a coordinate system, and geoms &lt;80&gt;&lt;94&gt; visual marks that represent data points.\n\nToday’s datasets\n\nIn this class, we will use one of the datasets that come with the ggplot2 package.\n\nggplot2::diamonds data frame contains data about 87 characters from Starwars\n\n\n\nGetting familiar with the data - Exercise 1\n\n# help(\"diamonds\")\nhead(diamonds)\n\n# A tibble: 6 x 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.~\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver~\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,~\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ~\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64~\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58~\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34~\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.~\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.~\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.~\n\n# View(diamonds)\n\n\n\nThe basic syntax of ggplot()\nggplot(): build plots piece by piece\nThe concept of ggplot divides a plot into three different fundamental parts:\nplot = data + coordinate-system + geometry.\ndata: a data frame\ncoordinate-system: specify x and y variables\ngeometry: specify type of plots - histogram, boxplot, line, density, dotplot, bar, etc.\n\n\n\n\n\naesthetics can map variables in the data to visual properties of the geom (aesthetics) like size, color, and x and y locations to make the plot more information rich.\n\n\n\n\n\n\n\nMaking a plot step-by-step (Exercise 2)\n\n# initialize with data\nggplot(data = diamonds) # specify which dataframe to use - no plot yet\n\n\n\n#  specify the coordinate system\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) # data to map onto x and y axes\n\n\n\n# specify geometry\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point() # `geom` is 'geom_point()'\n\n\n\n######### Note: the position of `+` is important ##########\n\n\n# map aesthetics to other variables\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point()\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point(alpha = 0.2) # adjusting transparency of points\n\n\n\n\n\n\nLooking under the hood of ggplot (Exercise 3)\n\n# initialize with data\np1 &lt;- ggplot(data = diamonds) # specify which dataframe to use - no plot yet\n\n#  specify the coordinate system\np2 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) # data to map onto x and y axes\n\n# specify geometry\np3 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point() # `geom` is 'geom_boxplot()'\n\n# map aesthetics to other variables\np4 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point() # `geom` is 'geom_boxplot()'\n\np5 &lt;- ggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut, size = carat)\n) +\n  geom_point(alpha = 0.2) # `geom` is 'geom_boxplot()'\n\ntypeof(p1)\n\n[1] \"list\"\n\nnames(p1)\n\n[1] \"data\"        \"layers\"      \"scales\"      \"mapping\"     \"theme\"      \n[6] \"coordinates\" \"facet\"       \"plot_env\"    \"labels\"     \n\nhead(p1$data)\n\n# A tibble: 6 x 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\nsummary(p1)\n\ndata: carat, cut, color, clarity, depth, table, price, x, y, z\n  [53940x10]\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n\nsummary(p2)\n\ndata: carat, cut, color, clarity, depth, table, price, x, y, z\n  [53940x10]\nmapping:  x = ~carat, y = ~price\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n\nsummary(p3)\n\ndata: carat, cut, color, clarity, depth, table, price, x, y, z\n  [53940x10]\nmapping:  x = ~carat, y = ~price\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n-----------------------------------\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\nsummary(p4)\n\ndata: carat, cut, color, clarity, depth, table, price, x, y, z\n  [53940x10]\nmapping:  x = ~carat, y = ~price, colour = ~cut, size = ~carat\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n-----------------------------------\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\nsummary(p5)\n\ndata: carat, cut, color, clarity, depth, table, price, x, y, z\n  [53940x10]\nmapping:  x = ~carat, y = ~price, colour = ~cut, size = ~carat\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n-----------------------------------\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n\n\n\nggplot is overkill for simple plots, but powerfully simple in making complex plots\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat)\n) +\n  geom_histogram() # histogram\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n## why can't I just do this?\nhist(diamonds$carat)\n\n\n\n\nYou can. But the advantage of ggplot is that it is equally “simple” to make basic and complex plots. The underlying grammar lets you exquisitly customize the appearance of your plot and make publishable figures."
  },
  {
    "objectID": "content/bootcamp/r/class-04.html#create-more-complex-plots",
    "href": "content/bootcamp/r/class-04.html#create-more-complex-plots",
    "title": "Exercises-04",
    "section": "Create more complex plots",
    "text": "Create more complex plots\n\n\n\n\n\n\nGeom function\n\nUse a geom function to represent data points, use the geom&lt;80&gt;&lt;99&gt;s aesthetic properties to represent variables.\nEach function returns a layer.\nThere are a LOT of geom functions in ggplot that are specific to plots with 1, 2, or 3 variables (i.e. if you really need to plot 3 variables - better to use aesthetics instead)\n\n\n\nGeom functions for one variable - Exercise 4\n\n\n\n\n\n\n# bar plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut)\n) +\n  geom_bar() # bar\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, fill = clarity)\n) +\n  geom_bar()\n\n\n\n# density plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat)\n) +\n  geom_density() # density\n\n\n\n# color the density plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat)\n) +\n  geom_density(fill = \"tomato1\")\n\n\n\n# plot subsets by mapping `fill` to `cut`\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, fill = cut)\n) +\n  geom_density(alpha = 0.8)\n\n\n\n\n\n# use ggridges to plot subsets in a staggered fashion!\nggplot(\n  data = diamonds,\n  mapping = aes(carat, y = cut, fill = cut)\n) +\n  geom_density_ridges() # function from the ggridges package, NOT ggplot2\n\nPicking joint bandwidth of 0.0647\n\n\n\n\n\n\n\nGeom functions for two variables\nWith two variables, depending on the nature of the data, you can have different kinds of geoms: - discrete x, continuous y - continuous x, continuous y - continuous bivariate - & others (check out the cheatsheet!)\n\n\ndiscrete x, continuous y - Exercise 5\n\n\n\n\n\n\n# column plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price)\n) +\n  geom_col()\n\n\n\noptions(scipen = 10000) # disables scientific notation - only have to type once for the whole Rmd\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price)\n) +\n  geom_col()\n\n\n\n# box plot\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price)\n) +\n  geom_boxplot()\n\n\n\n\n\n# box plot with fill color by cut\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price, fill = cut)\n) +\n  geom_boxplot()\n\n\n\n# violin plot with fill color by cut\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = price, fill = cut)\n) +\n  geom_violin()\n\n\n\n\n\n\ncontinuous x, continuous y - Exercise 6\n\n\n\n\n\n\n# subset diamonds to see points better\ndiamonds_subset &lt;- diamonds %&gt;% sample_n(size = 1000)\n\n# scatter plot\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point()\n\n\n\n# geom_smooth\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n# combining geoms - 1\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n# combining geoms - 2\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point() +\n  geom_rug()\n\n\n\n\n\n\ncontinuous bivariate - Exercise 7\n\n\n\n\n\n\n# scatter plot\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point()\n\n\n\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_hex()\n\n\n\n\n\n\nGeom functions for three variables - Exercise 8\n\n\n\n\n\nOne example with geom_tile()\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = carat, fill = price)\n) +\n  geom_tile(colour = \"white\") +\n  scale_fill_gradientn(colors = c(\"red\", \"white\", \"blue\"))\n\n\n\n\n\n\nshape, size, fill, color, and transparency - Exercise 9\nR has 25 built in shapes that are identified by numbers. There are some seeming duplicates: for example, 0, 15, and 22 are all squares. The difference comes from the interaction of the colour and fill aesthetics. The hollow shapes (0&lt;80&gt;&lt;93&gt;14) have a border determined by colour; the solid shapes (15&lt;80&gt;&lt;93&gt;18) are filled with colour; the filled shapes (21&lt;80&gt;&lt;93&gt;24) have a border of colour and are filled with fill.\n\n\n\n\n\n\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price)\n) +\n  geom_point(aes(fill = cut, size = depth),\n    alpha = 0.8,\n    shape = 24,\n    color = \"white\"\n  )\n\n\n\n\nNote that aesthetics can also be defined within geoms\n\n\nPosition adjustments - Exercise 10\nPosition adjustments determine how to arrange geoms that would otherwise occupy the same space.\n\n\n\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, fill = clarity)\n) +\n  geom_bar()\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, fill = clarity)\n) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nJitter can be used to avoid over-plotting along with geom_point.\n\n\nCoordinate and Scale Functions - Exercise 11\nWe won’t go into these functions too much today, but here is a brief overview:\n\nThe coordinate system determines how the x and y aesthetics combine to position elements in the plot. The default coordinate system is Cartesian ( coord_cartesian() ), which can be tweaked with coord_map() , coord_fixed() , coord_flip() , and coord_trans() , or completely replaced with coord_polar()\nScales control the details of how data values are translated to visual properties. There are 20+ scale functions. We will look at one; the ggplot2 cheatsheet is your friend for the rest.\n\n\n# coord transform\nggplot(diamonds_subset, aes(carat, price)) +\n  geom_point() +\n  coord_trans(x = \"log10\")\n\n\n\n\n\n# coord_flip\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut)\n) +\n  geom_bar() # bar\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut)\n) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n# scales\nggplot(diamonds_subset, aes(carat, price)) +\n  geom_point() +\n  scale_x_log10()\n\n\n\n\nBrief aside: ggplot can also handle on-the-fly data transformations like below.\n\n# log transformed carat and USD converted to CAD\nggplot(diamonds_subset, aes(log10(carat), price * 1.32)) +\n  geom_point()\n\n\n\n\n\n\nZooming into a plot - Exercise 12\nOne might often want to change the limits of x or y axes to zoom in. There are multiple ways to do this.\n\nggplot(diamonds_subset, aes(carat, price)) +\n  geom_point(alpha = 0.5) +\n  coord_cartesian(xlim = c(0, 2), ylim = c(0, 5000))\n\n\n\nggplot(diamonds_subset, aes(carat, price)) +\n  geom_point(alpha = 0.5) +\n  xlim(0, 2) +\n  ylim(0, 5000)\n\nWarning: Removed 269 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nFaceting to plot subsets of data into separate panels - Exercise 13\nFacets divide a plot into subplots based on the values of one or more discrete variables.\n\n\n\n\n\n\n# density plot for data subsets\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, fill = cut)\n) +\n  geom_density(alpha = 0.8)\n\n\n\n\n\n# density plot with facets\nggplot(\n  data = diamonds,\n  mapping = aes(x = log(price), fill = cut)\n) +\n  geom_density(color = \"black\") +\n  facet_wrap(~cut, nrow = 1)\n\n\n\n# scatter plot with facets\nggplot(\n  data = diamonds,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = .05) +\n  facet_wrap(~cut, nrow = 1)\n\n\n\n\n\n\nThemes - Exercise 14\nThemes can significantly affect the appearance of your plot. Thanksfully, there are a lot to choose from.\n\n\n\n\n\n\n# default theme\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8)\n\n\n\n# theme black & white\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_bw()\n\n\n\n# theme few\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_few()\n\n\n\n\n\n# theme wsj\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_wsj()\n\n\n\n# theme economist\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_economist()\n\n\n\n\nYou can also customize pre-existing themes\n\nmytheme &lt;- theme_minimal(base_size = 15) +\n  theme(\n    aspect.ratio = 1,\n    panel.background =\n      element_rect(\n        colour = \"black\",\n        size = 1\n      )\n  )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\ni Please use the `linewidth` argument instead.\n\nggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  mytheme # notice the lack of parantheses, because this is an object, not a function\n\n\n\n\n\n\nLabels & Legends - Exercise 15\n\n\n\n\n\n\nggplot(\n  data = diamonds,\n  mapping = aes(x = cut, y = log(price), fill = cut)\n) +\n  geom_boxplot() +\n  labs(\n    y = \"Price (log scale)\", x = \"Cut\", color = \"Cut\",\n    title = \"Distribution of diamond prices by cut\",\n    subtitle = \"Data come from a random sample of 1000 diamonds\",\n    caption = \"Source: diamonds dataset from ggplot2\"\n  ) +\n  annotate(geom = \"text\", x = 1, y = 5, label = \"Random text\") +\n  theme_bw()"
  },
  {
    "objectID": "content/bootcamp/r/class-04.html#additional-points",
    "href": "content/bootcamp/r/class-04.html#additional-points",
    "title": "Exercises-04",
    "section": "Additional points",
    "text": "Additional points\n\nHow to add a line to a plot? (Exercise 16)\n\np &lt;- ggplot(\n  data = diamonds_subset,\n  mapping = aes(x = carat, y = price, color = cut)\n) +\n  geom_point(alpha = 0.8) +\n  theme_few()\n\np + geom_line()\n\n\n\np + geom_hline(aes(yintercept = 5000))\n\n\n\np + geom_vline(aes(xintercept = 2))\n\n\n\np + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\np + geom_abline(aes(intercept = 0.5, slope = 5000))\n\n\n\n\n\n\nHow to combine multiple plots into a figure? (Exercise 17)\n\nplot1 &lt;- p\nplot2 &lt;- p + geom_hline(aes(yintercept = 5000))\nplot3 &lt;- p + geom_vline(aes(xintercept = 2))\nplot4 &lt;- p + geom_abline(aes(intercept = 0.5, slope = 5000))\n\nall_plots &lt;- plot_grid(plot1, plot2, plot3, plot4, labels = c(\"A\", \"B\", \"C\", \"D\"), nrow = 2)\nall_plots\n\n\n\n# we have 4 legends, which is too many - can they be removed?\n# Yes, but it is not exactly straightforward\nlegend &lt;- get_legend(plot1 + theme(legend.position = \"bottom\"))\nplot1 &lt;- p + theme(legend.position = \"none\")\nplot2 &lt;- p + geom_hline(aes(yintercept = 5000)) + theme(legend.position = \"none\")\nplot3 &lt;- p + geom_vline(aes(xintercept = 2)) + theme(legend.position = \"none\")\nplot4 &lt;- p + geom_abline(aes(intercept = 0.5, slope = 5000)) + theme(legend.position = \"none\")\n\nall_plots &lt;- plot_grid(plot1, plot2, plot3, plot4, labels = c(\"A\", \"B\", \"C\", \"D\"), nrow = 2)\nplot_final &lt;- plot_grid(all_plots, legend, ncol = 1, rel_heights = c(1, .1))\nplot_final\n\n\n\n\nMore information on using plot_grid (from package cowplot) is here\n\n\nSaving plots (Exercise 18)\n\nggsave(\"img/plot_final.png\", width = 5, height = 5)\n# Saves last plot as 5&lt;e2&gt;&lt;80&gt;&lt;99&gt; x 5&lt;e2&gt;&lt;80&gt;&lt;99&gt; file named \"plot_final.png\" in working directory. Matches file type to file extension\n\n\n\nThe Final Problem Set and Grading Rubric\n\nThe final problem set assignment will be posted by 4pm today (i.e. not at noon)\nIt is due Monday, August 31, by noon\nYou can work on the assignment in class tomorrow - which will be used for review and answering questions\nGrading rubric will be listed at the beginning of the problem set.\n\n\n\nAcknowledgements\nThe material for this class was heavily borrowed from: * R for data science by Hadley Wickham: https://r4ds.had.co.nz/index.html * TheRBootcamp: https://therbootcamp.github.io/BaselRBootcamp_2018April/_sessions/D3S2_PlottingI/PlottingI_practical_answers.html\n\n\nFurther Reading & Resources\n\nR for data science https://r4ds.had.co.nz/index.html\nAdvanced R by Hadley Wickam https://adv-r.hadley.nz/\nData Science with R by Garrett Grolemund https://garrettgman.github.io/tidying/\nggplot2 reference: https://ggplot2.tidyverse.org/reference/\nggthemes: https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html",
    "href": "content/bootcamp/stats/class-01.html",
    "title": "Stats Class 01",
    "section": "",
    "text": "Understand probability distributions\nLearn to apply probabilistic thinking to biological data"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#class-objectives",
    "href": "content/bootcamp/stats/class-01.html#class-objectives",
    "title": "Stats Class 01",
    "section": "",
    "text": "Understand probability distributions\nLearn to apply probabilistic thinking to biological data"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#an-episode-in-the-life-of-a-data-scientist",
    "href": "content/bootcamp/stats/class-01.html#an-episode-in-the-life-of-a-data-scientist",
    "title": "Stats Class 01",
    "section": "An Episode in the Life of a Data Scientist*^",
    "text": "An Episode in the Life of a Data Scientist*^\n*All characters are made up\n^All data is simulated\nMatt: “Here is some microscopy data. What insight can you find in it? (Yes it is not RNA and I want you to use R not python. No, my account has not been hacked.)”\nYou: “What is in the data file?”\nMatt: “I measured the radius of gyration (I made up this quantity for cells) and the largest length for a bunch of cells from different cell types.”\nYou get a brand new dataset. You are seeing it for the first time.\nFirst - unboxing\n\n# read dataset\ndata &lt;- read.csv(file = \"data/cell_dimensions.tsv\", sep = \"\\t\", stringsAsFactors = T)\nhead(data)\n\n   Cell_Rg Cell_Len   Cell_Type\n1 2.441927 4.143111 Fibroblasts\n2 2.832714 6.648487 Fibroblasts\n3 3.085169 7.822642 Fibroblasts\n4 3.243857 7.997854 Fibroblasts\n5 3.359737 7.448261 Fibroblasts\n6 3.434748 7.800078 Fibroblasts\n\n\nOk, so Matt gave you a clean file. There are 3 columns -\nCell_Rg - possibly radius of gyration?\nCell_len - possibly largest length?\nCell_Type - self explanatory"
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#how-many-data-points-are-there",
    "href": "content/bootcamp/stats/class-01.html#how-many-data-points-are-there",
    "title": "Stats Class 01",
    "section": "How many data points are there?",
    "text": "How many data points are there?\n\nnrow(data)\n\n[1] 400\n\n\nHow do you get a feel for 400 datapoints? You cannot see trends or get “insights” directly looking at the raw data at this scale. So you need to “summarise the data” to get “Estimates of Location”\n\nsummary(data)\n\n    Cell_Rg          Cell_Len            Cell_Type  \n Min.   : 2.263   Min.   : 0.000   Fibroblasts:100  \n 1st Qu.: 4.503   1st Qu.: 5.407   Muscle     :100  \n Median : 5.477   Median : 7.290   Neuron     :100  \n Mean   : 6.173   Mean   : 6.695   mESC       :100  \n 3rd Qu.: 6.751   3rd Qu.: 8.900                    \n Max.   :15.928   Max.   :12.626                    \n\n\nLet us go through these summary statistics one-by-one.\nMinimum and Maximum - self explanatory\nMedian - midpoint when you sort that column. Let us verify.\nThere are 400 data points. Mid point will be between 200 and 201.\n\na &lt;- sort(data$Cell_Rg)[200]\nb &lt;- sort(data$Cell_Rg)[201]\nprint(c(a, b))\n\n[1] 5.475759 5.478244\n\n(a + b) / 2\n\n[1] 5.477002\n\n\nA more general definition of median is that it is 0.5 quantile (or 50th percentile).\nSo then first quartile (“1st Qu.”) would be 0.25 quantile or 25th percentile, third quartile (“3rd Qu.”) would be 0.75 quantile or 75th percentile. For 400 observations, where will the first and third quartile would be around? We can use quantile function to find out:\n\nquantile(seq(1, 400), 0.25)\n\n   25% \n100.75 \n\nquantile(seq(1, 400), 0.75)\n\n   75% \n300.25 \n\n\nR will interpolate to get the quantile values.\n\nquantile(data$Cell_Rg, 0.25)\n\n     25% \n4.502879 \n\nquantile(data$Cell_Rg, 0.75)\n\n     75% \n6.750579 \n\n\nMean is the average: sum of all observations divided by the number of observations\n\nsum(data$Cell_Rg) / nrow(data)\n\n[1] 6.172667\n\n\nBut this is the mean of all observations. Let us find the mean for each cell type\n\nlibrary(tidyverse)\n\nSorted by mean Rg:\n\ndata %&gt;%\n  group_by(Cell_Type) %&gt;%\n  summarise(mean_Rg = mean(Cell_Rg), mean_len = mean(Cell_Len)) %&gt;%\n  arrange(mean_Rg)\n\n# A tibble: 4 x 3\n  Cell_Type   mean_Rg mean_len\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Neuron         4.85     7.84\n2 Fibroblasts    5.05     6.97\n3 Muscle         7.08    10.1 \n4 mESC           7.72     1.84\n\n\nSorted by mean Cell_Len:\n\ndata %&gt;%\n  group_by(Cell_Type) %&gt;%\n  summarise(mean_Rg = mean(Cell_Rg), mean_len = mean(Cell_Len)) %&gt;%\n  arrange(mean_len)\n\n# A tibble: 4 x 3\n  Cell_Type   mean_Rg mean_len\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 mESC           7.72     1.84\n2 Fibroblasts    5.05     6.97\n3 Neuron         4.85     7.84\n4 Muscle         7.08    10.1 \n\n\nSo mESC has the largest Rg and neuron has the smallest.\nmESC has the smallest length and neuron/muscle have really long lengths.\n(according to the mean…)\nMean could be skewed due to outliers. Median is a better measure in presence of outliers. If we repeat the above analysis with medians:\nSorted by median Rg:\n\ndata %&gt;%\n  group_by(Cell_Type) %&gt;%\n  summarise(median_Rg = median(Cell_Rg), median_len = median(Cell_Len)) %&gt;%\n  arrange(median_Rg)\n\n# A tibble: 4 x 3\n  Cell_Type   median_Rg median_len\n  &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 Neuron           4.86       7.88\n2 Fibroblasts      5.04       6.91\n3 mESC             5.22       1   \n4 Muscle           7.10      10.1 \n\n\nSorted by median Cell_Len:\n\ndata %&gt;%\n  group_by(Cell_Type) %&gt;%\n  summarise(median_Rg = median(Cell_Rg), median_len = median(Cell_Len)) %&gt;%\n  arrange(median_len)\n\n# A tibble: 4 x 3\n  Cell_Type   median_Rg median_len\n  &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 mESC             5.22       1   \n2 Fibroblasts      5.04       6.91\n3 Neuron           4.86       7.88\n4 Muscle           7.10      10.1 \n\n\nHmm, mESC seems to have changed the most. Having extreme values skew the mean is more representative of real data you will come across.\nThis big difference between mean and median is suspicious for mESC. So we need to see how the Rg of mESC is distributed. Histogram bins the data to give us a discretized distribution:\n\n# get mESC Rg:\nmesc_rg &lt;- data[data$Cell_Type == \"mESC\", ]$Cell_Rg\n# plot histogram\nhist(mesc_rg, breaks = 50, freq = FALSE, xlim = c(0, 20))\n\n\n\n\nCan we “smooth” the histogram?\n\nhist(mesc_rg, breaks = 50, freq = FALSE, xlim = c(0, 20))\nlines(density(mesc_rg), xlim = c(0, 20), lwd = 3, col = \"blue\")\n\n\n\n\nSo obviously, there is an outlier population that is skewing the mean, but the median is not as affected by the outliers.\nThis distribution is also telling us that the trend in mean or median doesn’t account for the “spread” of the data. To compare how spread out the data is across the four cell types, we can use boxplots:\n\nlibrary(ggplot2)\nggplot(data) +\n  geom_boxplot(aes(x = Cell_Type, y = Cell_Rg), notch = T) +\n  theme_bw()\n\nNotch went outside hinges\ni Do you want `notch = FALSE`?\n\n\n\n\n\nSo the variability and two sided distribution is hidden by boxplots, violin plots help with that\n\nggplot(data) +\n  geom_violin(aes(x = Cell_Type, y = Cell_Rg)) +\n  geom_boxplot(aes(x = Cell_Type, y = Cell_Rg), width = 0.05) +\n  theme_bw()\n\n\n\n\nWith these exploratory analyses, we can hypothesize that muscle has higher Rg than fibroblasts and neurons. mESC has a complex distribution that needs further experiments, so we won’t analyze that cell type further."
  },
  {
    "objectID": "content/bootcamp/stats/class-01.html#how-significant-are-differences-between-muscle-cells-and-fibroblasts",
    "href": "content/bootcamp/stats/class-01.html#how-significant-are-differences-between-muscle-cells-and-fibroblasts",
    "title": "Stats Class 01",
    "section": "How significant are differences between muscle cells and fibroblasts?",
    "text": "How significant are differences between muscle cells and fibroblasts?\nWe want to ultimately answer the question: Does a given cell type have a different size than others?\nUsing statistical concepts, we can answer this question in many ways:\n\nWe can make an assumption of the underlying probability distribution - this gives us powerful tools to ask how different the size of two cell types are.\nWe could use resampling (class 2)\nWe could use non-parametric statistical tests (class 2)\n\nLet us explore underlying distribution of Rg of muscle cells\n\n# Get muscle Rg as a vector:\nmuscle_rg &lt;- data[data$Cell_Type == \"Muscle\", ]$Cell_Rg\n# generate histogram\nmuscle_rg_hist &lt;- hist(muscle_rg, breaks = 30, freq = F, xlim = c(0, 12))\n\n\n\n\nA probability density function (pdf) is associated with continuous random variable.\n\nA pdf is greater than or equal to zero at all values of x\nA pdf integrates to 1\n\nA normal distribution is an example of a pdf that occurs commonly in the world around us.\nWe can use the normal distribution as an example to understand how to use pdfs.\nLet us assume that the muscle cell Rg is normally distributed with an average of 7 micron and a standard deviation of 1 micron. The ideal distribution would look like this:\n\n# Use seq to create a range of numbers\ncell_size_list &lt;- seq(0, 12, 0.2)\n# Use dnorm to calculate probability at each point of the vector above\ndist_cell_size_muscle &lt;- dnorm(cell_size_list, mean = 7, sd = 1)\nplot(cell_size_list, dist_cell_size_muscle, type = \"l\")\n\n\n\n\nLet us say you fit a normal distribution to your muscle cell Rg data, this is how it might look:\n\nplot(muscle_rg_hist$mids, muscle_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_muscle, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\n\nGoodness of fit\nHow do we see how good a fit this is? We will not go through rigorous measures of goodness of fit in this class, but we will do a quick visual check on how good a fit is using Q-Q plot.\nQ-Q plot compares the quantiles of two distributions. In our case, we have the observed distribution and the theoretical distribution.\nTo generate quantiles of our theoretical distribution, we will use qnorm:\n\ntheoretical_muscle &lt;- qnorm(ppoints(100), mean = 7, sd = 1)\n\nHere is how the theoretical and observed numbers are distributed. We are going to plot the “quintile” on y axis and the Rg on x axis:\n\nplot(y = ppoints(100), x = theoretical_muscle, xlim = c(4, 10), type = \"l\")\nlines(y = ppoints(100), x = sort(muscle_rg), xlim = c(4, 10), type = \"l\", lwd = 2, col = \"red\")\n\n\n\n\nA Q-Q plot compares each point on black and red lines at each quintile:\n\nqqplot(muscle_rg, theoretical_muscle)\nabline(a = 0, b = 1, col = \"blue\")\n\n\n\n\nIf the points fall close to the diagonal, the two distributions are similar.\nPoints either falling below or above the line at the edges indicate that the tails of the distribution don’t agree.\n\n\nImportant note:\nThe two distributions being compared should be scaled to be similar. Here I generated the values of the normal distribution at the same position as that of the observed distribution.\n\n\nWe will next repeat this analysis for fibroblasts:\nSimilarly for fibbroblasts, a normal distribution could explain the underlying distribution of Rg.\nFirst the histogram of observed values:\n\n# Get fibroblast Rg as a vector:\nfib_rg &lt;- data[data$Cell_Type == \"Fibroblasts\", ]$Cell_Rg\n# generate histogram\nfib_rg_hist &lt;- hist(fib_rg, breaks = 30, freq = F, xlim = c(0, 12))\n\n\n\n\nThen the theoretical distribution:\n\ndist_cell_size_fib &lt;- dnorm(cell_size_list, mean = 5, sd = 1)\n# plots:\nplot(fib_rg_hist$mids, fib_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_fib, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\nAnd finally the Q-Q plot:\n\ntheoretical_fib &lt;- qnorm(ppoints(100), mean = 5, sd = 1)\nqqplot(fib_rg, theoretical_fib)\nabline(a = 0, b = 1, col = \"blue\")\n\n\n\n\nThis Q-Q plot looks good as well.\n\nNext class, we will start by asking how we can use these theoretical distributions to figure out if Rg of muscle and fibroblasts are different."
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#provide-a-simple-and-flexible-framework",
    "href": "content/bootcamp/stats/class-03.html#provide-a-simple-and-flexible-framework",
    "title": "Bootcamp: Stats class 3",
    "section": "Provide a simple and flexible framework",
    "text": "Provide a simple and flexible framework"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#variables-definitions",
    "href": "content/bootcamp/stats/class-03.html#variables-definitions",
    "title": "Bootcamp: Stats class 3",
    "section": "Variables definitions",
    "text": "Variables definitions\n\nRandom variables (x, y)\nResponse Variable ( y - aka dependent or outcome variable): this variable is predicted or its variation is explained by the explanatory variable. In an experiment, this is the outcome that is measured following manipulation of the explanatory variable.\nExplanatory Variable ( x - aka independent or predictor variable): explains variations in the response variable. In an experiment, it is manipulated by the researcher.\n\n\nQuantitative Variables\nDiscrete variable: numeric variables that have a countable number of values between any two values - integer in R (e.g., number of mice, read counts).\nContinuous variable: numeric variables that have an infinite number of values between any two values - numeric in R (e.g., normalized expression values, fluorescent intensity).\n\n\nCategorical Variables\nNominal variable: (unordered) random variables have categories where order doesn’t matter - factor in R (e.g., country, type of gene, genotype).\nOrdinal variable: (ordered) random variables have ordered categories - order of levels in R ( e.g. grade of tumor)."
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#hypothesis-testing-definitions",
    "href": "content/bootcamp/stats/class-03.html#hypothesis-testing-definitions",
    "title": "Bootcamp: Stats class 3",
    "section": "Hypothesis testing definitions",
    "text": "Hypothesis testing definitions\nHypothesis testing is a statistical analysis that uses sample data to assess two mutually exclusive theories about the properties of a population. Statisticians call these theories the null hypothesis and the alternative hypothesis. A hypothesis test assesses your sample statistic and factors in an estimate of the sample error to determine which hypothesis the data support.\nWhen you can reject the null hypothesis, the results are statistically significant, and your data support the theory that an effect exists at the population level.\nA legal analogy: Guilty or not guilty?\nThe statistical concept of ‘significant’ vs. ‘not significant’ can be understood by comparing to the legal concept of ‘guilty’ vs. ‘not guilty’.\nIn the American legal system (and much of the world) a criminal defendant is presumed innocent until proven guilty. If the evidence proves the defendant guilty beyond a reasonable doubt, the verdict is ‘guilty’. Otherwise the verdict is ‘not guilty’. In some countries, this verdict is ‘not proven’, which is a better description. A ‘not guilty’ verdict does not mean the judge or jury concluded that the defendant is innocent -- it just means that the evidence was not strong enough to persuade the judge or jury that the defendant was guilty.\nIn statistical hypothesis testing, you start with the null hypothesis (usually that there is no difference between groups). If the evidence produces a small enough P value, you reject that null hypothesis, and conclude that the difference is real. If the P value is higher than your threshold (usually 0.05), you don’t reject the null hypothesis. This doesn’t mean the evidence convinced you that the treatment had no effect, only that the evidence was not persuasive enough to convince you that there is an effect.\nEffect — the difference between the population value and the null hypothesis value. The effect is also known as population effect or the difference. Typically, you do not know the size of the actual effect. However, you can use a hypothesis test to help you determine whether an effect exists and to estimate its size.\nNull Hypothesis or \\(\\mathcal{H}_0\\) — one of two mutually exclusive theories about the properties of the population in hypothesis testing. Typically, the null hypothesis states that there is no effect (i.e., the effect size equals zero).\nAlternative Hypothesis or \\(\\mathcal{H}_1\\) — the other theory about the properties of the population in hypothesis testing. Typically, the alternative hypothesis states that a population parameter does not equal the null hypothesis value. In other words, there is a non-zero effect. If your sample contains sufficient evidence, you can reject the null and favor the alternative hypothesis.\nP-values — the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. Lower p-values represent stronger evidence against the null. P-values in conjunction with the significance level determines whether your data favor the null or alternative hypothesis.\nStatQuest: P Values, clearly explained\nStatQuest: How to calculate p-values\nSignificance Level or \\(a\\) — an evidentiary standard set before the study. It is the probability that you say there is an effect when there is no effect (the probability of rejecting the null hypothesis given that it is true). Lower significance levels indicate that you require stronger evidence before you will reject the null.It is usually set at or below .05.\n\n\n\nGuinness"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#null-hypothesis-testing",
    "href": "content/bootcamp/stats/class-03.html#null-hypothesis-testing",
    "title": "Bootcamp: Stats class 3",
    "section": "Null hypothesis testing",
    "text": "Null hypothesis testing\n\nSpecify the variables\nDeclare null hypothesis \\(\\mathcal{H}_0\\)\nCalculate test-statistic, exact p-value\nGenerate and visualize data reflecting null-distribution\nCalculate the p-value from the test statistic and null distribution\n\n*4-5: For calculating empirical p-value"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#the-simplicity-underlying-common-tests",
    "href": "content/bootcamp/stats/class-03.html#the-simplicity-underlying-common-tests",
    "title": "Bootcamp: Stats class 3",
    "section": "The simplicity underlying common tests",
    "text": "The simplicity underlying common tests\nMost of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. In particular, it all comes down to:\n\\(y = a \\cdot x + b\\)\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model."
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#parametric-vs-non-parametric-tests",
    "href": "content/bootcamp/stats/class-03.html#parametric-vs-non-parametric-tests",
    "title": "Bootcamp: Stats class 3",
    "section": "Parametric vs Non-Parametric tests",
    "text": "Parametric vs Non-Parametric tests\nParametric tests are suitable for normally distributed data.\nNon-Parametric tests are suitable for any continuous data. For the sake of simplicity and sticking with a consistent framework, we will consider Non-Parametric tests as the ranked versions of the corresponding parametric tests.\nMore on choosing Parametric vs Non-Parametric\n\n\n\n\n\nInfo\nParametric\nNon-Parametric\n\n\n\n\nbetter descriptor\nmean\nmedian\n\n\n# of samples (N)\nmany\nfew"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#correlation-vs-regression",
    "href": "content/bootcamp/stats/class-03.html#correlation-vs-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Correlation vs Regression",
    "text": "Correlation vs Regression\nCorrelation is primarily used to quickly and concisely summarize the direction and strength of the relationships between a set of 2 or more numeric variables.\nRegression is primarily used to build models/equations to predict a key response, Y, from a set of predictor (X) variables.\n\n\n\n\n\n\n\n\n\nCorrelation\nRegression\n\n\n\n\nDescription\nAssociation between 2 or more variables\nHow an independent variable is numerically related to the dependent variable\n\n\nUsage\nTo represent linear relationship between two variables\nTo fit a best line and estimate one variable on the basis of another variable\n\n\nDependent vs Independent variables\nDoesn’t matter\nmust define (i.e. order of relationship matters)\n\n\nInterpretation\nCorrelation coefficient indicates the extent to which two variables move together\nRegression indicates the impact of a unit change in the known variable (x) on the estimated variable (y)\n\n\nGoal\nTo find a numerical value expressing the relationship between variables\nTo estimate values of random variable on the basis of the values of fixed variable\n\n\n\n*Borrowed from and more info available here and here.\n\nPearson Correlation\nIt was developed by Karl Pearson from a related idea introduced by Francis Galton in the 1880s, and for which the mathematical formula was derived and published by Auguste Bravais in 1844.[a][6][7][8][9] The naming of the coefficient is thus an example of Stigler’s Law (see list of examples here).\nInterpretation of coefficient:\n1 = perfect linear correlation\n0 = no correlation\n-1 = perfect linear anti-correlation\n\\(Corr(x,y) = \\displaystyle \\frac {\\sum_{i=1}^{n} (x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i=1}^{n} \\sqrt(x_{i} - \\overline{x})^2 \\sqrt(y_{i} - \\overline{y})^2}\\)\n\\(x_{i}\\) = the “i-th” observation of the variable \\(x\\)\n\\(\\overline{x}\\) = mean of all observations of \\(x\\)\n\\(y_{i}\\) = the “i-th” observation of the variable \\(y\\)\n\\(\\overline{y}\\) = mean of all observations of \\(y\\)\n\nr_pearson &lt;- cor(x = biochem$tot_cholesterol, y = biochem$weight)\n\nr_pearson\n\n[1] 0.3540731\n\n# average total cholesterol\navg_chol &lt;- mean(biochem$tot_cholesterol)\n\n# average weight\navg_weight &lt;- mean(biochem$weight)\n\n# difference from mean total cholesterol\ndiff_chol &lt;- biochem$tot_cholesterol - avg_chol\n\n# difference from mean total cholesterol\ndiff_weight &lt;- biochem$weight - avg_weight\n\n# follow formula above\nmanual_pearson &lt;- sum(diff_chol * diff_weight) / (\n  sqrt(sum(diff_chol^2)) * sqrt(sum(diff_weight^2)))\n\nmanual_pearson\n\n[1] 0.3540731\n\nidentical(manual_pearson, r_pearson)\n\n[1] FALSE\n\n# cov(x = biochem$tot_cholesterol, y = biochem$weight)/(sd(biochem$tot_cholesterol)*sd(biochem$weight))\n\n\n\n\nSpearman Correlation (nonparametric)\nSpearman’s rank correlation coefficientor Spearman’s &lt;81&gt;, named after Charles Spearman is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a monotonic function.\nMore info here.\n\nx &lt;- seq(1, 30, 1)\ny &lt;- 2^x\n\nplot(x, y, type = \"l\", las = 2)\n\n\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.5199042\n\ncor(x, y, method = \"spearman\")\n\n[1] 1\n\ncor(x = biochem$tot_cholesterol, y = biochem$weight, method = \"spearman\")\n\n[1] 0.3596281"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#regression",
    "href": "content/bootcamp/stats/class-03.html#regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Regression",
    "text": "Regression"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#equation-for-a-line",
    "href": "content/bootcamp/stats/class-03.html#equation-for-a-line",
    "title": "Bootcamp: Stats class 3",
    "section": "Equation for a line",
    "text": "Equation for a line\nRemember:\n\\(y = a \\cdot x + b\\)\nOR\n\\(y = b + a \\cdot x\\)\n\\(a\\) is the SLOPE\n\\(b\\) is the y-intercept\n\n\n\n\n\n\\(a\\) = 2 (the slope)\n\\(b\\) = 0.5 (the y-intercept)"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#stats-equation-for-a-line",
    "href": "content/bootcamp/stats/class-03.html#stats-equation-for-a-line",
    "title": "Bootcamp: Stats class 3",
    "section": "Stats equation for a line",
    "text": "Stats equation for a line\nModel: the recipe for \\(y\\) is a slope (\\(\\beta_1\\)) times \\(x\\) plus an intercept (\\(\\beta_0\\)).\n\\(y = \\beta_0 + \\beta_1 x \\qquad \\qquad \\mathcal{H}_0: \\beta_1 = 0\\)\n… which is the same has \\(y = a \\cdot x + b\\) (here ordered as \\(y = b + a \\cdot x\\)). In R we are lazy and write y ~ 1 + x which R reads like y = 1*number + x*othernumber and the task of t-tests, lm, etc., is simply to find the numbers that best predict \\(y\\).\nEither way you write it, it’s an intercept (\\(\\beta_0\\)) and a slope (\\(\\beta_1\\)) yielding a straight line:\n\n\n\n\n\n\\(\\beta_0\\) = 0.5 (the y-intercept)\n\\(\\beta_1\\) = 2 (the slope)\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(y = .5 \\cdot 1 + 2 \\cdot x\\)\nOur mission: FIND THE BEST \\(\\beta\\) coefficients"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#linear-regression",
    "href": "content/bootcamp/stats/class-03.html#linear-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nSTEP 1: Make a scatter plot visualize the linear relationship between x and y.\nSTEP 2: Perform the regression\nSTEP 3: Look at the \\(R^2\\), \\(F\\)-value and \\(p\\)-value\nSTEP 4: Visualize fit and errors\nSTEP 5: Calculate \\(R^2\\), \\(F\\)-value and \\(p\\)-value ourselves\n\n\n\nSTEP 1: Can mouse cholesterol levels help explain mouse weight?\nPlot weight (y, response variable) and cholesterol levels (x, explanatory variable) of the mice.\n\nggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size = .5) +\n  scale_color_manual() +\n  theme_minimal()\n\n\n\n\n\n\n\nSTEP 2: Do the regression\nLet’s fit a line (linear model).\nRemember: \\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nlinear model equation: \\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol\\)\n\\(\\mathcal{H}_0:\\) Mouse \\(cholesterol\\) does NOT explain \\(weight\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(weight = \\beta_0 \\cdot 1 + 0 \\cdot cholesterol\\)\n\\(weight = \\beta_0 \\cdot 1\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\nFull model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(\\mathcal{H}_1:\\) Mouse \\(cholesterol\\) does explain \\(weight\\)\n\\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol\\)\nThe cool thing here is that we can assess and compare our null and alternative hypothesis by learning and examining the model coefficients (intercept and slope). Essentially, we are comparing a complex model (including cholesterol) to a simple model (weight).\nhttps://statisticsbyjim.com/regression/interpret-constant-y-intercept-regression/\n\n\nSTEP 4: Look at the \\(R^2\\), \\(F\\)-value and \\(p\\)-value\n\n# fitting a line\nfit_WvC &lt;- lm(\n  data = biochem,\n  formula = weight ~ 1 + tot_cholesterol\n)\n\n\n# base R summary of fit\nsummary(fit_WvC)\n\n\nCall:\nlm(formula = weight ~ 1 + tot_cholesterol, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9624 -2.1349 -0.2627  2.0113 10.2927 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      14.5560     0.3635   40.04   &lt;2e-16 ***\ntot_cholesterol   1.8516     0.1159   15.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 1780 degrees of freedom\nMultiple R-squared:  0.1254,    Adjusted R-squared:  0.1249 \nF-statistic: 255.1 on 1 and 1780 DF,  p-value: &lt; 2.2e-16\n\n\nThat’s a lot of info, but how would I access it? Time to meet your new best friend — Broom\n\n# information about the model fit\nglance(fit_WvC)\n\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.125         0.125  3.12      255. 8.92e-54     1 -4556. 9117. 9134.\n# i 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# information about the intercept and coefficients\ntidy(fit_WvC)\n\n# A tibble: 2 x 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        14.6      0.363      40.0 1.48e-250\n2 tot_cholesterol     1.85     0.116      16.0 8.92e- 54\n\nchol_intercept &lt;- tidy(fit_WvC)[1, 2]\n\nchol_slope &lt;- tidy(fit_WvC)[2, 2]\n# for every 1 unit increase in cholesterol there is a 1.85 unit increase weight\n\n\n# add residuals and other information\naugment(fit_WvC)\n\n# A tibble: 1,782 x 8\n   weight tot_cholesterol .fitted .resid     .hat .sigma     .cooksd .std.resid\n    &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1   20.3            3.01    20.1  0.171 0.000566   3.12 0.000000846     0.0547\n 2   16.1            2.46    19.1 -3.01  0.00107    3.12 0.000501       -0.965 \n 3   19.5            3.57    21.2 -1.67  0.000906   3.12 0.000129       -0.534 \n 4   22.2            2.61    19.4  2.81  0.000853   3.12 0.000347        0.901 \n 5   17.3            2.04    18.3 -1.03  0.00203    3.12 0.000111       -0.331 \n 6   18.1            2.86    19.9 -1.75  0.000622   3.12 0.0000981      -0.561 \n 7   25.6            3.22    20.5  5.08  0.000592   3.12 0.000786        1.63  \n 8   18.6            3.47    21.0 -2.38  0.000782   3.12 0.000228       -0.763 \n 9   23.1            3.35    20.8  2.34  0.000669   3.12 0.000189        0.750 \n10   17.3            2.29    18.8 -1.50  0.00140    3.12 0.000161       -0.480 \n# i 1,772 more rows\n\n# add residuals and other information into the biochem object\nbiochem_WvC &lt;- augment(fit_WvC, data = biochem)\n\nFor every 1 unit increase in \\(cholesterol\\) there is a 1.8516281 increase in \\(weight\\).\n\n\n\nSTEP 5: Visualize fit and errors\n\nggplot(data = biochem_WvC, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size = .5, col = \"white\") +\n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"pink\", size = 1) +\n  geom_smooth(method = lm, col = \"black\", se = F, size = 1, linetype = \"dashed\") +\n  scale_color_manual() +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_point(size = .5, aes(color = .resid)) +\n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"red\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\n\n\n\nSTEP 6: Calculate \\(R^2\\), \\(F\\)-value and \\(p\\)-value ourselves\n\nDEFINITIONS\n\\(SS_{mean}\\) — sum of squared errors around the mean of \\(y\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (data - mean)^2\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (y_{i} - \\overline{y})^2\\)\n\\(Var_{mean}\\) — think of it like the average of the sum of squared error around the mean of \\(y\\)\n\\(Var_{mean} = \\displaystyle \\frac {\\sum_{i=1}^{n} (y_{i} - \\overline{y})^2}{n}\\)\n\\(SS_{fit}\\) — sum of squared errors around the least-squares fit\n\\(SS_{fit} = \\sum_{i=1}^{n} (data - line)^2\\)\n\\(SS_{fit} = \\sum_{i=1}^{n} (y_{i} - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot x)^2\\)\nResiduals, \\(e\\) — the difference between the observed value of the dependent variable \\(y\\) and the predicted value \\(\\widehat{y}\\) is called the residual. Each data point has one residual.\n\\(e = y_{i} - \\widehat{y}\\)\n\\(Var_{fit}\\) — think of it like the average of the sum of squared errors around the least-squares fit\n\\(Var_{fit} = \\displaystyle \\frac {\\sum_{i=1}^{n} (y_{i} - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot x))}{n}\\)\n\nfit_W &lt;- lm(formula = weight ~ 1, data = biochem)\n\nsummary(fit_W)\n\n\nCall:\nlm(formula = weight ~ 1, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3409 -2.4409 -0.4409  2.2591  9.9591 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20.24091    0.07903   256.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.336 on 1781 degrees of freedom\n\nbiochem_W &lt;- augment(fit_W, data = biochem)\n\n\n\nSuper nice way of visualizing\nFirst, let’s look at \\(SS_{mean}\\) for avg weight:\n\np_W &lt;- ggplot(data = biochem_W, aes(x = tot_cholesterol, y = weight)) +\n  geom_hline(yintercept = biochem_W$.fitted, col = \"red\", size = .5) + # plot linear model fit\n  geom_point(size = .5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) + # plot line representing residuals\n  theme_minimal()\n\nNow, let’s look at \\(SS_{fit}\\) for the \\(weight\\) ~ \\(cholesterol\\) :\n\np_WvC &lt;- ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_abline(intercept = pull(chol_intercept), slope = pull(chol_slope), col = \"red\") +\n  geom_point(size = .5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  # guides(color = FALSE) + # no legend for color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\nplot_grid(p_W, p_WvC, ncol = 2, labels = c(\"weight by intercept\", \"weight by cholesterol\"))\n\n\n\n\n\n\n\nFor which mice does the model fit perform the most poorly?\nGotta check residuals!\n\n# make new variable exception = absolute value of resid &gt; 9 then subject id\n\n\n\nbiochem_WvC$exceptions &lt;- if_else(\n  condition = abs(biochem_WvC$.resid) &lt; 9,\n  true = \"\",\n  false = biochem_WvC$subject_name\n)\n\nggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight, label = exceptions)) +\n  geom_point(color = ifelse(biochem_WvC$exceptions == \"\", \"grey50\", \"red\")) +\n  geom_text_repel() +\n  geom_point(size = .5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  # guides(color = FALSE) + # no legend for color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\n\n\n\n\n\nBig Baby and KryptoNate\nNate Robinson aka “KryptoNate”\n5’ 9”\n180 lbs\n\nGlen Davis aka “Big Baby”\n6’ 9”\n280 lbs\n\n\n\\(R^2\\) or coefficient of determination — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\\(R^2 = \\displaystyle \\frac {SS_{mean} - SS_{fit}}{SS_{mean}}\\)\n\\(R^2 = \\displaystyle \\frac {SS_{w} - SS_{wc}}{SS_{w}}\\)\n\nss.fit &lt;- sum(biochem_WvC$.resid^2)\n\n\nss.mean &lt;- sum(biochem_W$.resid^2)\n\n\n# Calc R^2 value\nbiochem_WvC_rsq &lt;- (ss.mean - ss.fit) / ss.mean\nbiochem_WvC_rsq\n\n[1] 0.1253678\n\nglance(fit_WvC) %&gt;% pull(r.squared)\n\n[1] 0.1253678\n\n\n\n\nInterpretation of \\(R^2\\)\nThere is a 13% reduction in the variance when we take mouse \\(cholesterol\\) into account\nOR\nMouse \\(cholesterol\\) explains 13% in player \\(weight\\)\nBy the way, this is the same \\(R\\) as from the Pearson correlation:\n\n# Pearson correlation R value\ncor(biochem$weight, biochem$tot_cholesterol, method = \"pearson\")\n\n[1] 0.3540731\n\n# Pearson correlation R^2 value\ncor(biochem$weight, biochem$tot_cholesterol, method = \"pearson\")^2\n\n[1] 0.1253678\n\n\n\n\n\nThe F-statistic\nF-statistic — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\\(F = \\displaystyle \\frac{SS_{fit}/(p_{fit}-p_{mean})} {SS_{mean}/(n-p_{fit})}\\)\n\\(p_{fit}\\) — number of parameters in the fit line\n\\(p_{mean}\\) — number of parameters in the mean line\n\\(n\\) — number of data points\n\n# F-value\nbiochem_WvC_F &lt;- ((ss.mean - ss.fit) / (2 - 1)) /\n  (ss.fit / (nrow(biochem_WvC) - 2))\n\nbiochem_WvC_F\n\n[1] 255.1411\n\nglance(fit_WvC) %&gt;% pull(statistic)\n\n   value \n255.1411 \n\n\n\n\nP-value from the F-statistic\nWe need to generate a null distribution of \\(F-statistic\\) values to compare to our observed \\(F-statistic\\).\nTherefore, we will randomize the player Height and Weight and then calculate the \\(F-statistic\\).\n\n\nWe will do this many many times to generate a null distribution of \\(F-statistic\\)s.\n\n \n  \nThe p-value will be the probability of obtaining an \\(F-statistic\\) in the null distribution at least as extreme as our observed \\(F-statistic\\).\nAnother beautiful Statquest explaining how to go from F-statistic to p-value\n\n# set up an empty tibble to hold our null distribution\nfake_biochem &lt;- tribble()\n\n\n# sample function to randomize/permute data\nsample(x = 1:10)\n\n [1]  3  8  2  9  1  6  7 10  5  4\n\n# we will perform 100 permutations\nmyPerms &lt;- 100\n\nfor (i in 1:myPerms) {\n  tmp &lt;- bind_cols(\n    biochem_WvC[sample(nrow(biochem_WvC)), \"weight\"],\n    biochem_WvC[sample(nrow(biochem_WvC)), \"tot_cholesterol\"],\n    \"perm\" = factor(rep(i, nrow(biochem_WvC)))\n  )\n\n  fake_biochem &lt;- bind_rows(fake_biochem, tmp)\n  rm(tmp)\n}\n\n\n# let's look at permutations 1 and 2\nggplot(fake_biochem %&gt;% filter(perm %in% c(1:2)), aes(x = weight, y = tot_cholesterol, color = perm)) +\n  geom_point(size = .1) +\n  theme_minimal()\n\n\n\n\n\n\nRemember your best friend\nBROOM\n\n# here we will calculate and extract linear model results for each permutation individualy using nest, mutate, and map functions\n\nfake_biochem_lms &lt;- fake_biochem %&gt;%\n  nest(data = -perm) %&gt;%\n  mutate(\n    fit = map(data, ~ lm(weight ~ tot_cholesterol, data = .x)),\n    glanced = map(fit, glance)\n  ) %&gt;%\n  unnest(glanced)\n\n\n\nLet’s take a look at the null distribution of F-statistics from the randomized values\n\nfake_biochem_lms %&gt;%\n  ggplot(., aes(x = statistic)) +\n  geom_density(color = \"red\") +\n  theme_minimal()\n\n\n\n\nremember that the \\(F-statistic\\) we observed was ~255!\n\nfake_biochem_lms %&gt;%\n  ggplot(., aes(x = statistic)) +\n  xlim(0, biochem_WvC_F * 1.1) +\n  geom_density(color = \"red\") +\n  geom_vline(xintercept = biochem_WvC_F, color = \"blue\") +\n  #  scale_x_log10() +\n  theme_minimal()\n\n\n\nglance(fit_WvC) %&gt;% pull(p.value)\n\n       value \n8.924155e-54 \n\n\n\n\nInterpretation of \\(p-value\\)\nThere is no value more extreme than our observed \\(F-statistic\\).\nTherefore, the empirical \\(p-value &lt; 0.001\\) — empirical what we calculated by randomizing our data.\nThe exact \\(p-value\\) is 8.9241551^{-54} AKA “a ridiculously small number”.\nCorrect — Assuming that \\(cholesterol\\) has zero effect on \\(weight\\) in the population, you’d obtain the sample effect, or larger, in 8.9241551^{-54} AKA “a ridiculously small number” of studies because of random sample error.\nIncorrect —There’s a 8.9241551^{-54} AKA “a ridiculously small number” chance of making a mistake by rejecting the null hypothesis.\nMore on p-value (mis)interpretation\n\n\n\n\nHow to find the best (least squares) fit?\n\nRotate the line of fit\n\nFind the fit that minimizes the Sum of Squared Residuals or \\(SS_{fit}\\)\n\nThis is the derivative (slope of tangent at best point = 0) of the function describing the \\(SS_{fit}\\) and the next rotation is 0.\n\nStatQuest: Fitting a line to data, aka least squares, aka linear regression.\nStatQuest: Gradient Descent, Step-by-Step\n\n\n\nNon-parametric version\n\nTheory: rank-transformation\nrank simply takes a list of numbers and “replace” them with the integers of their rank (1st smallest, 2nd smallest, 3rd smallest, etc.). So the result of the rank-transformation rank(c(3.6, 3.4, -5.0, 8.2)) is 3, 2, 1, 4. See that in the figure above?\nA signed rank is the same, just where we rank according to absolute size first and then add in the sign second. So the signed rank here would be 2, 1, -3, 4. Or in code:\n\nnpfit_WvC &lt;- lm(formula = rank(weight) ~ 1 + rank(tot_cholesterol), data = biochem)\n\ntidy(npfit_WvC)\n\n# A tibble: 2 x 5\n  term                  estimate std.error statistic   p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)            571.      22.8         25.1 4.01e-119\n2 rank(tot_cholesterol)    0.360    0.0221      16.3 1.54e- 55\n\nglance(npfit_WvC) %&gt;% pull(r.squared)\n\n[1] 0.1293324\n\n# cor(biochem$weight, biochem$tot_cholesterol, method = \"spearman\")^2"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#multiple-regression",
    "href": "content/bootcamp/stats/class-03.html#multiple-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Multiple regression",
    "text": "Multiple regression\nRemember:\n\\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nLet’s add an explanatory variable:\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z\\)\nlinear model equation: \\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium\\)\n\\(\\mathcal{H}_0:\\) Mouse \\(cholesterol\\) and \\(sodium\\) does NOT explain \\(weight\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1, \\beta_2 = 0\\)\n\\(weight = \\beta_0 \\cdot 1 + 0 \\cdot cholesterol + 0 \\cdot sodium\\)\n\\(weight = \\beta_0 \\cdot 1\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1,\\beta_2 \\neq 0\\)\n\nFull model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot z\\)\n\\(\\mathcal{H}_1:\\) Mouse \\(cholesterol\\) and \\(sodium\\) does explain \\(weight\\)\n\\(weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot cholesterol + \\beta_2 \\cdot sodium\\)\n\n# how do different variables correlate with weight?\nbiochem %&gt;%\n  select_if(is.numeric) %&gt;%\n  cor() %&gt;%\n  as.data.frame() %&gt;%\n  select(weight) %&gt;%\n  arrange(-weight) %&gt;%\n  rownames()\n\n[1] \"weight\"          \"tot_cholesterol\" \"age\"             \"sodium\"         \n[5] \"glucose\"         \"calcium\"         \"litter\"          \"cage_density\"   \n\n# does sodium + cholesterol predict weight better than cholesterol alone?\nfit_WvC_S &lt;- lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + sodium)\n\n\nfit_WvC_S %&gt;% glance()\n\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.171         0.170  3.04      183. 3.72e-73     2 -4508. 9024. 9046.\n# i 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nbiochem_WvC_S &lt;- augment(fit_WvC_S, data = biochem)\n\nss.fit_C_S &lt;- sum(biochem_WvC_S$.resid^2)\n\nvar.fit_C_S &lt;- ss.fit_C_S / nrow(biochem_WvC_S)\n\n\nss.fit_C &lt;- sum(biochem_WvC$.resid^2)\n\nvar.fit_C &lt;- ss.fit_C / nrow(biochem_WvC)\n\n\n\nfit_Wvall &lt;- lm(data = biochem, formula = weight ~ 1 + tot_cholesterol + age + sodium + glucose + calcium + litter + cage_density)\n\nfit_Wvall %&gt;% glance()\n\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.253         0.250  2.89      85.7 1.33e-107     7 -4415. 8849. 8898.\n# i 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nfit_Wvall %&gt;% tidy()\n\n# A tibble: 8 x 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      -6.53      1.68       -3.90 9.98e- 5\n2 tot_cholesterol   1.97      0.109      18.2  9.08e-68\n3 age               0.133     0.0161      8.28 2.49e-16\n4 sodium            0.179     0.0145     12.4  1.12e-33\n5 glucose           0.159     0.0284      5.59 2.59e- 8\n6 calcium          -5.65      0.692      -8.16 6.13e-16\n7 litter           -0.0557    0.0523     -1.06 2.88e- 1\n8 cage_density     -0.316     0.0648     -4.87 1.22e- 6\n\nggplot(data = biochem, aes(y = weight, x = calcium)) +\n  geom_point(size = .5) +\n  scale_color_manual() +\n  theme_minimal()"
  },
  {
    "objectID": "content/bootcamp/stats/class-03.html#non-linear-regression",
    "href": "content/bootcamp/stats/class-03.html#non-linear-regression",
    "title": "Bootcamp: Stats class 3",
    "section": "Non-linear regression",
    "text": "Non-linear regression\nNon-linear regression — observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.\nDANGER OVERFITTING\n\nPerform loess regression\nLoess regression — a non-parametric technique that uses local weighted regression to fit a smooth curve through points in a scatter plot. LOESS combines much of the simplicity of linear least squares regression with the flexibility of nonlinear regression. It does this by fitting simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point. In fact, one of the chief attractions of this method is that the data analyst is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data.\n\n# local-weighted regression fit\nloessfit_WvC &lt;- loess(formula = weight ~ tot_cholesterol, data = biochem)\n\nloess_biochem_WvC &lt;- augment(loessfit_WvC, biochem)\n\n\nsummary(loessfit_WvC)\n\nCall:\nloess(formula = weight ~ tot_cholesterol, data = biochem)\n\nNumber of Observations: 1782 \nEquivalent Number of Parameters: 5.24 \nResidual Standard Error: 3.112 \nTrace of smoother matrix: 5.73  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate     cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n\nsummary(fit_WvC)\n\n\nCall:\nlm(formula = weight ~ 1 + tot_cholesterol, data = biochem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9624 -2.1349 -0.2627  2.0113 10.2927 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      14.5560     0.3635   40.04   &lt;2e-16 ***\ntot_cholesterol   1.8516     0.1159   15.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 1780 degrees of freedom\nMultiple R-squared:  0.1254,    Adjusted R-squared:  0.1249 \nF-statistic: 255.1 on 1 and 1780 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nPlot loess fit depicting residuals\n\nggplot(data = loess_biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_smooth(method = lm, col = \"black\", se = F, size = .25, linetype = \"dashed\") + # linear fit black dashed line\n  geom_smooth(method = loess, col = \"red\", se = F, size = .25) + # loess fit red line\n  geom_point(size = .5, aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") +\n  # guides(color = FALSE) +\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html",
    "href": "content/bootcamp/stats/class-02.html",
    "title": "Stats Class 02",
    "section": "",
    "text": "Understand how to determine if two samples come from different distributions\n\nUsing theoretical distributions\nUsing resampling/bootstrap\nUsing non-parametric tests\n\nLearn how to determine if overlap of two categories is significant"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#class-objectives",
    "href": "content/bootcamp/stats/class-02.html#class-objectives",
    "title": "Stats Class 02",
    "section": "",
    "text": "Understand how to determine if two samples come from different distributions\n\nUsing theoretical distributions\nUsing resampling/bootstrap\nUsing non-parametric tests\n\nLearn how to determine if overlap of two categories is significant"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#continuing-from-last-class",
    "href": "content/bootcamp/stats/class-02.html#continuing-from-last-class",
    "title": "Stats Class 02",
    "section": "Continuing from last class",
    "text": "Continuing from last class\nLet us start by reading in the dataset:\n\n# read dataset\ndata &lt;- read.csv(file = \"data/cell_dimensions.tsv\", sep = \"\\t\", stringsAsFactors = T)\nhead(data)\n\n   Cell_Rg Cell_Len   Cell_Type\n1 2.441927 4.143111 Fibroblasts\n2 2.832714 6.648487 Fibroblasts\n3 3.085169 7.822642 Fibroblasts\n4 3.243857 7.997854 Fibroblasts\n5 3.359737 7.448261 Fibroblasts\n6 3.434748 7.800078 Fibroblasts\n\n\nWe generated observed and theoretical distributions of Rg. First muscle:\n\n# Get muscle Rg as a vector:\nmuscle_rg &lt;- data[data$Cell_Type == \"Muscle\", ]$Cell_Rg\n# generate histogram\nmuscle_rg_hist &lt;- hist(muscle_rg, breaks = 30, plot = F)\n# Use seq to create a range of numbers\ncell_size_list &lt;- seq(0, 12, 0.2)\n# Use dnorm to calculate probability at each point of the vector above\ndist_cell_size_muscle &lt;- dnorm(cell_size_list, mean = 7, sd = 1)\nplot(muscle_rg_hist$mids, muscle_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_muscle, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\nNext, fibroblasts:\n\n# Get Fib Rg as a vector:\nfib_rg &lt;- data[data$Cell_Type == \"Fibroblasts\", ]$Cell_Rg\n# generate histogram\nfib_rg_hist &lt;- hist(fib_rg, breaks = 30, plot = F)\n# Use dnorm to calculate probability at each point of the cell_size_list vector\ndist_cell_size_fib &lt;- dnorm(cell_size_list, mean = 5, sd = 1)\nplot(fib_rg_hist$mids, fib_rg_hist$density, xlim = c(0, 12), ylim = c(0, 0.5))\nlines(cell_size_list, dist_cell_size_fib, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5))\n\n\n\n\nWe are confident that Rg is sampled from Normal distributions, so we can take advantage of knowing exact details of the theoretical distributions. We could work out the math using formulae, or we could sample the distribution to get an intuitive feel.\nThe question we are asking is: Is the Rg of muscle different from Rg of fibroblasts?\nWe now know that the Rg is sampled from normal distribution as follows:\n\nMuscle - mean = 7, sd = 1\nFibroblasts - mean = 5, sd = 1\n\nThe advantage of having a theoretical distribution is that we can calculate exact differences. We will use 10,000 points sampled from each distribution to illustrate this. Let us generate 10,000 points from these distributions:\n\ntheoretical_fib &lt;- rnorm(10000, mean = 5, sd = 1)\ntheoretical_muscle &lt;- rnorm(10000, mean = 7, sd = 1)\n\nThe difference in Rg between the two cell types would be simply:\n\ndiff_cell_Rg &lt;- theoretical_muscle - theoretical_fib\n\nWhat is the distribution of the difference?\n\nhist(diff_cell_Rg, breaks = 100, freq = F)\n\n\n\n\nThe difference is also a Normal distribution!\nThe mean of this distribution is simply difference in mean of the starting distributions: 7-5 = 2. The sd is the square root of sum of squares of the sd of the starting distributions:\n\nsqrt(1^2 + 1^2)\n\n[1] 1.414214\n\n\nLet us see if this is the case, by overlaying the theoretical normal distribution over the histogram of differences:\n\nhist(diff_cell_Rg, breaks = 100, freq = F, xlim = c(-3, 10))\nlines(x = seq(-3, 10, 0.1), y = dnorm(seq(-3, 10, 0.1), m = 2, sd = 1.414), lwd = 3, col = \"red\", xlim = c(-3, 10))\n\n\n\n\nThe probability that muscle has larger Rg than fibroblasts is the probability that the difference is &gt; 0.\nWe can use the theoritical distribution to figure this out:\n\npnorm(0, mean = 2, sd = 1.414, lower.tail = F)\n\n[1] 0.9213817\n\n\nSo 92% of muscle cells will have bigger Rg than fibroblasts.\nNow, here again are the theeoretical distributions of the two cell types:\n\nplot(cell_size_list, dist_cell_size_muscle, col = \"red\", xlim = c(0, 12), ylim = c(0, 0.5), type = \"l\")\nlines(cell_size_list, dist_cell_size_fib, col = \"blue\", xlim = c(0, 12), ylim = c(0, 0.5))\nabline(v = 7, col = \"red\")\nabline(v = 5, col = \"blue\")\n\n\n\n\nTo ask if muscle has significantly higher Rg, you could ask, what is the probability of seeing an Rg in fibroblasts that this higher than mean Rg of muscle:\n\npnorm(7, mean = 5, sd = 1, lower.tail = F)\n\n[1] 0.02275013\n\n\n2.2% of fibroblasts have Rg greater than or equal to the mean Rg of muscle.\n(Is this the p-value?)\nBelow we will go through two methods to determine the statistical significance, or p-value of this comparison."
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#bootstrapping",
    "href": "content/bootcamp/stats/class-02.html#bootstrapping",
    "title": "Stats Class 02",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nTo get statistical significance, we ask: “what is the probability of the observed measure occuring in a null distribution?”\nWe could either use a theoretical null distribution or create our own using existing data.\nWhat if we didn’t know the underlying distribution of the data or we didn’t care? Biological systems are complex and many times, the standard theoretical distributions will not capture the observed distribution.\nWith increasing number of data points and increasing computing power, we don’t need to rely on theoretical distributions.\nWe can create our own multiverse (a.k.a the null distribution) with resampling/bootstrapping.\nHere are the steps involved in bootstrapping:\n\nWe will aggregate our observations into a new vector. This is the starting point of creating our multiverse.\n\n\nall_obs &lt;- c(muscle_rg, fib_rg)\n\n\nWhen we sample 100 observations from “all_obs” randomly, we create a new Rg dataset sampled from the combined data:\n\n\ntmp_muscle_rg &lt;- sample(all_obs, 100, replace = T)\ntmp_fib_rg &lt;- sample(all_obs, 100, replace = T)\n\nWe could do this without replacement too:\n\nidx_b &lt;- sample(1:200, 100)\nidx_a &lt;- setdiff(1:200, idx_b)\ntmp_muscle_rg &lt;- all_obs[idx_a]\ntmp_fib_rg &lt;- all_obs[idx_b]\n\n\nWe then calculate the difference in mean of the two sampled datasets. Here, we just performed an experiment where we sampled two sets of Rg from a single distribution and calculated the differences in mean between the two sets.\n\n\ntmp_diff &lt;- mean(tmp_muscle_rg) - mean(tmp_fib_rg)\ntmp_diff\n\n[1] -0.0470043\n\n\n\nWe repeat this many times (here let us do 1000 repetitions) to get the null distribution of the differences in mean. The null distribution assumes that the Rg for muscle and fibroblasts were drawn from the same distribution:\n\n\n# initialize a vector with 0s\nbs_mean_diff &lt;- rep(0, 1000)\nfor (i in 1:1000) {\n  tmp_muscle_rg &lt;- sample(all_obs, 100, replace = T)\n  tmp_fib_rg &lt;- sample(all_obs, 100, replace = T)\n  bs_mean_diff[i] &lt;- mean(tmp_muscle_rg) - mean(tmp_fib_rg)\n  # if you want to sample without replacement:\n  # idx_b &lt;- sample(1:200,100)\n  # idx_a &lt;- setdiff(1:200,idx_b)\n  # bs_mean_diff[i] = mean(all_obs[idx_a]) - mean(all_obs[idx_b])\n}\n\nLet us see the distribution of the mean difference of random sampling of Rg with the observed difference in red:\n\nhist(bs_mean_diff, freq = F, xlim = c(-3, 3))\nabline(v = mean(muscle_rg) - mean(fib_rg), col = \"red\", lwd = 3, xlim = c(-3, 3))\n\n\n\n\nThe observed difference is way higher than anything we saw in 1000 samples from the comobined Rg’s.\nHow often do we see a mean difference in sampling that is higher than observed?\n\nmean(bs_mean_diff &gt; (mean(muscle_rg) - mean(fib_rg)))\n\n[1] 0\n\n\nNever! So the difference in mean Rg we observe between muscle and fibroblasts is highly statistically significant. In other words, p=0\nBootstrapping requires no assumptions and can be applied to any dataset succesfully to determine if the same measure from two samples are significantly different.\nLet us do the same compaarison between fibroblasts and neurons:\n\nneuron_rg &lt;- data[data$Cell_Type == \"Neuron\", ]$Cell_Rg\nall_obs &lt;- c(neuron_rg, fib_rg)\nbs_mean_diff &lt;- rep(0, 10000)\nfor (i in 1:10000) {\n  # tmp_neuron_rg &lt;- sample(all_obs,100,replace = T)\n  # tmp_fib_rg &lt;- sample(all_obs,100,replace = T)\n  # bs_mean_diff[i] = mean(tmp_neuron_rg) - mean(tmp_fib_rg)\n  idx_b &lt;- sample(1:200, 100)\n  idx_a &lt;- setdiff(1:200, idx_b)\n  bs_mean_diff[i] &lt;- mean(all_obs[idx_a]) - mean(all_obs[idx_b])\n}\nhist(bs_mean_diff, freq = F, xlim = c(-1, 1))\nabline(v = mean(neuron_rg) - mean(fib_rg), col = \"red\", lwd = 3, xlim = c(-1, 1))\n\n\n\n\n\n# mean(neuron_rg)-mean(fib_rg)\nmean(bs_mean_diff &lt; (mean(neuron_rg) - mean(fib_rg)))\n\n[1] 0.0771\n\n\nHere, the probability of the difference in mean observed in the null distribution is 0.0824, higher than accepted level of significance (0.05)."
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#non-parametric-tests",
    "href": "content/bootcamp/stats/class-02.html#non-parametric-tests",
    "title": "Stats Class 02",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nAnother way to ask if the two samples are drawn from different distributions is to use non parametric statistical tests that have inbuilt null distributions. The two that will be useful in a wide range of scenarios are Kolmogrov Smirnov test and Wilcoxon test, both of which are easy to implement in R.\nThe “p.value” variable gives the probability that the two samples were drawn from the same underlying distribution. Hence, if p.value is less than 0.05 (or a more stringent cutoff), it means the two samples are significantly different.\nKS test:\n\nks.test(muscle_rg, fib_rg)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  muscle_rg and fib_rg\nD = 0.7, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\nks.test(neuron_rg, fib_rg)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  neuron_rg and fib_rg\nD = 0.1, p-value = 0.6994\nalternative hypothesis: two-sided\n\n\nWilcoxon test:\n\nwilcox.test(muscle_rg, fib_rg)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  muscle_rg and fib_rg\nW = 9227, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\nwilcox.test(neuron_rg, fib_rg)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  neuron_rg and fib_rg\nW = 4444, p-value = 0.1747\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "content/bootcamp/stats/class-02.html#overlaps",
    "href": "content/bootcamp/stats/class-02.html#overlaps",
    "title": "Stats Class 02",
    "section": "Overlaps",
    "text": "Overlaps\nWe come across the question of overlaps quite a bit in biological datasets.\nA typical question would be: “Does my list of upregulated genes overlap with [my favorite] category of genes?”\n(Or the dreaded Reviewer comment “You should do a GO analysis” which means you have to add a supplementary figure no one cares about.)\nLet us take an experiment where you measured change in gene expression and categorized each gene as either “Upregulaated” or “Not_Upregulated”.\nYou hypothesize that your intervention affects cell cycle genes, so you get a list of cell cycle genes from a database and categorize each gene in your gene list as either cell cycle gene or not.\nIf the two categories have nothing to do with each other, then they co-occur just by chance. They would be “independent” of each other and we can calculate the joint probability for such a scenario as follows:\nIf Cell_Cycle and Upregulation are independent:\nP(Cell_Cycle \\(\\bigcap\\) Upregulation) = P(Cell_Cycle)*P(Upregulation)\nWe now have an observed probability of intetrsection from your experiment. So we can determine the ratio of observed over expected:\nP(Cell_Cycle \\(\\bigcap\\) Upregulation) / ( P(Cell_Cycle)*P(Upregulation) )\nMore the ratio greater than 1, more significant the intersection between the two categories.\nLet us look at such an experiment:\n\noverlap_dat &lt;- read.csv(file = \"data/Gene_Association.csv\", sep = \",\")\nhead(overlap_dat)\n\n  X...Gene Cell_Cycle_Gene  Exp_Change\n1   Gene_1      Cell_Cycle Upregulated\n2   Gene_2      Cell_Cycle Upregulated\n3   Gene_3      Cell_Cycle Upregulated\n4   Gene_4      Cell_Cycle Upregulated\n5   Gene_5      Cell_Cycle Upregulated\n6   Gene_6      Cell_Cycle Upregulated\n\n\nLet us calculate individual probabilities and joint probability:\n\nct_total &lt;- nrow(overlap_dat)\nct_cell_cycle &lt;- nrow(overlap_dat[overlap_dat$Cell_Cycle_Gene == \"Cell_Cycle\", ])\n\np_cell_cycle &lt;- ct_cell_cycle / ct_total\n\nct_upreg &lt;- nrow(overlap_dat[overlap_dat$Exp_Change == \"Upregulated\", ])\n\np_upreg &lt;- ct_upreg / ct_total\n\nct_both &lt;- nrow(overlap_dat[overlap_dat$Cell_Cycle_Gene == \"Cell_Cycle\" &\n  overlap_dat$Exp_Change == \"Upregulated\", ])\np_joint &lt;- ct_both / ct_total\n\nnoquote(paste0(\"p_cell_cycle \", p_cell_cycle))\n\n[1] p_cell_cycle 0.15\n\nnoquote(paste0(\"p_upreg \", p_upreg))\n\n[1] p_upreg 0.19\n\nnoquote(paste0(\"p_joint \", p_joint))\n\n[1] p_joint 0.11\n\nexcess &lt;- p_joint / (p_cell_cycle * p_upreg)\nnoquote(paste0(\"Observed/Expected = \", excess))\n\n[1] Observed/Expected = 3.85964912280702\n\n\nIs this statistically significant?\nWe will use the Fisher exact test* to get the significance of this observation, or what is the probability of observing this overlap from two randomly distributed variables.\n*(chi squared test and hypergeometric test can also be used)\nFirst we will generate a contingency table:\n\nc_table &lt;- table(overlap_dat$Cell_Cycle_Gene, overlap_dat$Exp_Change)\nc_table\n\n                \n                 Not_Upregulated Upregulated\n  Cell_Cycle                   4          11\n  Not_Cell_Cycle              77           8\n\n\nYou can see that off diagonal seems enriched already.\n\nfisher.test(c_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  c_table\np-value = 5.137e-07\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.007467239 0.172362221\nsample estimates:\nodds ratio \n0.04022862 \n\n\nThe overlap is highly significant.\n\nUsing bootstrap\nWe can estimate significance alternatively using bootstrapping:\n\n# make a copy of the overlap_dat dataframe so we can shuffle\n# the cell_cycle column each repetition of the bootstrap.\n\ntemp_overlap_df &lt;- overlap_dat\n\n# initialize vector that will contain number of genes that\n# have both a cell cycle category and are upregulated\n\noverlap_ct &lt;- rep(0, 1000)\nfor (i in 1:1000) {\n  # shuffle Cell_Cycle_Gene column\n  temp_overlap_df$Cell_Cycle_Gene &lt;- sample(overlap_dat$Cell_Cycle_Gene, nrow(overlap_dat))\n  # count overlap\n  overlap_ct[i] &lt;- nrow(temp_overlap_df[temp_overlap_df$Cell_Cycle_Gene == \"Cell_Cycle\" &\n    temp_overlap_df$Exp_Change == \"Upregulated\", ])\n}\n\nWe have generated a distribution of number of genes that would have overlapping cell cycle category and upregulation category by chance. Now let us compare the distribution with what we observe:\n\nhist(overlap_ct, freq = F, xlim = c(0, 12), breaks = 10)\n\nabline(v = ct_both, col = \"red\", lwd = 3, xlim = c(0, 12))\n\n\n\n\nAnd the p-value from bootstrapping:\n\nmean(overlap_ct &gt; ct_both)\n\n[1] 0\n\n\nSo the bootstrap p-value is 0."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#genomics---lots-of-data---lots-of-hypothesis-tests",
    "href": "content/bootcamp/stats/class-05.html#genomics---lots-of-data---lots-of-hypothesis-tests",
    "title": "Bootcamp: Stats class 5",
    "section": "Genomics -> Lots of Data -> Lots of Hypothesis Tests",
    "text": "Genomics -&gt; Lots of Data -&gt; Lots of Hypothesis Tests\nIn a typical RNA-seq experiment, we test ~10K different hypotheses. For example, you have 10K genes and for each gene you test whether the mean expression changed in condition A vs condition B. Using a standard p-value cut-off of 0.05, we’d expect 500 genes to be deemed “significant” by chance. Thus, we concerned about False Positives or Type I Errors.\n\nSo, we want to control our type 1 error. We can approach this in two different ways.\n\nControl overall  (also known as family-wise error rate or FWER), which will affect the * for each test. That is, we are controlling the overall probability of making at least one false discovery. Bonferroni and Sidak corrections all control FWER.\nControl false discovery rate (FDR). Where FWER controls for the probability for making a type 1 error at all, these procedures allow for type 1 errors (false positives) but control the proportion of these false positives in relation to true positives. This is done by adjusting the decision made for the p-value associated with each individual test to decide rejection or not. Because this will result in a higher type 1 error rate, it has higher power. This affords a higher probability of true discoveries. The step procedures control for FDR.\n\n\nBonferroni Correction\n**The most conservative of corrections, the Bonferroni correction is also perhaps the most straightforward in its approach. Simply divide  by the number of tests (m).\nHowever, with many tests, * will become very small. This reduces power, which means that we are very unlikely to make any true discoveries.\n\n\nSidak Correction\n*** = 1-(1-)^(1/m)\n\n\nHolm’s Step-Down Procedure\n**An update of the Bonferroni correction, this procedure is more powerful. Rather than controlling the FWER, Holm’s procedure controls for the false discovery rate (FDR) and is performed after conducting all hypothesis tests and finding associated p-values at  within a set.\nThe step-down procedure is best illustrated with an example. Say we have three hypotheses, each with the associated p-value:\nH1: 0.025\nH2: 0.003\nH3: 0.01\nStep 1: Order p-values from smallest to greatest\nH2: 0.003\nH3: 0.01\nH1: 0.025\nStep 2: Use the Holm-Bonferroni formula for the first-ranked (smallest) p-value\n&lt;ce&gt;&lt;b1&gt;* = &lt;ce&gt;&lt;b1&gt;/(n-rank+1)\n* = 0.05/(3–1+1) = 0.0167\nStep 3: Compare the first-ranked p-value with the * calculated from Step 2\n0.003 &lt; 0.0167\nBecause the p-value for H2 is less than the calculated *, we can reject H2.\nMove onto the next ranked p-value and repeat steps 2–3, calculating the * for its respective rank and comparing it to that p-value. Continue until you reach the first non-rejected hypothesis. You would then fail to reject all following hypotheses.\n\n\nHochberg’s Step-Up Procedure\n**More powerful than Holm’s step-down procedure, Hochberg’s step-up procedure also seeks to control the FDR and follows a similar process, only p-values are ranked from largest to smallest.\nFor each ranked p-value, it is compared to the * calculated for its respective rank (same formula as Holm’s procedure). Testing continues until you reach the first non-rejected hypothesis. You would then fail to reject all following hypotheses.\n\nrna &lt;- read_csv(file = \"data/deltaRNA_test.csv\", show_col_types = FALSE) %&gt;%\n  select(gene_id, pvalue) %&gt;%\n  na.omit()\n\nrna$fdr &lt;- p.adjust(p = rna$pvalue, method = \"fdr\", n = nrow(rna))\n\nrna$BH &lt;- p.adjust(p = rna$pvalue, method = \"BH\", n = nrow(rna))\n\nrna$bon &lt;- p.adjust(p = rna$pvalue, method = \"bonferroni\", n = nrow(rna))\n\n\nrna_long &lt;- rna %&gt;% pivot_longer(cols = pvalue:bon, names_to = \"type\")\n\n\n\np_none &lt;- ggplot(data = rna, aes(x = pvalue, y = pvalue)) +\n  geom_point(size = .1) +\n  ggtitle(\"None\") +\n  theme_minimal()\n\n\np_FDR &lt;- ggplot(data = rna, aes(x = pvalue, y = fdr)) +\n  geom_point(size = .1) +\n  ggtitle(\"FDR\") +\n  theme_minimal()\n\np_BH &lt;- ggplot(data = rna, aes(x = pvalue, y = BH)) +\n  geom_point(size = .1) +\n  ggtitle(\"BH\") +\n  theme_minimal()\n\np_bon &lt;- ggplot(data = rna, aes(x = pvalue, y = bon)) +\n  geom_point(size = .1) +\n  ggtitle(\"Bonferroni\") +\n  theme_minimal()\n\nplot_grid(p_none, p_bon, p_BH, p_FDR, ncol = 2, nrow = 2)\n\n\n\nggplot(data = rna_long, aes(x = value, color = type)) +\n  stat_ecdf() +\n  theme_minimal() +\n  xlab(\"p-values\") +\n  ylab(\"cumulative fraction\")"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#bayes-rule",
    "href": "content/bootcamp/stats/class-05.html#bayes-rule",
    "title": "Bootcamp: Stats class 5",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nThe conditional probability of the event \\(A\\) conditional on the event \\(B\\) is given by \\[\n  P(A \\mid B) = \\frac{P(A \\,\\&\\, B)}{P(B)}.\n\\]\nThis section introduces how the Bayes’ rule is applied to calculating conditional probability, and several real-life examples are demonstrated. Finally, we compare the Bayesian and frequentist definition of probability.\n\nBayes’ Rule and Diagnostic Testing\nTo better understand conditional probabilities and their importance, let us consider an example involving the human immunodeficiency virus (HIV). In the early 1980s, HIV had just been discovered and was rapidly expanding. There was major concern with the safety of the blood supply. Also, virtually no cure existed making an HIV diagnosis basically a death sentence, in addition to the stigma that was attached to the disease.\nThese made false positives and false negatives in HIV testing highly undesirable. A false positive is when a test returns postive while the truth is negative. That would for instance be that someone without HIV is wrongly diagnosed with HIV, wrongly telling that person they are going to die and casting the stigma on them. A false negative is when a test returns negative while the truth is positive. That is when someone with HIV undergoes an HIV test which wrongly comes back negative. The latter poses a threat to the blood supply if that person is about to donate blood.\nThe probability of a false positive if the truth is negative is called the false positive rate. Similarly, the false negative rate is the probability of a false negative if the truth is positive. Note that both these rates are conditional probabilities: The false positive rate of an HIV test is the probability of a positive result conditional on the person tested having no HIV.\nThe HIV test we consider is an enzyme-linked immunosorbent assay, commonly known as an ELISA. We would like to know the probability that someone (in the early 1980s) has HIV if ELISA tests positive. For this, we need the following information. ELISA’s true positive rate (one minus the false negative rate), also referred to as sensitivity, recall, or probability of detection, is estimated as\n\\[\n  P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 93\\% = 0.93.\n\\]\nIts true negative rate (one minus the false positive rate), also referred to as specificity, is estimated as\n\\[\n  P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV}) = 99\\% = 0.99.\n\\]\nAlso relevant to our question is the prevalence of HIV in the overall population, which is estimated to be 1.48 out of every 1000 American adults. We therefore assume\n\\[\\begin{equation}\n  P(\\text{Person tested has HIV}) = \\frac{1.48}{1000} = 0.00148.\n  (\\#eq:HIVpositive)\n\\end{equation}\\]\nNote that the above numbers are estimates. For our purposes, however, we will treat them as if they were exact.\nOur goal is to compute the probability of HIV if ELISA is positive, that is \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})\\). In none of the above numbers did we condition on the outcome of ELISA. Fortunately, Bayes’ rule allows is to use the above numbers to compute the probability we seek. Bayes’ rule states that\n\\[\\begin{equation}\n  P(\\text{Person tested has HIV}  \\mid \\text{ELISA is positive}) = \\frac{P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive})}{P(\\text{ELISA is positive})}.\n   (\\#eq:HIVconditional)\n\\end{equation}\\]\nThis can be derived as follows. For someone to test positive and be HIV positive, that person first needs to be HIV positive and then secondly test positive. The probability of the first thing happening is \\(P(\\text{HIV positive}) = 0.00148\\). The probability of then testing positive is \\(P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) = 0.93\\), the true positive rate. This yields for the numerator\n\\[\\begin{multline}\n  P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has HIV}) \\\\\n  &= 0.00148 \\cdot 0.93\n  = 0.0013764.\n  \\end{split}\n  (\\#eq:HIVjoint)\n\\end{multline}\\]\nThe first step in the above equation is implied by Bayes’ rule: By multiplying the left- and right-hand side of Bayes’ rule by \\(P(B)\\), we obtain \\[\n  P(A \\mid B) P(B) = P(A \\,\\&\\, B).\n\\]\nThe denominator in @ref(eq:HIVconditional) can be expanded as\n\\[\\begin{multline*}\n  P(\\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has HIV} \\,\\&\\, \\text{ELISA is positive}) + P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  &= 0.0013764 + 0.0099852 = 0.0113616\n  \\end{split}\n\\end{multline*}\\]\nwhere we used @ref(eq:HIVjoint) and\n\\[\\begin{multline*}\n  P(\\text{Person tested has no HIV} \\,\\&\\, \\text{ELISA is positive}) \\\\\n  \\begin{split}\n  &= P(\\text{Person tested has no HIV}) P(\\text{ELISA is positive} \\mid \\text{Person tested has no HIV}) \\\\\n  &= \\left(1 - P(\\text{Person tested has HIV})\\right) \\cdot \\left(1 - P(\\text{ELISA is negative} \\mid \\text{Person tested has no HIV})\\right) \\\\\n  &= \\left(1 - 0.00148\\right) \\cdot \\left(1 - 0.99\\right) = 0.0099852.\n  \\end{split}\n\\end{multline*}\\]\nPutting this all together and inserting into @ref(eq:HIVconditional) reveals \\[\\begin{equation}\n  P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) = \\frac{0.0013764}{0.0113616} \\approx 0.12.\n  (\\#eq:HIVresult)\n\\end{equation}\\] So even when the ELISA returns positive, the probability of having HIV is only 12%. An important reason why this number is so low is due to the prevalence of HIV. Before testing, one’s probability of HIV was 0.148%, so the positive test changes that probability dramatically, but it is still below 50%. That is, it is more likely that one is HIV negative rather than positive after one positive ELISA test.\nQuestions like the one we just answered (What is the probability of a disease if a test returns positive?) are crucial to make medical diagnoses. As we saw, just the true positive and true negative rates of a test do not tell the full story, but also a disease’s prevalence plays a role. Bayes’ rule is a tool to synthesize such numbers into a more useful probability of having a disease after a test result.\n\nWhat is the probability that someone who tests positive does not actually have HIV?\n\nWe found in @ref(eq:HIVresult) that someone who tests positive has a \\(0.12\\) probability of having HIV. That implies that the same person has a \\(1-0.12=0.88\\) probability of not having HIV, despite testing positive.\n\nIf the individual is at a higher risk for having HIV than a randomly sampled person from the population considered, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n\nIf the person has a priori a higher risk for HIV and tests positive, then the probability of having HIV must be higher than for someone not at increased risk who also tests positive. Therefore, \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &gt; 0.12\\) where \\(0.12\\) comes from @ref(eq:HIVresult).\nOne can derive this mathematically by plugging in a larger number in @ref(eq:HIVpositive) than 0.00148, as that number represents the prior risk of HIV. Changing the calculations accordingly shows \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &gt; 0.12\\).\n\nIf the false positive rate of the test is higher than 1%, how, if at all, would you expect $P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive})$ to change?\n\nIf the false positive rate increases, the probability of a wrong positive result increases. That means that a positive test result is more likely to be wrong and thus less indicative of HIV. Therefore, the probability of HIV after a positive ELISA goes down such that \\(P(\\text{Person tested has HIV} \\mid \\text{ELISA is positive}) &lt; 0.12\\).\n\n\nBayes Updating\nIn the previous section, we saw that one positive ELISA test yields a probability of having HIV of 12%. To obtain a more convincing probability, one might want to do a second ELISA test after a first one comes up positive. What is the probability of being HIV positive if also the second ELISA test comes back positive?\nTo solve this problem, we will assume that the correctness of this second test is not influenced by the first ELISA, that is, the tests are independent from each other. This assumption probably does not hold true as it is plausible that if the first test was a false positive, it is more likely that the second one will be one as well. Nonetheless, we stick with the independence assumption for simplicity.\nIn the last section, we used \\(P(\\text{Person tested has HIV}) = 0.00148\\), see @ref(eq:HIVpositive), to compute the probability of HIV after one positive test. If we repeat those steps but now with \\(P(\\text{Person tested has HIV}) = 0.12\\), the probability that a person with one positive test has HIV, we exactly obtain the probability of HIV after two positive tests. Repeating the maths from the previous section, involving Bayes’ rule, gives\n\\[\\begin{multline}\n  P(\\text{Person tested has HIV} \\mid \\text{Second ELISA is also positive}) \\\\\n  \\begin{split}\n  &= \\frac{P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Person tested has HIV})}{P(\\text{Second ELISA is also positive})} \\\\\n  &= \\frac{0.12 \\cdot 0.93}{\n  \\begin{split}\n  &P(\\text{Person tested has HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has HIV}) \\\\\n  &+ P(\\text{Person tested has no HIV}) P(\\text{Second ELISA is positive} \\mid \\text{Has no HIV})\n  \\end{split}\n  } \\\\\n  &= \\frac{0.1116}{0.12 \\cdot 0.93 + (1 - 0.12)\\cdot (1 - 0.99)} \\approx 0.93.\n  \\end{split}\n  (\\#eq:Bayes-updating)\n\\end{multline}\\]\nSince we are considering the same ELISA test, we used the same true positive and true negative rates as in Section @ref(sec:diagnostic-testing). We see that two positive tests makes it much more probable for someone to have HIV than when only one test comes up positive.\nThis process, of using Bayes’ rule to update a probability based on an event affecting it, is called Bayes’ updating. More generally, the what one tries to update can be considered ‘prior’ information, sometimes simply called the prior. The event providing information about this can also be data. Then, updating this prior using Bayes’ rule gives the information conditional on the data, also known as the posterior, as in the information after having seen the data. Going from the prior to the posterior is Bayes updating.\nThe probability of HIV after one positive ELISA, 0.12, was the posterior in the previous section as it was an update of the overall prevalence of HIV, @ref(eq:HIVpositive). However, in this section we answered a question where we used this posterior information as the prior. This process of using a posterior as prior in a new problem is natural in the Bayesian framework of updating knowledge based on the data."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#fathers-of-statistics",
    "href": "content/bootcamp/stats/class-05.html#fathers-of-statistics",
    "title": "Bootcamp: Stats class 5",
    "section": "Fathers of statistics",
    "text": "Fathers of statistics\nThe torch was passed within the triumvirate of Galton, Pearson, and Fisher.\n\nSir Francis Galton (1822-1911)\n\n\n\nfrom galton.org\n\n\n\nDiscovered regression to the mean\nRe-discovered correlation and regression and discovered how to apply these in anthropology, psychology, and more\nDefined the concept of standard deviation\nEstablished the field of Eugenics in 1883\nDarwin’s cousin.\n\nGalton’s reasoning for coining the term eugenics:\n\n“We greatly want a brief word to express the science of improving stock, which…takes cognisance of all influences that tend in however remote a degree to give the more suitable races or strains of blood a better chance of prevailing speedily over the less suitable than they otherwise would have had.”\n\n\n\nKarl Pearson (1857-1936)\n\n\n\nhttps://www.britannica.com/biography/Karl-Pearson\n\n\nKarl Pearson was Galton’s protg and directly or contributed to:\n\nDeveloped hypothesis testing\nDeveloped the use of p-values\nDefined the Chi-Squared test\nCorrelation coefficient\nPrinciple components analysis\n\nAlso authors of timeless “classics” such as:\nThe Woman’s Question\nNational Life from the standpoint of science\nIn the year Mein Kampf was published, Pearson wrote an article called:\nTHE PROBLEM OF ALIEN IMMIGRATION INTO GREAT BRITAIN, ILLUSTRATED BY AN EXAMINATION OF RUSSIAN AND POLISH JEWISH CHILDREN\nHere is an excerpt:\n\n“[they] will develop into a parasitic race…Taken on the average, and regarding both sexes, this alien Jewish population is somewhat inferior physically and mentally to the native population.”\n\n\n\nSir Ronald Aylmer Fisher (1890-1962)\n\n\n\nhttps://www.42evolution.org/ronald-a-fisher/\n\n\nFisher’s work in statistics established and promoted many important methods of statistical inference. His contributions include:\n\nEstablishing p = 0.05 as the normal threshold for significant p-values\nPromoting Maximum Likelihood Estimation\nDeveloping the ANalysis Of VAriance (ANOVA) The iris dataset (this seems an incredibly minor contribution but I use it daily)\nThe Genetical Theory of Natural Selection, which blended the work of Mendel and Darwin.\n\nThere is no lack of Fisher’s strong and consistent support for eugenics. Here is an example from as late as 1954.\n\n\n\nLetter from R.A. Fisher to R. Ruggles Gates. Ronald Fisher Archive. University of Adelaide."
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#storytime-galton-laboratory",
    "href": "content/bootcamp/stats/class-05.html#storytime-galton-laboratory",
    "title": "Bootcamp: Stats class 5",
    "section": "Storytime: Galton Laboratory",
    "text": "Storytime: Galton Laboratory\nGalton founded the Eugenics Record Office (1904)\nGalton Eugenics Laboratory as part of University College London (UCL). Created by Pearson and funded by Galton. (1907)\nGalton left UCL enough money to create a Chair in National Eugenics, filled by Pearson and then Fisher. Hell of a name for an endowed chair!\nAnnals of Human Genetics: It was established in 1925 Pearson as the Annals of Eugenics, and obtained its current name in 1954.\nGalton laboratory was incorporated into the Department of Eugenics, Biometry and Genetic at UCL in 1944.\nRenamed to the Department of Human Genetics and Biometry in 1966.\nBecame part of the Department of Biology at UCL in 1996.\nIn 2020: UCL renames three facilities that honoured prominent eugenicists\nThese horrendous views did not appear to be common at UCL in the 1930s. For example, they were not held by JBS Haldane, Egon Pearson (son of Karl), and Lionel Penrose.\n\nWhat about in the US?\nCSHL - Eugenics Archive\nU.S. Scientists’ Role in the Eugenics Movement (1907–1939): A Contemporary Biologist’s Perspective\nCharles Davenport (first director of CSHL) and the Carnegie Insitution\nCold Spring Harbor and German Eugenics in the 1930s\nEugenics and the history of Science and AAAS"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#section",
    "href": "content/bootcamp/stats/class-05.html#section",
    "title": "Bootcamp: Stats class 5",
    "section": "",
    "text": "from “America’s Shameful History of Eugenics and Forced Sterilizations”"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#modern-day-eugenics-and-beyond",
    "href": "content/bootcamp/stats/class-05.html#modern-day-eugenics-and-beyond",
    "title": "Bootcamp: Stats class 5",
    "section": "Modern day: Eugenics and beyond",
    "text": "Modern day: Eugenics and beyond\n\n[Sordid genealogies: a conjectural history of Cambridge Analytica’s\neugenic roots](https://www.nature.com/articles/s41599-020-0505-5)\n\n\nAmerican Renaissance\n\n‘Race’ cannot be biologically defined due to genetic variation among human individuals and populations. (A) The old concept of the “five races:” African, Asian, European, Native American, and Oceanian. (B) Actual genetic variation in humans.\n\n\nPolygenic Traits, Human Embryos, and Eugenic Dreams\nAn academic study debunked the idea of “Screening Human Embryos for Polygenic Traits,” but the CEO of the company Stephen Hsu cofounded announced that they had screened human embryos for polygenic traits.\n\nThe amoral nonsense of Orchid’s embryo selection\n\n\n\n“Superior: The Return of Race Science”\n\n\n\nhttps://en.wikipedia.org/wiki/Superior:_The_Return_of_Race_Science\n\n\n\n\n“Weapons of Math Destruction”\n\n\n\nhttps://en.wikipedia.org/wiki/Weapons_of_Math_Destruction"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#multiple-testing",
    "href": "content/bootcamp/stats/class-05.html#multiple-testing",
    "title": "Bootcamp: Stats class 5",
    "section": "Multiple testing",
    "text": "Multiple testing\n\nHow does multiple testing correction work?\n\nMultiple Testing — How Should You Adjust?\n\nMultiple Comparisons\n\nAn Overview of Methods to Address the Multiple Comparison Problem"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#bayesian-statistics-1",
    "href": "content/bootcamp/stats/class-05.html#bayesian-statistics-1",
    "title": "Bootcamp: Stats class 5",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nHigh-speed intro to Bayes’s rule\nAn Introduction to Bayesian Thinking\nBayes’ Theorem, Clearly Explained!!!!"
  },
  {
    "objectID": "content/bootcamp/stats/class-05.html#modern-statistics-beer-and-eugenics-1",
    "href": "content/bootcamp/stats/class-05.html#modern-statistics-beer-and-eugenics-1",
    "title": "Bootcamp: Stats class 5",
    "section": "Modern Statistics, Beer, and Eugenics",
    "text": "Modern Statistics, Beer, and Eugenics\n\nWhy We Might Not Have Statistics Without Guinness Brewery\nStatistics, Eugenics, and Me\nIs Statistics Racist?\nBeer Vs. Eugenics: The Good And The Bad Uses Of Statistics\nEngineering American society: the lesson of eugenics\nEugenics – journey to the dark side at the dawn of statistics\nHow Eugenics Shaped Statistics\nFrancis Galton’s Statistical Ideas: The Influence of Eugenics\nR. A. Fisher: a faith fit for eugenics\nSordid genealogies: a conjectural history of Cambridge Analytica’s eugenic roots\nU.S. Scientists’ Role in the Eugenics Movement (1907–1939): A Contemporary Biologist’s Perspective\nBiomedical centre memorial to victims of Nazi research\nBerlin Wild—and the Max Delbrck Center for Molecular Medicine\nEugenics timeline\nKarl Pearson praised Hitler and Nazi Race Hygiene\nRonald Fisher Is Not Being ‘Cancelled’, But His Eugenic Advocacy Should Have Consequences"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#provide-a-simple-and-flexible-framework",
    "href": "content/bootcamp/stats/class-04.html#provide-a-simple-and-flexible-framework",
    "title": "Bootcamp: Stats class 4",
    "section": "Provide a simple and flexible framework",
    "text": "Provide a simple and flexible framework"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#variables-definitions",
    "href": "content/bootcamp/stats/class-04.html#variables-definitions",
    "title": "Bootcamp: Stats class 4",
    "section": "Variables definitions",
    "text": "Variables definitions\n\nRandom variables (x, y)\nResponse Variable ( y - aka dependent or outcome variable): this variable is predicted or its variation is explained by the explanatory variable. In an experiment, this is the outcome that is measured following manipulation of the explanatory variable.\nExplanatory Variable ( x - aka independent or predictor variable): explains variations in the response variable. In an experiment, it is manipulated by the researcher.\n\n\nQuantitative Variables\nDiscrete variable: numeric variables that have a countable number of values between any two values - integer in R (e.g., number of mice, read counts).\nContinuous variable: numeric variables that have an infinite number of values between any two values - numeric in R (e.g., normalized expression values, fluorescent intensity).\n\n\nCategorical Variables\nNominal variable: (unordered) random variables have categories where order doesn’t matter - factor in R (e.g., country, type of gene, genotype).\nOrdinal variable: (ordered) random variables have ordered categories - order of levels in R ( e.g. grade of tumor)."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#hypothesis-testing-definitions",
    "href": "content/bootcamp/stats/class-04.html#hypothesis-testing-definitions",
    "title": "Bootcamp: Stats class 4",
    "section": "Hypothesis testing definitions",
    "text": "Hypothesis testing definitions\nHypothesis testing is a statistical analysis that uses sample data to assess two mutually exclusive theories about the properties of a population. Statisticians call these theories the null hypothesis and the alternative hypothesis. A hypothesis test assesses your sample statistic and factors in an estimate of the sample error to determine which hypothesis the data support.\nWhen you can reject the null hypothesis, the results are statistically significant, and your data support the theory that an effect exists at the population level.\nA legal analogy: Guilty or not guilty?\nThe statistical concept of ‘significant’ vs. ‘not significant’ can be understood by comparing to the legal concept of ‘guilty’ vs. ‘not guilty’.\nIn the American legal system (and much of the world) a criminal defendant is presumed innocent until proven guilty. If the evidence proves the defendant guilty beyond a reasonable doubt, the verdict is ‘guilty’. Otherwise the verdict is ‘not guilty’. In some countries, this verdict is ‘not proven’, which is a better description. A ‘not guilty’ verdict does not mean the judge or jury concluded that the defendant is innocent -- it just means that the evidence was not strong enough to persuade the judge or jury that the defendant was guilty.\nIn statistical hypothesis testing, you start with the null hypothesis (usually that there is no difference between groups). If the evidence produces a small enough P value, you reject that null hypothesis, and conclude that the difference is real. If the P value is higher than your threshold (usually 0.05), you don’t reject the null hypothesis. This doesn’t mean the evidence convinced you that the treatment had no effect, only that the evidence was not persuasive enough to convince you that there is an effect.\nEffect — the difference between the population value and the null hypothesis value. The effect is also known as population effect or the difference. Typically, you do not know the size of the actual effect. However, you can use a hypothesis test to help you determine whether an effect exists and to estimate its size.\nNull Hypothesis or \\(\\mathcal{H}_0\\) — one of two mutually exclusive theories about the properties of the population in hypothesis testing. Typically, the null hypothesis states that there is no effect (i.e., the effect size equals zero).\nAlternative Hypothesis or \\(\\mathcal{H}_1\\) — the other theory about the properties of the population in hypothesis testing. Typically, the alternative hypothesis states that a population parameter does not equal the null hypothesis value. In other words, there is a non-zero effect. If your sample contains sufficient evidence, you can reject the null and favor the alternative hypothesis.\nP-values — the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. Lower p-values represent stronger evidence against the null. P-values in conjunction with the significance level determines whether your data favor the null or alternative hypothesis.\nStatQuest: P Values, clearly explained\nStatQuest: How to calculate p-values\nSignificance Level or \\(a\\) — an evidentiary standard set before the study. It is the probability that you say there is an effect when there is no effect (the probability of rejecting the null hypothesis given that it is true). Lower significance levels indicate that you require stronger evidence before you will reject the null.It is usually set at or below .05.\n\n\n\nGuinness"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#null-hypothesis-testing",
    "href": "content/bootcamp/stats/class-04.html#null-hypothesis-testing",
    "title": "Bootcamp: Stats class 4",
    "section": "Null hypothesis testing",
    "text": "Null hypothesis testing\n\nSpecify the variables\nDeclare null hypothesis \\(\\mathcal{H}_0\\)\nCalculate test-statistic, exact p-value\nGenerate and visualize data reflecting null-distribution\nCalculate the p-value from the test statistic and null distribution\n\n*4-5: For calculating empirical p-value"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#the-simplicity-underlying-common-tests",
    "href": "content/bootcamp/stats/class-04.html#the-simplicity-underlying-common-tests",
    "title": "Bootcamp: Stats class 4",
    "section": "The simplicity underlying common tests",
    "text": "The simplicity underlying common tests\nMost of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. In particular, it all comes down to:\n\\(y = a \\cdot x + b\\)\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#parametric-vs-non-parametric-tests",
    "href": "content/bootcamp/stats/class-04.html#parametric-vs-non-parametric-tests",
    "title": "Bootcamp: Stats class 4",
    "section": "Parametric vs Non-Parametric tests",
    "text": "Parametric vs Non-Parametric tests\nParametric tests are suitable for normally distributed data.\nNon-Parametric tests are suitable for any continuous data. For the sake of simplicity and sticking with a consistent framework, we will consider Non-Parametric tests as the ranked versions of the corresponding parametric tests.\nMore on choosing Parametric vs Non-Parametric\n\n\n\n\n\nInfo\nParametric\nNon-Parametric\n\n\n\n\nbetter descriptor\nmean\nmedian\n\n\n# of samples (N)\nmany\nfew"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#equation-for-a-line-stats-version",
    "href": "content/bootcamp/stats/class-04.html#equation-for-a-line-stats-version",
    "title": "Bootcamp: Stats class 4",
    "section": "Equation for a line (Stats version)",
    "text": "Equation for a line (Stats version)\nModel: the recipe for \\(y\\) is a slope (\\(\\beta_1\\)) times \\(x\\) plus an intercept (\\(\\beta_0\\), aka a straight line).\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x \\qquad \\mathcal{H}_0: \\beta_1 = 0 \\qquad y = \\beta_0 \\cdot 1\\)\n\\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x \\qquad \\mathcal{H}_0: \\beta_1 \\neq 0\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#find-the-best-beta-coefficients",
    "href": "content/bootcamp/stats/class-04.html#find-the-best-beta-coefficients",
    "title": "Bootcamp: Stats class 4",
    "section": "Find the best \\(\\beta\\) coefficients",
    "text": "Find the best \\(\\beta\\) coefficients\nThese \\(\\beta\\) coefficients are also called the paramters of the model. The \\(\\beta\\) coefficients returned are for the lineear model that best fits the data."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis",
    "href": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis",
    "title": "Bootcamp: Stats class 4",
    "section": "Specify variables and hypothesis",
    "text": "Specify variables and hypothesis\nRemember: \\(y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(cholesterol\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\nSimple model: \\(y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x\\)\n\\(\\mathcal{H}_1:\\) mouse \\(cholesterol\\) does explain \\(weight\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#relationship-between-mouse-weight-and-cholesterol",
    "href": "content/bootcamp/stats/class-04.html#relationship-between-mouse-weight-and-cholesterol",
    "title": "Bootcamp: Stats class 4",
    "section": "Relationship between mouse weight and cholesterol",
    "text": "Relationship between mouse weight and cholesterol\n\n# fitting a line weight vs intercept (mean weight)\nfit_W &lt;- lm(formula = weight ~ 1, data = biochem)\n\n# augment data to add fit/residuals\nbiochem_W &lt;- augment(fit_W, data = biochem)\n\n# plot data\np_wch &lt;- ggplot(data = biochem, aes(y = weight, x = tot_cholesterol)) +\n  geom_point(size = .5) +\n  geom_smooth(method = lm, col = \"red\") +\n  scale_color_manual() +\n  theme_minimal()\n\np_WvInt_res &lt;- ggplot(data = biochem_W, aes(x = tot_cholesterol, y = weight)) +\n  geom_hline(yintercept = biochem_W$.fitted, col = \"red\", size = .5) + # plot linear model fit\n  geom_point(size = .5, aes(color = .resid)) + # plot height as points and color code by the value of the residual\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .25) + # plot line representing residuals\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n\n# fitting a line weight vs icholesterol\nfit_WvC &lt;- lm(\n  data = biochem,\n  formula = weight ~ 1 + tot_cholesterol\n)\n\n# augment data to add fit/residuals\nbiochem_WvC &lt;- augment(fit_WvC, data = biochem)\n\n# plot data\np_WvC_res &lt;- ggplot(data = biochem_WvC, aes(x = tot_cholesterol, y = weight)) +\n  geom_point(size = .5, aes(color = .resid)) +\n  geom_smooth(method = lm, col = \"red\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code\n  geom_segment(aes(xend = tot_cholesterol, yend = .fitted), alpha = .1) + # plot line representing residuals\n  theme_minimal()\n\n\n\nbiochem_WvC_rsq &lt;- fit_WvC %&gt;%\n  glance() %&gt;%\n  pull(r.squared)\n\nbiochem_WvC_pval &lt;- fit_WvC %&gt;%\n  glance() %&gt;%\n  pull(p.value)\n\n\nplot_grid(p_WvInt_res, p_WvC_res, ncol = 2, labels = c(\"weight by intercept\", \"weight by cholesterol\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nTo what extent does mouse cholesterol predict mouse weight?\n\\(R^2\\) or coefficient of determination — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n13%\n\n\nProbability that relationship is due to chance?\n\\(p-value\\) — the probability of obtaining an \\(F-statistic\\) in the null distribution at least as extreme as our observed \\(F-statistic\\).\n8.9241551^{-54}"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#guinness-brewery-in-dublin",
    "href": "content/bootcamp/stats/class-04.html#guinness-brewery-in-dublin",
    "title": "Bootcamp: Stats class 4",
    "section": "Guinness Brewery in Dublin",
    "text": "Guinness Brewery in Dublin\n \nWe will compare mouse \\(weight\\) by \\(gender\\)."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis-1",
    "href": "content/bootcamp/stats/class-04.html#specify-variables-and-hypothesis-1",
    "title": "Bootcamp: Stats class 4",
    "section": "Specify variables and hypothesis",
    "text": "Specify variables and hypothesis\nModel: \\(y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(gender\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\\(\\mathcal{H}_1:\\) mouse \\(gender\\) does explain \\(weight\\)\nImportant: \\(x_{i}\\) is an indicator (0 or 1) saying whether data point i was sampled from one or the other group (female or male).\nWe will explore this in more detail soon."
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#calculate-ss_mean-for-weight-by-gender-female-vs-male",
    "href": "content/bootcamp/stats/class-04.html#calculate-ss_mean-for-weight-by-gender-female-vs-male",
    "title": "Bootcamp: Stats class 4",
    "section": "1. Calculate \\(SS_{mean}\\) for weight by gender (female vs male)",
    "text": "1. Calculate \\(SS_{mean}\\) for weight by gender (female vs male)\n\nCompare \\(SS_{mean}\\) for weight by cholesterol versus weight by gender\n\\(SS_{mean}\\) — sum of squares around the overall mean of \\(y\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (data - mean)^2\\)\n\\(SS_{mean} = \\sum_{i=1}^{n} (y_{i} - \\overline{y})^2\\)\n\nResiduals, \\(e\\) — the difference between the observed value of the dependent variable \\(y\\) and the predicted value \\(\\widehat{y}\\) is called the residual. Each data point has one residual.\n\\(e = y_{i} - \\widehat{y}\\)\n\n\nClass exercise 1:\nWhich of these are valid ways to calculate \\(SS_{mean}\\) from biochem_W?\n\n# A. sum((biochem_W$weight - biochem_W$.fitted)^2)\n# B. sum((biochem_W$weight - biochem_W$.resid)^2)\n# C. sum(biochem_W$.resid^2)\n# D. sum((biochem_W$weight - mean(biochem_W$weight))^2)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#calculate-ss_fit-for-weight-by-gender",
    "href": "content/bootcamp/stats/class-04.html#calculate-ss_fit-for-weight-by-gender",
    "title": "Bootcamp: Stats class 4",
    "section": "2. Calculate \\(SS_{fit}\\) for weight by gender",
    "text": "2. Calculate \\(SS_{fit}\\) for weight by gender\n\nCompare \\(SS_{fit}\\) vs \\(SS_{fit}\\) weight by gender\n\n# fitting a line weight vs intercept + gender\nfit_WvG &lt;- lm(formula = weight ~ 1 + gender, data = biochem)\n\n# augment (i.e. add fitted and residual values)\nbiochem_WvG &lt;- augment(fit_WvG, data = biochem)\n\n# plot of data with mean and colored by residuals\np_WvG_res &lt;- ggplot(biochem_WvG, aes(x = gender, y = weight)) +\n  geom_point(position = position_jitter(), aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  geom_segment(aes(x = .5, xend = 1.5, y = fit_WvG$coefficients[1], yend = fit_WvG$coefficients[1]), color = \"red\") +\n  geom_segment(aes(x = 1.5, xend = 2.5, y = sum(fit_WvG$coefficients)), yend = sum(fit_WvG$coefficients), color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNOTE: We are fitting 2 lines to the data\nFor the weight by intercept model (right) we fit 1 line.\nFor the weight by gender model (left) we fit 2 lines (i.e. male and female).\n\n\nExceptions to the fit\nCheck the residuals!\n\n\nMatrices Interlude Begin\n\nHow do we go from 2 fit lines to 1 equation\nSince we don’t want to calculate any of this by hand, the framework needs to be flexible such that a computer can execute for different flavors of comparison (cont y vs cont x, cont y vs 2 or more categorical x, …).\nLet’s break this down and focus on just a few players.\n\np_exc\n\n\n\n# ggplot(data = biochem_WvG, aes(.resid, color=gender)) +\n#   geom_density() +\n#   theme_minimal()\n\nRemember that:\n\\(weight\\) is \\(y\\)\n\\(F_{avg}\\) is the average \\(weight\\) of \\(females\\)\n\\(M_{avg}\\) is the average \\(weight\\) of \\(males\\)\n\nA048054885, female\n\\(y_{85}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{85}\\)\nA067109771, female\n\\(y_{71}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{71}\\)\n\nA066822351, male\n\\(y_{51}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{51}\\)\nA048274362, male\n\\(y_{62}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{62}\\)"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#need-a-volunteer",
    "href": "content/bootcamp/stats/class-04.html#need-a-volunteer",
    "title": "Bootcamp: Stats class 4",
    "section": "Need a volunteer",
    "text": "Need a volunteer\nMe: Ooohh my, imagine how tedious it would be to do this for all 1782 mice…\nVolunteer: Wait a sec…isn’t there a way to formulate this as a matrix algebra problem.\nMe: You’re right - I’m so glad you asked! Let’s wield our matrix-magic at this problem and see what happens.\n\\(f_{avg} = \\beta_0\\) is the average \\(weight\\) of \\(female\\) mice\n\\(m_{avg} = \\beta_1\\) is the average \\(weight\\) of \\(male\\) mice\n\\(\\begin{bmatrix} y_{85} \\\\ y_{71} \\\\ y_{51} \\\\y_{62} \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{85} \\\\ e_{71} \\\\ e_{51} \\\\e_{62} \\end{bmatrix}\\)\nSo basically this looks like the same equation for fitting a line we’ve been discussing, just w/a few more dimensions :)\nThis is a conceptual peak into the underbelly of how the \\(\\beta\\) cofficients and least squares is performed using matrix operations (remember linear algebra, maybe?). We will not go any deeper in this course, but if you are interested in learning more I recommend the following resources:\n\nLinear Models Pt.3 - Design Matrices\n\nA Matrix Formulation of the Multiple Regression Model\n\nMatrices Interlude FIN\n\n\nClass exercise 2:\nWhich of these are valid ways to calculate \\(SS_{mean}\\) from biochem_W?\n\\(SS_{fit}\\) — sum of squares around the least-squares fit\n\\(SS_{fit} = \\sum_{i=1}^{n} (data - line)^2\\)\nWhich of these are valid ways to calculate \\(SS_{fit}\\) from biochem_WvG?\n\n# A. sum(biochem_WvG$.resid^2)\n# B. sum((biochem_WvG$weight - biochem_WvG$.resid)^2)\n# C. sum((biochem_WvG$weight - biochem_WvG$.fitted)^2)\n# D. sum((biochem_WvG$weight - mean(biochem_WvG$weight))^2)\n\np_WvG_res"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#calculate-f-statistic",
    "href": "content/bootcamp/stats/class-04.html#calculate-f-statistic",
    "title": "Bootcamp: Stats class 4",
    "section": "3. Calculate \\(F-statistic\\)",
    "text": "3. Calculate \\(F-statistic\\)\nF-statistic — the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\\(F = \\displaystyle \\frac{SS_{fit}/(p_{fit}-p_{mean})} {SS_{mean}/(n-p_{fit})}\\)\n\\(p_{fit}\\) — number of parameters in the fit line\n\\(p_{mean}\\) — number of parameters in the mean line\n\\(n\\) — number of data points"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#sanity-check",
    "href": "content/bootcamp/stats/class-04.html#sanity-check",
    "title": "Bootcamp: Stats class 4",
    "section": "4. Sanity check",
    "text": "4. Sanity check\n\n# sum of squares of the residuals for the simple model\nss.mean &lt;- sum(biochem_W$.resid^2)\n\n# sum of squares of the residuals for the complex model\nss.fit &lt;- sum(biochem_WvG$.resid^2)\n\n#### COEFFICIENTS NEEL\n\n# number of paramters in simple model\npmean &lt;- 1\n\n# number of paramters in complex model\npfit &lt;- 2\n\n# F-value\nbiochem_WvG_F &lt;- ((ss.mean - ss.fit) / (pfit - 1)) /\n  (ss.fit / (nrow(biochem_WvG) - pfit))\n\nbiochem_WvG_F\n\n[1] 1296.507\n\nglance(fit_WvG) %&gt;% pull(statistic)\n\n   value \n1296.507 \n\n\n\nLet’s take a look at the statistic and p-values:\n\nfit_WvG_stats &lt;- tidy(fit_WvG) %&gt;%\n  filter(term == \"genderM\") %&gt;%\n  select(statistic, p.value)\n\n\n# to run a t.test in R we need numeric vectors for each of our groups of interest\nmale_weight_t &lt;- biochem_WvG %&gt;%\n  filter(gender == \"M\") %&gt;%\n  pull(weight)\n\nfemale_weight_t &lt;- biochem_WvG %&gt;%\n  filter(gender == \"F\") %&gt;%\n  pull(weight)\n\n\ntrad_t &lt;- t.test(male_weight_t, female_weight_t, var.equal = T)\n\ntrad_WvG_stats &lt;- tidy(trad_t) %&gt;% select(statistic, p.value)\n\nfit_WvG_stats\n\n# A tibble: 1 x 2\n  statistic   p.value\n      &lt;dbl&gt;     &lt;dbl&gt;\n1      36.0 9.25e-214\n\ntrad_WvG_stats\n\n# A tibble: 1 x 2\n  statistic   p.value\n      &lt;dbl&gt;     &lt;dbl&gt;\n1      36.0 9.25e-214"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#one-way-anova",
    "href": "content/bootcamp/stats/class-04.html#one-way-anova",
    "title": "Bootcamp: Stats class 4",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nLet’s compare the \\(weight\\) by \\(family\\), but only for a few selected families.\n\n# biochem %&gt;%\n#   group_by(family) %&gt;%\n#   count(family) %&gt;%\n#   arrage(-n) %&gt;%\n#   View()\n\nbigfams &lt;- biochem %&gt;%\n  group_by(family) %&gt;%\n  count(family) %&gt;%\n  filter(n &gt; 10) %&gt;%\n  pull(family)\n\nbiochem_bigfams &lt;- biochem %&gt;%\n  filter(family %in% bigfams)\n\n# i have pre-selected some families to compare\nmyfams &lt;- c(\n  \"B1.5:E1.4(4) B1.5:A1.4(5)\",\n  \"F1.3:A1.2(3) F1.3:E2.2(3)\",\n  \"A1.3:D1.2(3) A1.3:H1.2(3)\",\n  \"D5.4:G2.3(4) D5.4:C4.3(4)\"\n)\n\n# only keep the familys in myfams\nfam_data &lt;- biochem_bigfams %&gt;%\n  filter(family %in% myfams) %&gt;%\n  droplevels()\n\n# simplify family names and make factor\nfam_data$family &lt;- gsub(\n  pattern = \"\\\\..*\",\n  replacement = \"\",\n  x = fam_data$family\n) %&gt;%\n  as.factor()\n\n\n# make B1 the reference (most similar to overall mean)\nfam_data$family &lt;- relevel(x = fam_data$family, ref = \"B1\")\n\nModel: \\(y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}\\)\nNull Hypothesis: \\(\\mathcal{H}_0: \\beta_1 = 0\\)\n\\(\\mathcal{H}_0:\\) mouse \\(family\\) does NOT explain \\(weight\\)\nAlternative Hypothesis: \\(\\mathcal{H}_1: \\beta_1 \\neq 0\\)\n\\(\\mathcal{H}_1:\\) mouse \\(family\\) does explain \\(weight\\)\nImportant: \\(x_{i}\\) is an indicator (0 or 1) saying which group point \\(i\\) was sampled from using the matrix encoding of 0s and 1s.\nBelow is an example depicting 6 observations with 2 from each of 3 families:\n\\(\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3} \\\\y_{4} \\\\y_{5} \\\\y_{5} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{1} \\\\ e_{2} \\\\ e_{3} \\\\e_{4} \\\\e_{5} \\\\e_{6} \\end{bmatrix}\\)\n\n# fitting a line weight vs intercept + family\nfit_WvFam &lt;- lm(formula = weight ~ family, data = fam_data)\n\n\n# augment (i.e. add fitted and residual values)\nbiochem_WvFam &lt;- augment(fit_WvFam, fam_data)\n\nmean_B1 &lt;- fit_WvFam$coefficients[1]\nmean_A1 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[2]\nmean_D5 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[3]\nmean_F1 &lt;- fit_WvFam$coefficients[1] + fit_WvFam$coefficients[4]\n\n# plot of data with mean and colored by residuals\np_WvFam_res &lt;- ggplot(biochem_WvFam, aes(x = family, y = weight)) +\n  geom_point(position = position_jitter(), aes(color = .resid)) +\n  geom_segment(aes(x = .5, xend = 1.5, y = mean_B1, yend = mean_B1), color = \"red\") +\n  geom_segment(aes(x = 1.5, xend = 2.5, y = mean_A1, yend = mean_A1), color = \"red\") +\n  geom_segment(aes(x = 2.5, xend = 3.5, y = mean_D5, yend = mean_D5), color = \"red\") +\n  geom_segment(aes(x = 3.5, xend = 4.5, y = mean_F1, yend = mean_F1), color = \"red\") +\n  geom_segment(aes(x = .5, xend = 4.5, y = mean(weight), yend = mean(weight)), color = \"black\") +\n  scale_color_gradient2(low = \"blue\", mid = \"black\", high = \"yellow\") + # color code for plotting residuals\n  theme_minimal()\n\np_WvFam_res\n\n\n\n\n\n\nanova_WvFam &lt;- aov(formula = weight ~ family, data = fam_data)\n\n# are the coefficients the same?\nbind_cols(tidy(fit_WvFam) %&gt;% select(term, estimate),\n  ANOVA_coef = anova_WvFam$coefficients\n)\n\n# A tibble: 4 x 3\n  term        estimate ANOVA_coef\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   18.1       18.1  \n2 familyA1      -3.66      -3.66 \n3 familyD5       5.86       5.86 \n4 familyF1      -0.682     -0.682\n\nglance(fit_WvFam) %&gt;% select(p.value)\n\n# A tibble: 1 x 1\n   p.value\n     &lt;dbl&gt;\n1 7.62e-13\n\ntidy(anova_WvFam) %&gt;% select(p.value)\n\n# A tibble: 2 x 1\n    p.value\n      &lt;dbl&gt;\n1  7.62e-13\n2 NA       \n\n\n\n\nClass exercise 3:\n\n\n\n\n\nWhat are the \\(p_{mean}\\) and \\(p_{fit}\\) for the models above?\n\n\n\n\n\nParameter\nfit cholesterol\nfit gender\nfit family\n\n\n\n\n\\(p_{mean}\\)\n?\n?\n?\n\n\n\\(p_{fit}\\)\n?\n?\n?"
  },
  {
    "objectID": "content/bootcamp/stats/class-04.html#ancova-analysis-of-covariance",
    "href": "content/bootcamp/stats/class-04.html#ancova-analysis-of-covariance",
    "title": "Bootcamp: Stats class 4",
    "section": "ANCOVA, Analysis of Covariance",
    "text": "ANCOVA, Analysis of Covariance\nANOVA with more than one independent variable. In this case, what is the impact of mouse age on mouse weight for males vs females.\n\n# more info here https://towardsdatascience.com/doing-and-reporting-your-first-anova-and-ancova-in-r-1d820940f2ef\n\np_WvA_gender &lt;- ggplot(data = biochem, aes(y = weight, x = age, color = gender)) +\n  geom_point(size = .5) +\n  geom_smooth(method = lm) +\n  theme_minimal()\n\np_WvA_gender\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nWvA_gender &lt;- lm(formula = weight ~ 1 + age + gender, data = biochem)\n\nWvA_gender %&gt;% glance()\n\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.475         0.475  2.42      805. 8.37e-250     2 -4100. 8209. 8231.\n# i 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\naov(formula = weight ~ 1 + age + gender, data = biochem) %&gt;% glance()\n\n# A tibble: 1 x 6\n  logLik   AIC   BIC deviance  nobs r.squared\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1 -4100. 8209. 8231.   10402.  1782     0.475"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOLB 7950: Informatics for Molecular Biology",
    "section": "",
    "text": "MOLB 7950 - Fall 2023 Schedule\n    \n    \n      Classes held in-person in AHSB 2201, 9:00-10:30am\n    \n    \n      \n      Date\n      Block\n      Topic\n      Instructor\n      Title\n      \n        Links\n      \n    \n    \n      Page\n      Slides\n      Exercises\n      HW\n      Key\n    \n  \n  \n    \n      Week 1\n    \n    01\nMon, Aug 28, 2023\nBootcamp\nR\nHesselberth\nIntro to RStudio & R\n📃\n📄\n\n\n\n\n\n\n\n\n\n\n\n\n    02\nTue, Aug 29, 2023\nBootcamp\nR\nHesselberth\nTidy data\n📃\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    03\nWed, Aug 30, 2023\nBootcamp\nR\nHesselberth\ntidyr\n📃\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    04\nThu, Aug 31, 2023\nBootcamp\nR\nHesselberth\nggplot2\n📃\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    05\nFri, Sep 1, 2023\nBootcamp\nR\nHesselberth\ndplyr\n📃\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 2\n    \n    06\nMon, Sep 4, 2023\n-\n-\n-\nNO CLASS: LABOR DAY\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    07\nTue, Sep 5, 2023\nBootcamp\nR\nHesselberth\nadvanced ggplot2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    08\nWed, Sep 6, 2023\nBootcamp\nR\nHesselberth\nadvanced dplyr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    09\nThu, Sep 7, 2023\nBootcamp\nR\nHesselberth\nvignette\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    10\nFri, Sep 8, 2023\nBootcamp\nStatistics\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 3\n    \n    11\nMon, Sep 11, 2023\nBootcamp\nStatistics\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    12\nTue, Sep 12, 2023\nBootcamp\nStatistics\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    13\nWed, Sep 13, 2023\nBootcamp\nStatistics\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    14\nThu, Sep 14, 2023\nBootcamp\nStatistics\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    15\nFri, Sep 15, 2023\nBootcamp\nStatistics\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 4\n    \n    16\nMon, Sep 18, 2023\nDNA\nDNA-seq Overview\nHesselberth\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    17\nWed, Sep 20, 2023\nDNA\nChIP-seq\nHesselberth\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    18\nFri, Sep 22, 2023\nDNA\nChIP-seq\nHesselberth\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 5\n    \n    19\nMon, Sep 25, 2023\nDNA\nChromatin Accessibility\nHesselberth\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    20\nWed, Sep 27, 2023\nDNA\nChromatin Accessibility\nHesselberth\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    21\nFri, Sep 29, 2023\nDNA\nVignette\nHesselberth\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 6\n    \n    22\nMon, Oct 2, 2023\nRNA\nRNA-seq Overview\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    23\nWed, Oct 4, 2023\nRNA\nDifferential Gene Expression\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    24\nFri, Oct 6, 2023\nRNA\nDifferential Gene Expression\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 7\n    \n    25\nMon, Oct 9, 2023\nRNA\nAlternative Splicing\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    26\nWed, Oct 11, 2023\nRNA\nVignette\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    27\nFri, Oct 13, 2023\n-\n-\n-\nNO CLASS: CSDV RETREAT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 8\n    \n    28\nMon, Oct 16, 2023\nRNA\nRBP\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    29\nWed, Oct 18, 2023\nRNA\nRBP\nMukherjee\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    30\nFri, Oct 20, 2023\nRNA\nLong-read sequencing\nHesselberth\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 9\n    \n    31\nMon, Oct 23, 2023\nRNA\nSingle-cell\nRiemondy\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    32\nWed, Oct 25, 2023\nRNA\nSingle-cell\nRiemondy\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    33\nFri, Oct 27, 2023\n-\n-\n-\nNO CLASS: MOLB RETREAT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      Week 10\n    \n    34\nMon, Oct 30, 2023\nFinal\n-\n-\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    35\nWed, Nov 1, 2023\nFinal\n-\n-\n-"
  }
]